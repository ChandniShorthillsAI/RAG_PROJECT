
---

# ğŸ§¬ RAG-Based Chatbot on Human Evolution (Using Wikipedia + LLaMA 3)

This project builds a **Retrieval-Augmented Generation (RAG)** pipeline that scrapes content from **Wikipedia on Human Evolution**, generates dense embeddings using **SentenceTransformers**, stores them in a **FAISS** index, and uses **LLaMA 3** (locally hosted via **Ollama**) to answer user questions based on the retrieved context. An evaluation pipeline measures the quality of generated answers using NLP metrics like **BERTScore** and **F1 score**.

---

## ğŸ“Œ Components

### 1. ğŸ§¹ Web Scraping

- **Source**: Wikipedia page(s) related to **Human Evolution**.
- **Purpose**: Extract and clean textual data.
- **Output**: Combined text file saved under `data/`.

### 2. ğŸ§  Embeddings + FAISS Index

- **Embedding Model**: SentenceTransformers (`all-MiniLM-L6-v2` or similar).
- **Workflow**:
  - Chunk the scraped text using `RecursiveCharacterTextSplitter`.
  - Generate embeddings using SentenceTransformer.
  - Store the vectors in a FAISS index.

### 3. ğŸ¤– RAG Pipeline (QnA)

- **Input**: Questions from `generated_qa.csv`.
- **Retrieval**: Top-k similar chunks via FAISS.
- **LLM**: `llama3` served locally using [Ollama](https://ollama.com/).
- **Pipeline Script**: `rag_pipeline.py`
- **Output**: `qa_with_context.csv` containing the question, ground truth, retrieved context, and LLM-generated answer.

### 4. ğŸ“Š Evaluation Pipeline

- **Input**: `qa_with_context.csv`
- **Implemented Metrics**:
  - **BERTScore**
  - **F1 Score**
- **Scripts**:
  - `evaluate_llm_answers.py`
- **Outputs**:
  - `qa_with_scores.csv`: Scores per question
  - `qa_score_summary.csv`: Mean scores across all questions

---

## ğŸ“ Folder Structure

```
RAG_LLM/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ human_evolution_combined.txt         # Scraped Wikipedia data
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ index.faiss                          # FAISS index
â”‚   â””â”€â”€ index.pkl                            # Mapping info
â”‚
â”œâ”€â”€ utils/                                   #contains scraper.py and ui_app.py file
â”‚
â”œâ”€â”€ output_files/
â”‚   â”œâ”€â”€ generated_qa.csv                      # Input QA pairs
â”‚   â”œâ”€â”€ qa_with_context.csv                   # QA + context + LLM answer
â”‚   â”œâ”€â”€ qa_with_scores.csv                    # Evaluation per QA
â”‚   â””â”€â”€ qa_score_summary.csv                  # Overall evaluation summary
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_qa_dataset.py                # Script to create QA pairs
â”‚   â”œâ”€â”€ generate_llm_answers.py               # Script for RAG + answering
â”‚   â”œâ”€â”€ evaluate_llm_answers.py               # BERTScore and F1
â”‚   â””â”€â”€ rag_pipeline.py                       # Core pipeline
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

### 1. Install Requirements
```bash
pip install -r requirements.txt
```

### 2. Start LLaMA 3 Locally
```bash
ollama run llama3
```

### 3. Generate Answers
```bash
python src/generate_llm_answers.py
```

### 4. Evaluate Answers
```bash
python src/evaluate_llm_answers.py
```

---

## âœ… Output Files

- `qa_with_context.csv`: Context + generated answers.
- `qa_with_scores.csv`: Scores (BERT, F1) per QA.
- `qa_score_summary.csv`: Aggregate metrics across all QAs.

---

## ğŸ›  Tech Stack

- **Python**
- **BeautifulSoup**, `requests` for web scraping
- **SentenceTransformers** for text embeddings
- **FAISS** for vector similarity search
- **Ollama** with **LLaMA 3** for local inference
- **NLTK**, **scikit-learn**, **evaluate** for QA scoring

---

