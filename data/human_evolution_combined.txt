

---

# Human evolution

Homo sapiens is a distinct species of the hominid family of primates, which also includes all the great apes.[1] Over their evolutionary history, humans gradually developed traits such as bipedalism, dexterity, and complex language,[2] as well as interbreeding with other hominins (a tribe of the African hominid subfamily),[3] indicating that human evolution was not linear but weblike.[4][5][6][7] The study of the origins of humans involves several scientific disciplines, including physical and evolutionary anthropology, paleontology, and genetics; the field is also known by the terms anthropogeny, anthropogenesis, and anthropogony[8][9]—with the latter two sometimes used to refer to the related subject of hominization.
Primates diverged from other mammals about 85 million years ago (mya), in the Late Cretaceous period, with their earliest fossils appearing over 55 mya, during the Paleocene.[10] Primates produced successive clades leading to the ape superfamily, which gave rise to the hominid and the gibbon families; these diverged some 15–20 mya. African and Asian hominids (including orangutans) diverged about 14 mya. Hominins (including the Australopithecine and Panina subtribes) parted from the Gorillini tribe between 8 and 9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya.[11] The Homo genus is evidenced by the appearance of H. habilis over 2 mya,[a] while anatomically modern humans emerged in Africa approximately 300,000 years ago.
The evolutionary history of primates can be traced back 65 million years.[12][13][14][15][16] One of the oldest known primate-like mammal species, the Plesiadapis, came from North America;[17][18][19][20][21][22] another, Archicebus, came from China.[23] Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.
David R. Begun[24] concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to Dryopithecus, migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or "bush babies" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.
The earliest known catarrhine is Kamoyapithecus from the uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago.[25] Its ancestry is thought to be species related to Aegyptopithecus, Propliopithecus, and Parapithecus from the Faiyum, at around 35 mya.[26] In 2010, Saadanius was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 mya, helping to fill an 11-million-year gap in the fossil record.[27]
In the Early Miocene, about 22 million years ago, the many kinds of arboreally-adapted (tree-dwelling) primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to Victoriapithecus, the earliest Old World monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are Proconsul, Rangwapithecus, Dendropithecus, Limnopithecus, Nacholapithecus, Equatorius, Nyanzapithecus, Afropithecus, Heliopithecus, and Kenyapithecus, all from East Africa.
The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant, such as Otavipithecus from cave deposits in Namibia, and Pierolapithecus and Dryopithecus from France, Spain and Austria, is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, Oreopithecus, is from coal beds in Italy that have been dated to 9 million years ago.
Molecular evidence indicates that the lineage of gibbons diverged from the line of great apes some 18–12 mya, and that of orangutans (subfamily Ponginae)[b] diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown Southeast Asian hominoid population, but fossil proto-orangutans may be represented by Sivapithecus from India and Griphopithecus from Turkey, dated to around 10 mya.[28]
Hominidae subfamily Homininae (African hominids) diverged from Ponginae (orangutans) about 14 mya. Hominins (including humans and the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8 and 9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya.[11] The Homo genus is evidenced by the appearance of H. habilis over 2 mya,[a] while anatomically modern humans emerged in Africa approximately 300,000 years ago.
Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by Nakalipithecus fossils found in Kenya and Ouranopithecus found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus Pan) split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation – rain forest soils tend to be acidic and dissolve bone – and sampling bias probably contribute to this problem.
Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are Sahelanthropus tchadensis (7 Ma) and Orrorin tugenensis (6 Ma), followed by Ardipithecus (5.5–4.4 Ma), with species Ar. kadabba and Ar. ramidus.
It has been argued in a study of the life history of Ar. ramidus that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape.[30] This study demonstrated affinities between the skull morphology of Ar. ramidus and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos (Pan paniscus) the less aggressive species of the genus Pan, may have evolved via the process of self-domestication. Consequently, arguing against the so-called "chimpanzee referential model"[31] the authors suggest it is no longer tenable to use chimpanzee (Pan troglodytes) social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in Ar. ramidus and the implications this has for the evolution of hominin social psychology, they wrote:
Of course Ar. ramidus differs significantly from bonobos, bonobos having retained a functional canine honing complex. However, the fact that Ar. ramidus shares with bonobos reduced sexual dimorphism, and a more paedomorphic form relative to chimpanzees, suggests that the developmental and social adaptations evident in bonobos may be of assistance in future reconstructions of early hominin social and sexual psychology. In fact the trend towards increased maternal care, female mate selection and self-domestication may have been stronger and more refined in Ar. ramidus than what we see in bonobos.[30]: 128
The authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.
The genus Australopithecus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including Australopithecus anamensis, A. afarensis, A. sediba, and A. africanus. There is still some debate among academics whether certain African hominid species of this time, such as A. robustus and A. boisei, constitute members of the same genus; if so, they would be considered to be "robust australopiths" while the others would be considered "gracile australopiths". However, if these species do indeed constitute their own genus, then they may be given their own name, Paranthropus.
A new proposed species Australopithecus deyiremeda is claimed to have been discovered living at the same time period of A. afarensis. There is debate whether A. deyiremeda is a new species or is A. afarensis.[32] Australopithecus prometheus, otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the genus Australopithecus as old as afarensis.[33] Given the opposable big toe found on Little Foot, it seems that the specimen was a good climber. It is thought given the night predators of the region that he built a nesting platform at night in the trees in a similar fashion to chimpanzees and gorillas.
The earliest documented representative of the genus Homo is Homo habilis, which evolved around 2.8 million years ago,[34] and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of Homo erectus and Homo ergaster in the fossil record, cranial capacity had doubled to 850 cm3.[35] (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that H. erectus and H. ergaster were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between 1.3 to 1.8 million years ago.
According to the recent African origin theory, modern humans evolved in Africa possibly from H. heidelbergensis, H. rhodesiensis or H. antecessor and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of H. erectus, Denisova hominins, H. floresiensis, H. luzonensis and H. neanderthalensis, whose ancestors had left Africa in earlier migrations.[36][37][38][39][40] Archaic Homo sapiens, the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago.[41][42][43] Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited interbreeding between these species.[44][45][46] According to some anthropologists, the transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago (beginning of the Upper Paleolithic),[47] although others point to evidence of a gradual change over a longer time span during the Middle Paleolithic.[48]
Homo sapiens is the only extant species of its genus, Homo. While some (extinct) Homo species might have been ancestors of Homo sapiens, many, perhaps most, were likely "cousins", having speciated away from the ancestral hominin line.[50][51] There is yet no consensus as to which of these groups should be considered a separate species and which should be subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the genus Homo.[51] The Sahara pump theory (describing an occasionally passable "wet" Sahara desert) provides one possible explanation of the intermittent migration and speciation in the genus Homo.
Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices[52] of various Homo species and to study the role of diet in physical and behavioral evolution within Homo.[53][54][55][56][57]
Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatra in Indonesia some 70,000 years ago caused global starvation,[58] killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today.[59] The genetic and archaeological evidence for this remains in question however.[60] A 2023 genetic study suggests that a similar human population bottleneck of between 1,000 and 100,000 survivors occurred "around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction."[61][62]
Homo habilis lived from about 2.8[34] to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines with the development of smaller molars and larger brains. One of the first known hominins, it made tools from stone and perhaps animal bones, leading to its name homo habilis (Latin 'handy man') bestowed by discoverer Louis Leakey. Some scientists have proposed moving this species from Homo into Australopithecus due to the morphology of its skeleton being more adapted to living in trees rather than walking on two legs like later hominins.[63]
In May 2010, a new species, Homo gautengensis, was discovered in South Africa.[64]
These are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to Homo habilis is not yet clear.
The first fossils of Homo erectus were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material Anthropopithecus erectus (1892–1893, considered at this point as a chimpanzee-like fossil primate) and Pithecanthropus erectus (1893–1894, changing his mind as of based on its morphology, which he considered to be intermediate between that of humans and apes).[68] Years later, in the 20th century, the German physician and paleoanthropologist Franz Weidenreich (1873–1948) compared in detail the characters of Dubois' Java Man, then named Pithecanthropus erectus, with the characters of the Peking Man, then named Sinanthropus pekinensis. Weidenreich concluded in 1940 that because of their anatomical similarity with modern humans it was necessary to gather all these specimens of Java and China in a single species of the genus Homo, the species H. erectus.[69][70]
Homo erectus lived from about 1.8 Ma to about 70,000 years ago – which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby H. floresiensis survived it. The early phase of H. erectus, from 1.8 to 1.25 Ma, is considered by some to be a separate species, H. ergaster, or as H. erectus ergaster, a subspecies of H. erectus. Many paleoanthropologists now use the term Homo ergaster for the non-Asian forms of this group, and reserve H. erectus only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from H. ergaster.
In Africa in the Early Pleistocene, 1.5–1 Ma, some populations of Homo habilis are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, Homo erectus—in Africa.[71] This species also may have used fire to cook meat. Richard Wrangham notes that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, and "brains [swollen] to their current, horrendously fuel-inefficient size",[72] and hypothesizes that control of fire and cooking, which released increased nutritional value, was the key adaptation that separated Homo from tree-sleeping Australopithecines.[73]
These are proposed as species intermediate between H. erectus and H. heidelbergensis.
H. heidelbergensis ("Heidelberg Man") lived from about 800,000 to about 300,000 years ago. Also proposed as Homo sapiens heidelbergensis or Homo sapiens paleohungaricus.[77]
Homo neanderthalensis, alternatively designated as Homo sapiens neanderthalensis,[79] lived in Europe and Asia from 400,000[80] to about 28,000 years ago.[81] There are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal specimens, many relating to the superior Neanderthal adaptation to cold environments. Neanderthal surface to volume ratio was even lower than that among modern Inuit populations, indicating superior retention of body heat.
Neanderthals also had significantly larger brains, as shown from brain endocasts, casting doubt on their intellectual inferiority to modern humans. However, the higher body mass of Neanderthals may have required larger brain mass for body control.[82] Also, recent research by Pearce, Stringer, and Dunbar has shown important differences in brain architecture. The larger size of the Neanderthal orbital chamber and occipital lobe suggests that they had a better visual acuity than modern humans, useful in the dimmer light of glacial Europe.
Neanderthals may have had less brain capacity available for social functions. Inferring social group size from endocranial volume (minus occipital lobe size) suggests that Neanderthal groups may have been limited to 120 individuals, compared to 144[citation needed][83] possible relationships for modern humans. Larger social groups could imply that modern humans had less risk of inbreeding within their clan, trade over larger areas (confirmed in the distribution of stone tools), and faster spread of social and technological innovations. All these may have all contributed to modern Homo sapiens replacing Neanderthal populations by 28,000 BP.[82]
Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between H. neanderthalensis and H. sapiens, and that the two were separate species that shared a common ancestor about 660,000 years ago.[84][85][86] However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans c. 45,000-80,000 years ago, around the time modern humans migrated out from Africa, but before they dispersed throughout Europe, Asia and elsewhere.[87] The genetic sequencing of a 40,000-year-old human skeleton from Romania showed that 11% of its genome was Neanderthal, implying the individual had a Neanderthal ancestor 4–6 generations previously,[88] in addition to a contribution from earlier interbreeding in the Middle East. Though this interbred Romanian population seems not to have been ancestral to modern humans, the finding indicates that interbreeding happened repeatedly.[89]
All modern non-African humans have about 1% to 4% (or 1.5% to 2.6% by more recent data) of their DNA derived from Neanderthals.[90][87][91] This finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although this interpretation has been questioned.[92][93] Neanderthals and AMH Homo sapiens could have co-existed in Europe for as long as 10,000 years, during which AMH populations exploded, vastly outnumbering Neanderthals, possibly outcompeting them by sheer numbers.[94]
In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of another human species, the Denisovans.[95] Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.[44][96]
While the divergence point of the mtDNA was unexpectedly deep in time,[97] the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans.[44] Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years,[98] and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought.[96][99] Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.[100][101]
Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside Africa. Human leukocyte antigen (HLA) haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians,[46] indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a higher rate of depression.[102]
The flow of genes from Neanderthal populations to modern humans was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology reported in 2016 that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show more similarity to modern human genes than do European Neanderthal populations. This suggests Neanderthal populations interbred with modern humans around 100,000 years ago, probably somewhere in the Near East.[103]
Studies of a Neanderthal child at Gibraltar show from brain development and tooth eruption that Neanderthal children may have matured more rapidly than Homo sapiens.[104]
H. floresiensis, which lived from approximately 190,000 to 50,000 years before present (BP), has been nicknamed the hobbit for its small size, possibly a result of insular dwarfism.[105] H. floresiensis is intriguing both for its size and its age, being an example of a recent species of the genus Homo that exhibits derived traits not shared with modern humans. In other words, H. floresiensis shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm3 (considered small for a chimpanzee and less than a third of the H. sapiens average of 1400 cm3).[105]
However, there is an ongoing debate over whether H. floresiensis is indeed a separate species.[106] Some scientists hold that H. floresiensis was a modern H. sapiens with pathological dwarfism.[107] This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on H. floresiensis as a separate species is that it was found with tools only associated with H. sapiens.[107]
The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual.[106]
In 2016, fossil teeth and a partial jaw from hominins assumed to be ancestral to H. floresiensis were discovered[108] at Mata Menge, about 74 km (46 mi) from Liang Bua. They date to about 700,000 years ago[109] and are noted by Australian archaeologist Gerrit van den Bergh for being even smaller than the later fossils.[110]
A small number of specimens from the island of Luzon, dated 50,000 to 67,000 years ago, have recently been assigned by their discoverers, based on dental characteristics, to a novel human species, H. luzonensis.[111]
H. sapiens (the adjective sapiens is Latin for "wise" or "intelligent") emerged in Africa around 300,000 years ago, likely derived from H. heidelbergensis or a related lineage.[112][113] In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to modern humans (H. sapiens), representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.[114][115]
Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from H. erectus to H. sapiens. The direct evidence suggests there was a migration of H. erectus out of Africa, then a further speciation of H. sapiens from H. erectus in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed H. erectus. This migration and origin theory is usually referred to as the "recent single-origin hypothesis" or "out of Africa" theory. H. sapiens interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.[44][100]
The Toba catastrophe theory, which postulates a population bottleneck for H. sapiens about 70,000 years ago,[116] was controversial from its first proposal in the 1990s and by the 2010s had very little support.[117] Distinctive human genetic variability has arisen as the result of the founder effect, by archaic admixture and by recent evolutionary pressures.
Since Homo sapiens separated from its last common ancestor shared with chimpanzees, human evolution is characterized by a number of morphological, developmental, physiological, behavioral, and environmental changes.[9] Environmental (cultural) evolution discovered much later during the Pleistocene played a significant role in human evolution observed via human transitions between subsistence systems.[118][9] The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate.[119] Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in H. erectus.[120]
Bipedalism (walking on two legs) is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either Sahelanthropus[121] or Orrorin, both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorillas and chimpanzees, diverged from the hominin line over a period covering the same time, so either Sahelanthropus or Orrorin may be our last shared ancestor. Ardipithecus, a full biped, arose approximately 5.6 million years ago.[122]
The early bipeds eventually evolved into the australopithecines and still later into the genus Homo. There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion,[123] enabled long-distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat.[123][124][125] A 2007 study provides support for the hypothesis that bipedalism evolved because it used less energy than quadrupedal knuckle-walking.[126][127] However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal.[128] This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in habilines.
Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull.[129] The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.[130]
The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking;[28] bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, thus permitting the passage of newborns due to the increase in cranial size. This is limited to the upper portion, since further increase can hinder normal bipedal movement.[131]
The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth, which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit.[132] The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age.[125] The increased brain growth after birth and the increased dependency of children on mothers had a major effect upon the female reproductive cycle,[133] and the more frequent appearance of alloparenting in humans when compared with other hominids.[134] Delayed human sexual maturity also led to the evolution of menopause with one explanation, the grandmother hypothesis, providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.[135][136]
The human species eventually developed a much larger brain than that of other primates—typically 1,330 cm3 (81 cu in) in modern humans, nearly three times the size of a chimpanzee or gorilla brain.[139] After a period of stasis with Australopithecus anamensis and Ardipithecus, species which had smaller brains as a result of their bipedal locomotion,[140] the pattern of encephalization started with Homo habilis, whose 600 cm3 (37 cu in) brain was slightly larger than that of chimpanzees. This evolution continued in Homo erectus with 800–1,100 cm3 (49–67 cu in), and reached a maximum in Neanderthals with 1,200–1,900 cm3 (73–116 cu in), larger even than modern Homo sapiens. This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago. Encephalization may be due to a dependency on calorie-dense, difficult-to-acquire food.[141]
Furthermore, the changes in the structure of human brains may be even more significant than the increase in size.[142][143][144][53] Fossilized skulls shows the brain size in early humans fell within the range of modern humans 300,000 years ago, but only got its present-day brain shape between 100,000 and 35,000 years ago.[145]
The temporal lobes, which contain centers for language processing, have increased disproportionately, as has the prefrontal cortex, which has been related to complex decision-making and moderating social behavior.[139] Encephalization has been tied to increased starches[52] and meat[146][147] in the diet, however a 2022 meta study called into question the role of meat.[148] Other factors are the development of cooking,[149] and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex.[150] Changes in skull morphology, such as smaller mandibles and mandible muscle attachments, allowed more room for the brain to grow.[151]
The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.[152][153][154][155]
The immediate survival advantage of encephalization is difficult to discern, as the major brain changes from Homo erectus to Homo heidelbergensis were not accompanied by major changes in technology. It has been suggested that the changes were mainly social and behavioural, including increased empathic abilities,[156][157] increases in size of social groups,[150][158][159] and increased behavioral plasticity.[160] Humans are unique in the ability to acquire information through social transmission and adapt that information.[161] The emerging field of cultural evolution studies human sociocultural change from an evolutionary perspective.[162]
The reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans are the only hominoids in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling or overt changes in proceptivity during estrus).[175]
Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females.[176] These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.[177]
The ulnar opposition—the contact between the thumb and the tip of the little finger of the same hand—is unique to the genus Homo,[178] including Neanderthals, the Sima de los Huesos hominins and anatomically modern humans.[179][180] In other primates, the thumb is short and unable to touch the little finger.[179] The ulnar opposition facilitates the precision grip and power grip of the human hand, underlying all the skilled manipulations.
A number of other changes have also characterized the evolution of humans, among them an increased reliance on vision rather than smell (highly reduced olfactory bulb); a longer juvenile developmental period and higher infant dependency;[181] a smaller gut and small, misaligned teeth; faster basal metabolism;[182] loss of body hair;[183] an increase in eccrine sweat gland density that is ten times higher than any other catarrhinian primates,[184] yet humans use 30% to 50% less water per day compared to chimps and gorillas;[185] more REM sleep but less sleep in total;[186] a change in the shape of the dental arcade from u-shaped to parabolic; development of a chin (found in Homo sapiens alone); styloid processes; and a descended larynx. As the human hand and arms adapted to the making of tools and were used less for climbing, the shoulder blades changed too. As a side effect, it allowed human ancestors to throw objects with greater force, speed and accuracy.[187]
The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain.[189] Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes, on average, about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption.[190] Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.[191]
Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts.[189] There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.[192]
Many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are flakes from West Turkana, Kenya, which date to 3.3 million years ago.[193] The next oldest stone tools are from Gona, Ethiopia, and are considered the beginning of the Oldowan technology. These tools date to about 2.6 million years ago.[194] A Homo fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the Homo species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence.[195] The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from other apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.[196]
Bernard Wood noted that Paranthropus co-existed with the early Homo species in the area of the "Oldowan Industrial Complex" over roughly the same span of time. Although there is no direct evidence which identifies Paranthropus as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early Homo species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, Homo was always present, but Paranthropus was not.[195]
In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the Homo and Paranthropus species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.[195]
Anthropologists describe modern human behavior to include cultural and behavioral traits such as specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (such as grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks, as well as more general traits such as language and complex symbolic thinking. Debate continues as to whether a "revolution" led to modern humans ("big bang of human consciousness"), or whether the evolution was more gradual.[48]
Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase (H. habilis, H. ergaster, H. neanderthalensis) marked a new technology, followed by very slow development until the next phase. Currently paleoanthropologists are debating whether these Homo species possessed some or many modern human behaviors. They seem to have been culturally conservative, maintaining the same technologies and foraging patterns over very long periods.
Around 50,000 BP, human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a "Great Leap Forward",[197] or as the "Upper Palaeolithic Revolution",[198] due to the sudden appearance in the archaeological record of distinctive signs of modern behavior and big game hunting.[199] Evidence of behavioral modernity significantly earlier also exists from Africa, with older evidence of abstract imagery, widened subsistence strategies, more sophisticated tools and weapons, and other "modern" behaviors, and many scholars have recently argued that the transition to modernity occurred sooner than previously believed.[48][200][201][202]
Other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African Homo sapiens 300,000–200,000 years ago.[203][204][205][206][207] Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a 160 km (99 mi) sea journey 60,000 years ago, which may diminish the significance of the Upper Paleolithic Revolution.[208]
Modern humans started burying their dead, making clothing from animal hides, hunting with more sophisticated techniques (such as using pit traps or driving animals off cliffs), and cave painting.[209] As human culture advanced, different populations innovated existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of cultural variation, which had not been seen prior to 50,000 BP. Typically, the older H. neanderthalensis populations did not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal imitations of H. sapiens Aurignacian technologies.[210]
Anatomically modern human populations continue to evolve, as they are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in the modern age, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urbanization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations,[211] and more recent research indicates that for some traits, the developments and innovations of human culture have driven a new form of selection that coexists with, and in some cases has largely replaced, natural selection.[212]
Particularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes.
Other evolution is related to endemic diseases: the presence of malaria selects for sickle cell trait (the heterozygous form of sickle cell gene), while in the absence of malaria, the health effects of sickle-cell anemia select against this trait. For another example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons.[214][215] Some reported trends remain unexplained and the subject of ongoing research in the novel field of evolutionary medicine: polycystic ovary syndrome (PCOS) reduces fertility and thus is expected to be subject to extremely strong negative selection, but its relative commonality in human populations suggests a counteracting selection pressure. The identity of that pressure remains the subject of some debate.[216]
Recent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals,[217] as well as changes in metabolism due to changes in diet, such as lactase persistence.
Culturally-driven evolution can defy the expectations of natural selection: while human populations experience some pressure that drives a selection for producing children at younger ages, the advent of effective contraception, higher education, and changing social norms have driven the observed selection in the opposite direction.[218] However, culturally-driven selection need not necessarily work counter or in opposition to natural selection: some proposals to explain the high rate of recent human brain expansion indicate a kind of feedback whereupon the brain's increased social learning efficiency encourages cultural developments that in turn encourage more efficiency, which drive more complex cultural developments that demand still-greater efficiency, and so forth.[219] Culturally-driven evolution has an advantage in that in addition to the genetic effects, it can be observed also in the archaeological record: the development of stone tools across the Palaeolithic period connects to culturally-driven cognitive development in the form of skill acquisition supported by the culture and the development of increasingly complex technologies and the cognitive ability to elaborate them.[220]
In contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later.[221] Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.[221]
The name Homo of the biological genus to which humans belong is Latin for 'human'.[e] It was chosen originally by Carl Linnaeus in his classification system.[f] The English word human is from the Latin humanus, the adjectival form of homo. The Latin homo derives from the Indo-European root *dhghem, or 'earth'.[222] Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.[223]
The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's On the Origin of Species, in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that "Light will be thrown on the origin of man and his history."[224]
The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and other apes, and did so particularly in his 1863 book Evidence as to Man's Place in Nature. Many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans in his 1871 book The Descent of Man, and Selection in Relation to Sex.[225]
A major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of On the Origin of Species, and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were the remains of a modern human who had suffered some kind of illness.[226] Despite the 1891 discovery by Eugène Dubois of what is now called Homo erectus at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate.[227] In 1925, Raymond Dart described Australopithecus africanus.[228] The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain.
Although the brain was small (410 cm3), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.
During the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. These searches were carried out by the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave, fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and Homo species, and even H. erectus.
These finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after "Lucy", the most complete fossil member of the species Australopithecus afarensis, was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect.[229] Lucy was classified as a new species, Australopithecus afarensis, which is thought to be more closely related to the genus Homo as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range.[230] (The specimen was nicknamed "Lucy" after the Beatles' song "Lucy in the Sky with Diamonds", which was played loudly and repeatedly in the camp during the excavations.)[231] The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including Ardipithecus ramidus and A. kadabba.[232]
In 2013, fossil skeletons of Homo naledi, an extinct species of hominin assigned (provisionally) to the genus Homo, were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg.[233][234] As of September 2015[update], fossils of at least fifteen individuals, amounting to 1,550 specimens, have been excavated from the cave.[234] The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to Australopithecus, and a cranial morphology (skull shape) similar to early Homo species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils were dated close to 250,000 years ago,[235] and thus are not ancestral but contemporary with the first appearance of larger-brained anatomically modern humans.[236]
The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas).[237] The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.
In their seminal 1967 paper in Science, Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago,[237] at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably "Lucy", and reinterpretation of older fossil materials, notably Ramapithecus, showed the younger estimates to be correct and validated the albumin method.
Progress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins.[124][238][239] Application of the molecular clock principle revolutionized the study of molecular evolution.
On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimpanzees noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimpanzees to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimpanzee populations in eight locations suggests that chimpanzees reproduce at age 26.5 years on average; which suggests the human divergence from chimpanzees occurred between 7 and 13 mya. And these data suggest that Ardipithecus (4.5 Ma), Orrorin (6 Ma) and Sahelanthropus (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region.
Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between "proto-human" and "proto-chimpanzees" nonetheless occurred regularly enough to change certain genes in the new gene pool:
In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered Australopithecus anamensis. The find was overshadowed by Tim D. White's 1995 discovery of Ardipithecus ramidus, which pushed back the fossil record to 4.2 million years ago.
In 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named Orrorin tugenensis. And in 2001, a team led by Michel Brunet discovered the skull of Sahelanthropus tchadensis which was dated as 7.2 million years ago, and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin (cf Hominidae; terms "hominids" and hominins).
Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the genus Homo. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that the genus Homo have migrated out of Africa at least three and possibly four times (e.g. Homo erectus, Homo heidelbergensis and two or three times for Homo sapiens). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.[245]
Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artifacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus Homo at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago.[246] This suggests that the Asian "Chopper" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.
Up until the genetic evidence became available, there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that the genus Homo contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple of million years. This model was proposed in 1988 by Milford H. Wolpoff.[247][248] In contrast, the "out of Africa" model proposed that modern H. sapiens speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in the nearly complete replacement of other Homo species. This model has been developed by Chris Stringer and Peter Andrews.[249][250]
Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the "out of Africa" theory and weakened the views of multiregional evolutionism.[251] Aligned in genetic tree differences were interpreted as supportive of a recent single origin.[252]
"Out of Africa" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. "Out of Africa" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.[253]
A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 "ancestral population clusters". The research also located a possible origin of modern human migration in southwestern Africa, near the coastal border of Namibia and Angola.[254] The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared.[255] Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin.[256] All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.[257]
Recent sequencing of Neanderthal[90] and Denisovan[44] genomes shows that some admixture with these populations has occurred. All modern human groups outside Africa have 1–4% or (according to more recent research) about 1.5–2.6% Neanderthal alleles in their genome,[91] and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the "out of Africa" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that some researchers speculate might be linked to the Toba supervolcano catastrophe, a fairly small group left Africa and interbred with Neanderthals, probably in the Middle East, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in southeastern Asia, before populating Melanesia.[100] HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations.[46] The Denisovan EPAS1 gene has also been found in Tibetan populations.[258] Studies of the human genome using machine learning have identified additional genetic contributions in Eurasians from an "unknown" ancestral population potentially related to the Neanderthal-Denisovan lineage.[259]
There are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory,[260][261][262] which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant.[260] This group seems to have been dependent upon marine resources for their survival.
Stephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated "big game hunting" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum.[260] The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.[263]
On the basis of the early date of Badoshan Iranian Aurignacian, Oppenheimer suggests that this second dispersal may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.[199]
Recent genetic evidence suggests that all modern non-African populations, including those of Eurasia and Oceania, are descended from a single wave that left Africa between 65,000 and 50,000 years ago.[264][265][266]
The evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.[267]
The closest living relatives of humans are bonobos and chimpanzees (both genus Pan) and gorillas (genus Gorilla).[268] With the sequencing of both the human and chimpanzee genome, as of 2012[update] estimates of the similarity between their DNA sequences range between 95% and 99%.[268][269][31] It is also noteworthy that mice share around 97.5% of their working DNA with humans.[270] By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.
The gibbons (family Hylobatidae) and then the orangutans (genus Pongo) were the first groups to split from the line leading to the hominins, including humans—followed by gorillas (genus Gorilla), and, ultimately, by the chimpanzees (genus Pan). The splitting date between hominin and chimpanzee lineages is placed by some between 4 to 8 million years ago, that is, during the Late Miocene.[271][272][273][274] Speciation, however, appears to have been unusually drawn out. Initial divergence occurred sometime between 7 to 13 million years ago, but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at 5 to 6 million years ago.[275]
Genetic evidence has also been employed to compare species within the genus Homo, investigating gene flow between early modern humans and Neanderthals, and to enhance the understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.
Each time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants, a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.
Human evolutionary genetics studies how human genomes differ among individuals, the evolutionary past that gave rise to them, and their current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.
In May 2023, scientists reported a more complicated pathway of human evolution than previously understood. According to the studies, humans evolved from different places and times in Africa, instead of from a single location and period of time.[276][277]
There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages.[278] The earliest fossils that have been proposed as members of the hominin lineage are Sahelanthropus tchadensis dating from 7 million years ago, Orrorin tugenensis dating from 5.7 million years ago, and Ardipithecus kadabba dating to 5.6 million years ago. Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.
The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around 4 million years ago and diverged into robust (also called Paranthropus) and gracile branches, one of which (possibly A. garhi) probably went on to become ancestors of the genus Homo. The australopithecine species that is best represented in the fossil record is Australopithecus afarensis with more than 100 fossil individuals represented, found from Northern Ethiopia (such as the famous "Lucy"), to Kenya, and South Africa. Fossils of robust australopithecines such as A. robustus (or alternatively Paranthropus robustus) and A./P. boisei are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.
The earliest member of the genus Homo is Homo habilis which evolved around 2.8 million years ago.[34] H. habilis is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider Homo rudolfensis, a larger bodied group of fossils with similar morphology to the original H. habilis fossils, to be a separate species, while others consider them to be part of H. habilis—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.
During the next million years, a process of encephalization began and, by the arrival (about 1.9 million years ago) of H. erectus in the fossil record, cranial capacity had doubled. H. erectus were the first of the hominins to emigrate from Africa, and, from 1.8 to 1.3 million years ago, this species spread through Africa, Asia, and Europe. One population of H. erectus, also sometimes classified as separate species H. ergaster, remained in Africa and evolved into H. sapiens. It is believed that H. erectus and H. ergaster were the first to use fire and complex tools. In Eurasia, H. erectus evolved into species such as H. antecessor, H. heidelbergensis and H. neanderthalensis. The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 300–200,000 years ago such as the Herto and Omo remains of Ethiopia, Jebel Irhoud remains of Morocco, and Florisbad remains of South Africa; later fossils from the Skhul Cave in Israel and Southern Europe begin around 90,000 years ago (0.09 million years ago).
As modern humans spread out from Africa, they encountered other hominins such as H. neanderthalensis and the Denisovans, who may have evolved from populations of H. erectus that had left Africa around 2 million years ago. The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.[279][280]
This migration out of Africa is estimated to have begun about 70–50,000 years BP and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.[281]
The hypothesis of interbreeding, also known as hybridization, admixture or hybrid-origin theory, has been discussed ever since the discovery of Neanderthal remains in the 19th century.[282] The linear view of human evolution began to be abandoned in the 1970s as different species of humans were discovered that made the linear concept increasingly unlikely. In the 21st century with the advent of molecular biology techniques and computerization, whole-genome sequencing of Neanderthal and human genome were performed, confirming recent admixture between different human species.[90] In 2010, evidence based on molecular biology was published, revealing unambiguous examples of interbreeding between archaic and modern humans during the Middle Paleolithic and early Upper Paleolithic. It has been demonstrated that interbreeding happened in several independent events that included Neanderthals and Denisovans, as well as several unidentified hominins.[283] Today, approximately 2% of DNA from all non-African populations (including Europeans, Asians, and Oceanians) is Neanderthal,[90] with traces of Denisovan heritage.[284] Also, 4–6% of modern Melanesian genetics are Denisovan.[284] Comparisons of the human genome to the genomes of Neandertals, Denisovans and apes can help identify features that set modern humans apart from other hominin species. In a 2016 comparative genomics study, a Harvard Medical School/UCLA research team made a world map on the distribution and made some predictions about where Denisovan and Neanderthal genes may be impacting modern human biology.[285][286]
For example, comparative studies in the mid-2010s found several traits related to neurological, immunological,[287] developmental, and metabolic phenotypes, that were developed by archaic humans to European and Asian environments and inherited to modern humans through admixture with local hominins.[288][289]
Although the narratives of human evolution are often contentious, several discoveries since 2010 show that human evolution should not be seen as a simple linear or branched progression, but a mix of related species.[44][5][6][7] In fact, genomic research has shown that hybridization between substantially diverged lineages is the rule, not the exception, in human evolution.[4] Furthermore, it is argued that hybridization was an essential creative force in the emergence of modern humans.[4]
Stone tools are first attested around 2.6 million years ago, when hominins in Eastern Africa used so-called core tools, choppers made out of round cores that had been split by simple strikes.[290] This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000–10,000 years ago.
Archaeologists working in the Great Rift Valley in Kenya have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.[193][291][292][293]
The period from 700,000 to 300,000 years ago is also known as the Acheulean, when H. ergaster (or erectus) made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later "retouched" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers ("racloirs"), needles, and flattened needles were made.[290] Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). Bone tools were also made by H. sapiens in Africa by 90,000–70,000 years ago[200][294] and are also known from early H. sapiens sites in Eurasia by about 50,000 years ago.
This list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus Homo. Please see articles for more information.

---

# Adaptive evolution in the human genome

Adaptive evolution results from the propagation of advantageous mutations through positive selection. This is the modern synthesis of the process which Darwin and Wallace originally identified as the mechanism of evolution. However, in the last half century, there has been considerable debate as to whether evolutionary changes at the molecular level are largely driven by natural selection or random genetic drift. Unsurprisingly, the forces which drive evolutionary changes in our own species’ lineage have been of particular interest. Quantifying adaptive evolution in the human genome gives insights into our own evolutionary history and helps to resolve this neutralist-selectionist debate. Identifying specific regions of the human genome that show evidence of adaptive evolution helps us find functionally significant genes, including genes important for human health, such as those associated with diseases.
The methods used to identify adaptive evolution are generally devised to test the null hypothesis of neutral evolution, which, if rejected, provides evidence of adaptive evolution. These tests can be broadly divided into two categories.
Firstly, there are methods that use a comparative approach to search for evidence of function-altering mutations. The dN/dS rates-ratio test estimates ω, the rates at which nonsynonymous ('dN') and synonymous ('dS') nucleotide substitutions occur ('synonymous' nucleotide substitutions do not lead to a change in the coding amino acid, while 'nonsynonymous' ones do). In this model, neutral evolution is considered the null hypothesis, in which dN and dS approximately balance so that ω ≈ 1. The two alternative hypotheses are a relative absence of nonsynonymous substitutions (dN < dS; ω < 1), suggesting the effect on fitness ('fitness effect', or 'selection pressure') of such mutations is negative (purifying selection has operated over time); or a relative excess of nonsynonymous substitutions (dN > dS; ω > 1), indicating positive effect on fitness, i.e. diversifying selection (Yang and Bielawski 2000).
The McDonald–Kreitman (MK) test quantifies the amount of adaptive evolution occurring by estimating the proportion of nonsynonymous substitutions which are adaptive, referred to as α (McDonald and Kreitman 1991, Eyre-Walker 2006). α is calculated as: α = 1-(dspn/dnps), where dn and ds are as above, and pn and ps are the number of nonsynonymous (fitness effect assumed neutral or deleterious) and synonymous (fitness effect assumed neutral) polymorphisms respectively (Eyre-Walker 2006).
Note, both these tests are presented here in basic forms, and these tests are normally modified considerably to account for other factors, such as the effect of slightly deleterious mutations.
The other methods for detecting adaptive evolution use genome wide approaches, often to look for evidence of selective sweeps. Evidence of complete selective sweeps is shown by a decrease in genetic diversity, and can be inferred from comparing the patterns of the Site Frequency Spectrum (SFS, i.e. the allele frequency distribution) obtained with the SFS expected under a neutral model (Willamson et al. 2007). Partial selective sweeps provide evidence of the most recent adaptive evolution, and the methods identify adaptive evolution by searching for regions with a high proportion of derived alleles (Sabeti et al. 2006).
Examining patterns of Linkage Disequilibrium (LD) can locate signatures of adaptive evolution (Hawks et al. 2007, Voight et al. 2006). LD tests work on the basic principle that, assuming equal recombination rates, LD will rise with increasing natural selection. These genomic methods can also be applied to search for adaptive evolution in non-coding DNA, where putatively neutral sites are hard to identify (Ponting and Lunter 2006).
Another recent method used to detect selection in non-coding sequences examines insertions and deletions (indels), rather than point mutations (Lunter et al. 2006), although the method has only been applied to examine patterns of negative selection.
Many different studies have attempted to quantify the amount of adaptive evolution in the human genome, the vast majority using the comparative approaches outlined above. Although there are discrepancies between studies, generally there is relatively little evidence of adaptive evolution in protein coding DNA, with estimates of adaptive evolution often near 0% (see Table 1). The most obvious exception to this is the 35% estimate of α (Fay et al. 2001). This comparatively early study used relatively few loci (fewer than 200) for their estimate, and the polymorphism and divergence data used was obtained from different genes, both of which may have led to an overestimate of α. The next highest estimate is the 20% value of α (Zhang and Li 2005). However, the MK test used in this study was sufficiently weak that the authors state that this value of α is not statistically significantly different from 0%. Nielsen et al. (2005a)’s estimate that 9.8% of genes have undergone adaptive evolution also has a large margin of error associated with it, and their estimate shrinks dramatically to 0.4% when they stipulate that the degree of certainty that there has been adaptive evolution must be 95% or more.
This raises an important issue, which is that many of these tests for adaptive evolution are very weak. Therefore, the fact that many estimates are at (or very near to) 0% does not rule out the occurrence of any adaptive evolution in the human genome, but simply shows that positive selection is not frequent enough to be detected by the tests. In fact, the most recent study mentioned states that confounding variables, such as demographic changes, mean that the true value of α may be as high as 40% (Eyre-Walker and Keightley 2009). Another recent study, which uses a relatively robust methodology, estimates α at 10–20% (Boyko et al. 2008). Clearly, the debate over the amount of adaptive evolution occurring in human coding DNA is not yet resolved.
Even if low estimates of α are accurate, a small proportion of substitutions evolving adaptively can still equate to a considerable amount of coding DNA. Many authors, whose studies have small estimates of the amount of adaptive evolution in coding DNA, nevertheless accept that there has been some adaptive evolution in this DNA, because these studies identify specific regions within the human genome which have been evolving adaptively (e.g. Bakewell et al. (2007)). More genes underwent positive selection in chimpanzee evolution than in human.
The generally low estimates of adaptive evolution in human coding DNA can be contrasted with other species. Bakewell et al. (2007) found more evidence of adaptive evolution in chimpanzees than humans, with 1.7% of chimpanzee genes showing evidence of adaptive evolution (compared with the 1.1% estimate for humans; see Table 1). Comparing humans with more distantly related animals, an early estimate for α in Drosophila species was 45% (Smith and Eyre-Walker 2002), and later estimates largely agree with this (Eyre-Walker 2006). Bacteria and viruses generally show even more evidence of adaptive evolution; research shows values of α in a range of 50–85%, depending on the species examined (Eyre-Walker 2006). Generally, there does appear to be a positive correlation between (effective) population size of the species, and amount of adaptive evolution occurring in the coding DNA regions. This may be because random genetic drift becomes less powerful at altering allele frequencies, compared to natural selection, as population size increases.
Estimates of the amount of adaptive evolution in non-coding DNA are generally very low, although fewer studies have been done on non-coding DNA. As with the coding DNA however, the methods currently used are relatively weak. Ponting and Lunter (2006) speculate that underestimates may be even more severe in non-coding DNA, because non-coding DNA may undergo periods of functionality (and adaptive evolution), followed by periods of neutrality. If this is true, current methods for detecting adaptive evolution are inadequate to account for such patterns. Additionally, even if low estimates of the amount of adaptive evolution are correct, this can still equate to a large amount of adaptively evolving non-coding DNA, since non-coding DNA makes up approximately 98% of the DNA in the human genome. For example, Ponting and Lunter (2006) detect a modest 0.03% of non-coding DNA showing evidence of adaptive evolution, but this still equates to approximately 1 Mb of adaptively evolving DNA. Where there is evidence of adaptive evolution (which implies functionality) in non-coding DNA, these regions are generally thought to be involved in the regulation of protein coding sequences.
As with humans, fewer studies have searched for adaptive evolution in non-coding regions of other organisms. However, where research has been done on Drosophila, there appears to be large amounts of adaptively evolving non-coding DNA. Andolfatto (2005) estimated that adaptive evolution has occurred in 60% of untranslated mature portions of mRNAs, and in 20% of intronic and intergenic regions. If this is true, this would imply that much non-coding DNA could be of more functional importance than coding DNA, dramatically altering the consensus view. However, this would still leave unanswered what function all this non-coding DNA performs, as the regulatory activity observed thus far is in just a tiny proportion of the total amount of non-coding DNA. Ultimately, significantly more evidence needs to be gathered to substantiate this viewpoint.
Several recent studies have compared the amounts of adaptive evolution occurring between different populations within the human species. Williamson et al. (2007) found more evidence of adaptive evolution in European and Asian populations than African American populations. Assuming African Americans are representative of Africans, these results makes sense intuitively, because humans spread out of Africa approximately 50,000 years ago (according to the consensus Out-of-Africa hypothesis of human origins (Klein 2009)), and these humans would have adapted to the new environments they encountered. By contrast, African populations remained in a similar environment for the following tens of thousands of years, and were therefore probably nearer their adaptive peak for the environment. However, Voight et al. (2006) found evidence of more adaptive evolution in Africans, than in Non-Africans (East Asian and European populations examined), and Boyko et al. (2008) found no significant difference in the amount of adaptive evolution occurring between different human populations. Therefore, the evidence obtained so far is inconclusive as to what extent different human populations have undergone different amounts of adaptive evolution.
The rate of adaptive evolution in the human genome has often been assumed to be constant over time. For example, the 35% estimate for α calculated by Fay et al. (2001) led them to conclude that there was one adaptive substitution in the human lineage every 200 years since human divergence from old-world monkeys. However, even if the original value of α is accurate for a particular time period, this extrapolation is still invalid. This is because there has been a large acceleration in the amount of positive selection in the human lineage over the last 40,000 years, in terms of the number of genes that have undergone adaptive evolution (Hawks et al. 2007). This agrees with simple theoretical predictions, because the human population size has expanded dramatically in the last 40,000 years, and with more people, there should be more adaptive substitutions. Hawks et al. (2007) argue that demographic changes (particularly population expansion) may greatly facilitate adaptive evolution, an argument that somewhat corroborates the positive correlation inferred between population size and amount of adaptive evolution occurring mentioned previously.
It has been suggested that cultural evolution may have replaced genetic evolution, and hence slowed the rate of adaptive evolution over the past 10,000 years. However, it is possible that cultural evolution could actually increase genetic adaptation. Cultural evolution has vastly increased communication and contact between different populations, and this provides much greater opportunities for genetic admixture between the different populations (Hawks et al. 2007). However, recent cultural phenomena, such as modern medicine and the smaller variation in modern family sizes, may reduce genetic adaptation as natural selection is relaxed, overriding the increased potential for adaptation due to greater genetic admixture.
Studies generally do not attempt to quantify the average strength of selection propagating advantageous mutations in the human genome. Many models make assumptions about how strong selection is, and some of the discrepancies between the estimates of the amounts of adaptive evolution occurring have been attributed to the use of such differing assumptions (Eyre-Walker 2006). The way to accurately estimate the average strength of positive selection acting on the human genome is by inferring the distribution of fitness effects (DFE) of new advantageous mutations in the human genome, but this DFE is difficult to infer because new advantageous mutations are very rare (Boyko et al. 2008). The DFE may be exponential shaped in an adapted population (Eyre-Walker and Keightley 2007). However, more research is required to produce more accurate estimates of the average strength of positive selection in humans, which will in turn improve the estimates of the amount of adaptive evolution occurring in the human genome (Boyko et al. 2008).
A considerable number of studies have used genomic methods to identify specific human genes that show evidence of adaptive evolution. Table 2 gives selected examples of such genes for each gene type discussed, but provides nowhere near an exhaustive list of the human genes showing evidence of adaptive evolution. Below are listed some of the types of gene which show strong evidence of adaptive evolution in the human genome.
Bakewell et al. (2007) found that a relatively large proportion (9.7%) of positively selected genes were associated with diseases. This may be because diseases can be adaptive in some contexts. For example, schizophrenia has been linked with increased creativity (Crespi et al. 2007), perhaps a useful trait for obtaining food or attracting mates in Palaeolithic times. Alternatively, the adaptive mutations may be the ones which reduce the chance of disease arising due to other mutations. However, this second explanation seems unlikely, because the mutation rate in the human genome is fairly low, so selection would be relatively weak.
417 genes involved in the immune system showed strong evidence of adaptive evolution in the study of Nielsen et al. (2005a). This is probably because the immune genes may become involved in an evolutionary arms race with bacteria and viruses (Daugherty and Malik 2012; Van der Lee et al. 2017). These pathogens evolve very rapidly, so selection pressures change quickly, giving more opportunity for adaptive evolution.
247 genes in the testes showed evidence of adaptive evolution in the study of Nielsen et al. (2005a). This could be partially due to sexual antagonism. Male–female competition could facilitate an arms race of adaptive evolution. However, in this situation you would expect to find evidence of adaptive evolution in the female sexual organs also, but there is less evidence of this. Sperm competition is another possible explanation. Sperm competition is strong, and sperm can improve their chances of fertilising the female egg in a variety of ways, including increasing their speed, stamina or response to chemoattractants (Swanson and Vacquier 2002).
Genes involved in detecting smell show strong evidence of adaptive evolution (Voight et al. 2006), probably due to the fact that the smells encountered by humans have changed recently in their evolutionary history (Williamson et al. 2007). Humans’ sense of smell has played an important role in determining the safety of food sources.
Genes involved in lactose metabolism show particularly strong evidence of adaptive evolution amongst the genes involved in nutrition. A mutation linked to lactase persistence shows very strong evidence of adaptive evolution in European and American populations (Williamson et al. 2007), populations where pastoral farming for milk has been historically important.
Pigmentation genes show particularly strong evidence of adaptive evolution in non-African populations (Williamson et al. 2007). This is likely to be because those humans that left Africa approximately 50,000 years ago, entered less sunny climates, and so were under new selection pressures to obtain enough Vitamin D from the weakened sunlight.
There is some evidence of adaptive evolution in genes linked to brain development, but some of these genes are often associated with diseases, e.g. microcephaly (see Table 2). However, there is a particular interest in the search for adaptive evolution in brain genes, despite the ethical issues surrounding such research. If more adaptive evolution was discovered in brain genes in one human population than another, then this information could be interpreted as showing greater intelligence in the more adaptively evolved population.
Other gene types showing considerable evidence of adaptive evolution (but generally less evidence than the types discussed) include: genes on the X chromosome, nervous system genes, genes involved in apoptosis, genes coding for skeletal traits, and possibly genes associated with speech (Nielsen et al. 2005a, Williamson et al. 2007, Voight et al. 2006, Krause et al. 2007).
As noted previously, many of the tests used to detect adaptive evolution have very large degrees of uncertainty surrounding their estimates. While there are many different modifications applied to individual tests to overcome the associated problems, two types of confounding variables are particularly important in hindering the accurate detection of adaptive evolution: demographic changes and biased gene conversion.
Demographic changes are particularly problematic and may severely bias estimates of adaptive evolution. The human lineage has undergone both rapid population size contractions and expansions over its evolutionary history, and these events will change many of the signatures thought to be characteristic of adaptive evolution (Nielsen et al. 2007). Some genomic methods have been shown through simulations to be relatively robust to demographic changes (e.g. Willamson et al. 2007). However, no tests are completely robust to demographic changes, and new genetic phenomena linked to demographic changes have recently been discovered. This includes the concept of “surfing mutations”, where new mutations can be propagated with a population expansion (Klopfstein et al. 2006).
A phenomenon which could severely alter the way we look for signatures of adaptive evolution is biased gene conversion (BGC) (Galtier and Duret 2007). Meiotic recombination between homologous chromosomes that are heterozygous at a particular locus can produce a DNA mismatch. DNA repair mechanisms are biased towards repairing a mismatch to the CG base pair. This will lead allele frequencies to change, leaving a signature of non-neutral evolution (Galtier et al. 2001). The excess of AT to GC mutations in human genomic regions with high substitution rates (human accelerated regions, HARs) implies that BGC has occurred frequently in the human genome (Pollard et al. 2006, Galtier and Duret 2007). Initially, it was postulated that BGC could have been adaptive (Galtier et al. 2001), but more recent observations have made this seem unlikely. Firstly, some HARs show no substantial signs of selective sweeps around them. Secondly, HARs tend to be present in regions with high recombination rates (Pollard et al. 2006). In fact, BGC could lead to HARs containing a high frequency of deleterious mutations (Galtier and Duret 2007). However, it is unlikely that HARs are generally maladaptive, because DNA repair mechanisms themselves would be subject to strong selection if they propagated deleterious mutations. Either way, BGC should be further investigated, because it may force radical alteration of the methods which test for the presence of adaptive evolution.
(format of table and some data displayed as in Table 1 of Eyre-Walker (2006))

---

# Dual inheritance theory

Dual inheritance theory (DIT), also known as gene–culture coevolution or biocultural evolution,[1] was developed in the 1960s through early 1980s to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. Genes and culture continually interact in a feedback loop:[2] changes in genes can lead to changes in culture which can then influence genetic selection, and vice versa. One of the theory's central claims is that culture evolves partly through a Darwinian selection process, which dual inheritance theorists often describe by analogy to genetic evolution.[3]
'Culture', in this context, is defined as 'socially learned behavior', and 'social learning' is defined as copying behaviors observed in others or acquiring behaviors through being taught by others. Most of the modelling done in the field relies on the first dynamic (copying), though it can be extended to teaching. Social learning, at its simplest, involves blind copying of behaviors from a model (someone observed behaving), though it is also understood to have many potential biases, including success bias (copying from those who are perceived to be better off), status bias (copying from those with higher status), homophily (copying from those most like ourselves), conformist bias (disproportionately picking up behaviors that more people are performing), etc. Understanding social learning is a system of pattern replication, and understanding that there are different rates of survival for different socially learned cultural variants, this sets up, by definition, an evolutionary structure: cultural evolution.[4]
Because genetic evolution is relatively well understood, most of DIT examines cultural evolution and the interactions between cultural evolution and genetic evolution.
DIT holds that genetic and cultural evolution interacted in the evolution of Homo sapiens. DIT recognizes that the natural selection of genotypes is an important component of the evolution of human behavior and that cultural traits can be constrained by genetic imperatives. However, DIT also recognizes that genetic evolution has endowed the human species with a parallel evolutionary process of cultural evolution. DIT makes three main claims:[5]
The human capacity to store and transmit culture arose from genetically evolved psychological mechanisms. This implies that at some point during the evolution of the human species a type of social learning leading to cumulative cultural evolution was evolutionarily advantageous.
Social learning processes give rise to cultural evolution. Cultural traits are transmitted differently from genetic traits and, therefore, result in different population-level effects on behavioral variation.
Cultural traits alter the social and physical environments under which genetic selection operates. For example, the cultural adoptions of agriculture and dairying have, in humans, caused genetic selection for the traits to digest starch and lactose, respectively.[6][7][8][9][10][11] As another example, it is likely that once culture became adaptive, genetic selection caused a refinement of the cognitive architecture that stores and transmits cultural information. This refinement may have further influenced the way culture is stored and the biases that govern its transmission.
DIT also predicts that, under certain situations, cultural evolution may select for traits that are genetically maladaptive. An example of this is the demographic transition, which describes the fall of birth rates within industrialized societies. Dual inheritance theorists hypothesize that the demographic transition may be a result of a prestige bias, where individuals that forgo reproduction to gain more influence in industrial societies are more likely to be chosen as cultural models.[12][13]
People have defined the word "culture" to describe a large set of different phenomena.[14][15] A definition that sums up what is meant by "culture" in DIT is:
Culture is socially learned information stored in individuals' brains that is capable of affecting behavior.[16][17]
This view of culture emphasizes population thinking by focusing on the process by which culture is generated and maintained. It also views culture as a dynamic property of individuals, as opposed to a view of culture as a superorganic entity to which individuals must conform.[18] This view's main advantage is that it connects individual-level processes to population-level outcomes.[19]
Genes affect cultural evolution via psychological predispositions on cultural learning.[20] Genes encode much of the information needed to form the human brain. Genes constrain the brain's structure and, hence, the ability of the brain to acquire and store culture. Genes may also endow individuals with certain types of transmission bias (described below).
Culture can profoundly influence gene frequencies in a population.
One of the best known examples is the prevalence of the genotype for adult lactose absorption in human populations, such as Northern Europeans and some African societies, with a long history of raising cattle for milk. Until around 7,500 years ago,[21] lactase production stopped shortly after weaning,[22] and in societies which did not develop dairying, such as East Asians and Amerindians, this is still true today.[23][24] In areas with lactase persistence, it is believed that by domesticating animals, a source of milk became available while an adult and thus strong selection for lactase persistence could occur;[21][25] in a Scandinavian population, the estimated selection coefficient was 0.09-0.19.[25] This implies that the cultural practice of raising cattle first for meat and later for milk led to selection for genetic traits for lactose digestion.[26] Recently, analysis of natural selection on the human genome suggests that civilization has accelerated genetic change in humans over the past 10,000 years.[27]
Culture has driven changes to the human digestive systems making many digestive organs, such as teeth or stomach, smaller than expected for primates of a similar size,[28] and has been attributed to one of the reasons why humans have such large brains compared to other great apes.[29][30] This is due to food processing. Early examples of food processing include pounding, marinating and most notably cooking. Pounding meat breaks down the muscle fibres, hence taking away some of the job from the mouth, teeth and jaw.[31][32] Marinating emulates the action of the stomach with high acid levels. Cooking partially breaks down food making it more easily digestible. Food enters the body effectively partly digested, and as such food processing reduces the work that the digestive system has to do. This means that there is selection for smaller digestive organs as the tissue is energetically expensive,[28] those with smaller digestive organs can process their food but at a lower energetic cost than those with larger organs.[33] Cooking is notable because the energy available from food increases when cooked and this also means less time is spent looking for food.[29][34][35]
Humans living on cooked diets spend only a fraction of their day chewing compared to other extant primates living on raw diets. American girls and boys spent on average 7 to 8 percent of their day chewing respectively (1.68 to 1.92 hours per day), compared to chimpanzees, who spend more than 6 hours a day chewing.[36] This frees up time which can be used for hunting. A raw diet means hunting is constrained since time spent hunting is time not spent eating and chewing plant material, but cooking reduces the time required to get the day's energy requirements, allowing for more subsistence activities.[37] Digestibility of cooked carbohydrates is approximately on average 30% higher than digestibility of non-cooked carbohydrates.[34][38] This increased energy intake, more free time and savings made on tissue used in the digestive system allowed for the selection of genes for larger brain size.
Despite its benefits, brain tissue requires a large amount of calories, hence a main constraint in selection for larger brains is calorie intake. A greater calorie intake can support greater quantities of brain tissue. This is argued to explain why human brains can be much larger than other apes, since humans are the only ape to engage in food processing.[29] The cooking of food has influenced genes to the extent that, research suggests, humans cannot live without cooking.[39][29] A study on 513 individuals consuming long-term raw diets found that as the percentage of their diet which was made up of raw food and/or the length they had been on a diet of raw food increased, their BMI decreased.[39] This is despite access to many non-thermal processing, like grinding, pounding or heating to 48 °C. (118 °F).[39] With approximately 86 billion neurons in the human brain and 60–70 kg body mass, an exclusively raw diet close to that of what extant primates have would be not viable as, when modelled, it is argued that it would require an infeasible level of more than nine hours of feeding every day.[29] However, this is contested, with alternative modelling showing enough calories could be obtained within 5–6 hours per day.[40] Some scientists and anthropologists point to evidence that brain size in the Homo lineage started to increase well before the advent of cooking due to increased consumption of meat[28][40][41] and that basic food processing (slicing) accounts for the size reduction in organs related to chewing.[42] Cornélio et al. argues that improving cooperative abilities and a varying of diet to more meat and seeds improved foraging and hunting efficiency. It is this that allowed for the brain expansion, independent of cooking which they argue came much later, a consequence from the complex cognition that developed.[40] Yet this is still an example of a cultural shift in diet and the resulting genetic evolution. Further criticism comes from the controversy of the archaeological evidence available. Some claim there is a lack of evidence of fire control when brain sizes first started expanding.[40][43] Wrangham argues that anatomical evidence around the time of the origin of Homo erectus (1.8 million years ago), indicates that the control of fire and hence cooking occurred.[34] At this time, the largest reductions in tooth size in the entirety of human evolution occurred, indicating that softer foods became prevalent in the diet. Also at this time was a narrowing of the pelvis indicating a smaller gut and also there is evidence that there was a loss of the ability to climb which Wrangham argues indicates the control of fire, since sleeping on the ground needs fire to ward off predators.[44] The proposed increases in brain size from food processing will have led to a greater mental capacity for further cultural innovation in food processing which will have increased digestive efficiency further providing more energy for further gains in brain size.[45] This positive feedback loop is argued to have led to the rapid brain size increases seen in the Homo lineage.[45][40]
In DIT, the evolution and maintenance of cultures is described by five major mechanisms: natural selection of cultural variants, random variation, cultural drift, guided variation and transmission bias.
Differences between cultural phenomena result in differential rates of their spread; similarly, cultural differences among individuals can lead to differential survival and reproduction rates of individuals. The patterns of this selective process depend on transmission biases and can result in behavior that is more adaptive to a given environment.
Random variation arises from errors in the learning, display or recall of cultural information, and is roughly analogous to the process of mutation in genetic evolution.
Cultural drift is a process roughly analogous to genetic drift in evolutionary biology.[46][47][48] In cultural drift, the frequency of cultural traits in a population may be subject to random fluctuations due to chance variations in which traits are observed and transmitted (sometimes called "sampling error").[49] These fluctuations might cause cultural variants to disappear from a population. This effect should be especially strong in small populations.[50] A model by Hahn and Bentley shows that cultural drift gives a reasonably good approximation to changes in the popularity of American baby names.[49] Drift processes have also been suggested to explain changes in archaeological pottery and technology patent applications.[48] Changes in the songs of song birds are also thought to arise from drift processes, where distinct dialects in different groups occur due to errors in songbird singing and acquisition by successive generations.[51] Cultural drift is also observed in an early computer model of cultural evolution.[52]
Cultural traits may be gained in a population through the process of individual learning. Once an individual learns a novel trait, it can be transmitted to other members of the population. The process of guided variation depends on an adaptive standard that determines what cultural variants are learned.
Understanding the different ways that culture traits can be transmitted between individuals has been an important part of DIT research since the 1970s.[53][54] Transmission biases occur when some cultural variants are favored over others during the process of cultural transmission.[55] Boyd and Richerson (1985)[55] defined and analytically modeled a number of possible transmission biases. The list of biases has been refined over the years, especially by Henrich and McElreath.[56]
Content biases result from situations where some aspect of a cultural variant's content makes them more likely to be adopted.[57] Content biases can result from genetic preferences, preferences determined by existing cultural traits, or a combination of the two. For example, food preferences can result from genetic preferences for sugary or fatty foods and socially-learned eating practices and taboos.[57] Content biases are sometimes called "direct biases."[55]
Context biases result from individuals using clues about the social structure of their population to determine what cultural variants to adopt. This determination is made without reference to the content of the variant. There are two major categories of context biases: model-based biases, and frequency-dependent biases.
Model-based biases result when an individual is biased to choose a particular "cultural model" to imitate. There are four major categories of model-based biases: prestige bias, skill bias, success bias, and similarity bias.[5][58] A "prestige bias" results when individuals are more likely to imitate cultural models that are seen as having more prestige. A measure of prestige could be the amount of deference shown to a potential cultural model by other individuals. A "skill bias" results when individuals can directly observe different cultural models performing a learned skill and are more likely to imitate cultural models that perform better at the specific skill. A "success bias" results from individuals preferentially imitating cultural models that they determine are most generally successful (as opposed to successful at a specific skill as in the skill bias.) A "similarity bias" results when individuals are more likely to imitate cultural models that are perceived as being similar to the individual based on specific traits.
Frequency-dependent biases result when an individual is biased to choose particular cultural variants based on their perceived frequency in the population. The most explored frequency-dependent bias is the "conformity bias." Conformity biases result when individuals attempt to copy the mean or the mode cultural variant in the population. Another possible frequency dependent bias is the "rarity bias." The rarity bias results when individuals preferentially choose cultural variants that are less common in the population. The rarity bias is also sometimes called a "nonconformist" or "anti-conformist" bias.
In DIT, the evolution of culture is dependent on the evolution of social learning. Analytic models show that social learning becomes evolutionarily beneficial when the environment changes with enough frequency that genetic inheritance can not track the changes, but not fast enough that individual learning is more efficient.[59] For environments that have very little variability, social learning is not needed since genes can adapt fast enough to the changes that occur, and innate behaviour is able to deal with the constant environment.[60] In fast changing environments cultural learning would not be useful because what the previous generation knew is now outdated and will provide no benefit in the changed environment, and hence individual learning is more beneficial. It is only in the moderately changing environment where cultural learning becomes useful since each generation shares a mostly similar environment but genes have insufficient time to change to changes in the environment.[61] While other species have social learning, and thus some level of culture, only humans, some birds and chimpanzees are known to have cumulative culture.[62] Boyd and Richerson argue that the evolution of cumulative culture depends on observational learning and is uncommon in other species because it is ineffective when it is rare in a population. They propose that the environmental changes occurring in the Pleistocene may have provided the right environmental conditions.[61] Michael Tomasello argues that cumulative cultural evolution results from a ratchet effect that began when humans developed the cognitive architecture to understand others as mental agents.[62] Furthermore, Tomasello proposed in the 80s that there are some disparities between the observational learning mechanisms found in humans and great apes - which go some way to explain the observable difference between great ape traditions and human types of culture (see Emulation (observational learning)).
Although group selection is commonly thought to be nonexistent or unimportant in genetic evolution,[63][64][65] DIT predicts that, due to the nature of cultural inheritance, it may be an important force in cultural evolution. Group selection occurs in cultural evolution because conformist biases make it difficult for novel cultural traits to spread through a population (see above section on transmission biases). Conformist bias also helps maintain variation between groups. These two properties, rare in genetic transmission, are necessary for group selection to operate.[66] Based on an earlier model by Cavalli-Sforza and Feldman,[67] Boyd and Richerson show that conformist biases are almost inevitable when traits spread through social learning,[68] implying that group selection is common in cultural evolution. Analysis of small groups in New Guinea imply that cultural group selection might be a good explanation for slowly changing aspects of social structure, but not for rapidly changing fads.[69] The ability of cultural evolution to maintain intergroup diversity is what allows for the study of cultural phylogenetics.[70]
In 1876, Friedrich Engels wrote a manuscript titled The Part Played by Labour in the Transition from Ape to Man, accredited as a founding document of DIT;[71] “The approach to gene-culture coevolution first developed by Engels and developed later on by anthropologists…” is described by Stephen Jay Gould as “…the best nineteenth-century case for gene-culture coevolution.”[72] The idea that human cultures undergo a similar evolutionary process as genetic evolution also goes back to Darwin.[73] In the 1960s, Donald T. Campbell published some of the first theoretical work that adapted principles of evolutionary theory to the evolution of cultures.[74] In 1976, two developments in cultural evolutionary theory set the stage for DIT. In that year Richard Dawkins's The Selfish Gene introduced ideas of cultural evolution to a popular audience. Although one of the best-selling science books of all time, because of its lack of mathematical rigor, it had little effect on the development of DIT. Also in 1976, geneticists Marcus Feldman and Luigi Luca Cavalli-Sforza published the first dynamic models of gene–culture coevolution.[75] These models were to form the basis for subsequent work on DIT, heralded by the publication of three seminal books in the 1980s.
The first was Charles Lumsden and E.O. Wilson's Genes, Mind and Culture.[76] This book outlined a series of mathematical models of how genetic evolution might favor the selection of cultural traits and how cultural traits might, in turn, affect the speed of genetic evolution. While it was the first book published describing how genes and culture might coevolve, it had relatively little effect on the further development of DIT.[77] Some critics felt that their models depended too heavily on genetic mechanisms at the expense of cultural mechanisms.[78] Controversy surrounding Wilson's sociobiological theories may also have decreased the lasting effect of this book.[77]
The second 1981 book was Cavalli-Sforza and Feldman's Cultural Transmission and Evolution: A Quantitative Approach.[47] Borrowing heavily from population genetics and epidemiology, this book built a mathematical theory concerning the spread of cultural traits. It describes the evolutionary implications of vertical transmission, passing cultural traits from parents to offspring; oblique transmission, passing cultural traits from any member of an older generation to a younger generation; and horizontal transmission, passing traits between members of the same population.
The next significant DIT publication was Robert Boyd and Peter Richerson's 1985 Culture and the Evolutionary Process.[55] This book presents the now-standard mathematical models of the evolution of social learning under different environmental conditions, the population effects of social learning, various forces of selection on cultural learning rules, different forms of biased transmission and their population-level effects, and conflicts between cultural and genetic evolution. The book's conclusion also outlined areas for future research that are still relevant today.[79]
In their 1985 book, Boyd and Richerson outlined an agenda for future DIT research. This agenda, outlined below, called for the development of both theoretical models and empirical research. DIT has since built a rich tradition of theoretical models over the past two decades.[80] However, there has not been a comparable level of empirical work.
In a 2006 interview Harvard biologist E. O. Wilson expressed disappointment at the little attention afforded to DIT:
"...for some reason I haven't fully fathomed, this most promising frontier of scientific research has attracted very few people and very little effort."[81]
Kevin Laland and Gillian Ruth Brown attribute this lack of attention to DIT's heavy reliance on formal modeling.
"In many ways the most complex and potentially rewarding of all approaches, [DIT], with its multiple processes and cerebral onslaught of sigmas and deltas, may appear too abstract to all but the most enthusiastic reader. Until such a time as the theoretical hieroglyphics can be translated into a respectable empirical science most observers will remain immune to its message."[82]
Economist Herbert Gintis disagrees with this critique, citing empirical work as well as more recent work using techniques from behavioral economics.[83] These behavioral economic techniques have been adapted to test predictions of cultural evolutionary models in laboratory settings[84][85][86] as well as studying differences in cooperation in fifteen small-scale societies in the field.[87]
Since one of the goals of DIT is to explain the distribution of human cultural traits, ethnographic and ethnologic techniques may also be useful for testing hypothesis stemming from DIT. Although findings from traditional ethnologic studies have been used to buttress DIT arguments,[47][55] thus far there have been little ethnographic fieldwork designed to explicitly test these hypotheses.[69][87][88]
Herb Gintis has named DIT one of the two major conceptual theories with potential for unifying the behavioral sciences, including economics, biology, anthropology, sociology, psychology and political science. Because it addresses both the genetic and cultural components of human inheritance, Gintis sees DIT models as providing the best explanations for the ultimate cause of human behavior and the best paradigm for integrating those disciplines with evolutionary theory.[89] In a review of competing evolutionary perspectives on human behavior, Laland and Brown see DIT as the best candidate for uniting the other evolutionary perspectives under one theoretical umbrella.[90]
Two major topics of study in both sociology and cultural anthropology are human cultures and cultural variation. However, Dual Inheritance theorists charge that both disciplines too often treat culture as a static superorganic entity that dictates human behavior.[18][91] Cultures are defined by a suite of common traits shared by a large group of people. DIT theorists argue that this doesn't sufficiently explain variation in cultural traits at the individual level. By contrast, DIT models human culture at the individual level and views culture as the result of a dynamic evolutionary process at the population level.[18][92]
Evolutionary psychologists study the evolved architecture of the human mind. They see it as composed of many different programs that process information, each with assumptions and procedures that were specialized by natural selection to solve a different adaptive problem faced by our hunter-gatherer ancestors (e.g., choosing mates, hunting, avoiding predators, cooperating, using aggression).[93] These evolved programs contain content-rich assumptions about how the world and other people work. When ideas are passed from mind to mind, they are changed by these evolved inference systems (much like messages get changed in a game of telephone). But the changes are not usually random. Evolved programs add and subtract information, reshaping the ideas in ways that make them more "intuitive", more memorable, and more attention-grabbing. In other words, "memes" (ideas) are not precisely like genes. Genes are normally copied faithfully as they are replicated, but ideas normally are not. It's not just that ideas mutate every once in a while, like genes do. Ideas are transformed every time they are passed from mind to mind, because the sender's message is being interpreted by evolved inference systems in the receiver.[94][95] It is useful for some applications to note, however, that there are ways to pass ideas which are more resilient and involve substantially less mutation, such as by mass distribution of printed media.
There is no necessary contradiction between evolutionary psychology and DIT, but evolutionary psychologists argue that the psychology implicit in many DIT models is too simple; evolved programs have a rich inferential structure not captured by the idea of a "content bias". They also argue that some of the phenomena DIT models attribute to cultural evolution are cases of "evoked culture"—situations in which different evolved programs are activated in different places, in response to cues in the environment.[96]
Sociobiologists try to understand how maximizing genetic fitness, in either the modern era or past environments, can explain human behavior. When faced with a trait that seems maladaptive, some sociobiologists try to determine how the trait actually increases genetic fitness (maybe through kin selection or by speculating about early evolutionary environments). Dual inheritance theorists, in contrast, will consider a variety of genetic and cultural processes in addition to natural selection on genes.
Human behavioral ecology (HBE) and DIT have a similar relationship to what ecology and evolutionary biology have in the biological sciences. HBE is more concerned about ecological process and DIT more focused on historical process.[97] One difference is that human behavioral ecologists often assume that culture is a system that produces the most adaptive outcome in a given environment. This implies that similar behavioral traditions should be found in similar environments. However, this is not always the case. A study of African cultures showed that cultural history was a better predictor of cultural traits than local ecological conditions.[98]
Memetics, which comes from the meme idea described in Dawkins's The Selfish Gene, is similar to DIT in that it treats culture as an evolutionary process that is distinct from genetic transmission. However, there are some philosophical differences between memetics and DIT.[17] One difference is that memetics' focus is on the selection potential of discrete replicators (memes), where DIT allows for transmission of both non-replicators and non-discrete cultural variants. DIT does not assume that replicators are necessary for cumulative adaptive evolution. DIT also more strongly emphasizes the role of genetic inheritance in shaping the capacity for cultural evolution. But perhaps the biggest difference is a difference in academic lineage. Memetics as a label is more influential in popular culture than in academia. Critics of memetics argue that it is lacking in empirical support or is conceptually ill-founded, and question whether there is hope for the memetic research program succeeding. Proponents point out that many cultural traits are discrete, and that many existing models of cultural inheritance assume discrete cultural units, and hence involve memes.[99]
Israeli psychologist Liane Gabora has criticised DIT.[100][101][102] She argues that traits that are not transmitted by way of a self-assembly code (as in genetic evolution) is misleading, because this second use does not capture the algorithmic structure that makes an inheritance system require a particular kind of mathematical framework.[103]
Other criticisms of the effort to frame culture in tandem with evolution have been leveled by Richard Lewontin,[104] Niles Eldredge,[105] and Stuart Kauffman.[106]

---

# List of human evolution fossils

The following tables give an overview of notable finds of hominin fossils and remains relating to human evolution, beginning with the formation of the tribe Hominini (the divergence of the human and chimpanzee lineages) in the late Miocene, roughly 7 to 8 million years ago.
As there are thousands of fossils, mostly fragmentary, often consisting of single bones or isolated teeth with complete skulls and skeletons rare, this overview is not complete, but shows some of the most important findings. The fossils are arranged by approximate age as determined by radiometric dating and/or incremental dating and the species name represents current consensus; if there is no clear scientific consensus the other possible classifications are indicated.
The early fossils shown are not considered ancestors to Homo sapiens but are closely related to ancestors and are therefore important to the study of the lineage. After 1.5 million years ago (extinction of Paranthropus), all fossils shown are human (genus Homo). After 11,500 years ago (11.5 ka, beginning of the Holocene), all fossils shown are Homo sapiens (anatomically modern humans), illustrating recent divergence in the formation of modern human sub-populations.
The chimpanzee–human divergence likely took place around 10 to 7 million years ago.[1] The list of fossils begins with Graecopithecus, dated some 7.2 million years ago, which may or may not still be ancestral to both the human and the chimpanzee lineage. For the earlier history of the human lineage, see Timeline of human evolution#Hominidae, Hominidae#Phylogeny.

---

# Timeline of human evolution

The timeline of human evolution outlines the major events in the evolutionary lineage of the modern human species, Homo sapiens, throughout the history of life, beginning some 4 billion years ago down to recent evolution within H. sapiens during and since the Last Glacial Period.
It includes brief explanations of the various taxonomic ranks in the human lineage. The timeline reflects the mainstream views in modern taxonomy, based on the principle of phylogenetic nomenclature; in cases of open questions with no clear consensus, the main competing possibilities are briefly outlined.
A tabular overview of the taxonomic ranking of Homo sapiens (with age estimates for each rank) is shown below.
For another billion years, prokaryotes would continue to diversify undisturbed.
The Holozoa lineage of eukaryotes evolves many features for making cell colonies, and finally leads to the ancestor of animals (metazoans) and choanoflagellates.[5][6]
Proterospongia (members of the Choanoflagellata) are the best living examples of what the ancestor of all animals may have looked like. They live in colonies, and show a primitive level of cellular specialization for different tasks.
Urmetazoan: The first fossils that might represent animals appear in the 665-million-year-old rocks of the Trezona Formation of South Australia. These fossils are interpreted as being early sponges.[7] Multicellular animals may have existed from 800 Ma. Separation from the Porifera (sponges) lineage. Eumetazoa/Diploblast: separation from the Ctenophora ("comb jellies") lineage. Planulozoa/ParaHoxozoa: separation from the Placozoa and Cnidaria lineages. All diploblasts possess epithelia, nerves, muscles and connective tissue and mouths, and except for placozoans, have some form of symmetry, with their ancestors probably having radial symmetry like that of cnidarians. Diploblasts separated their early embryonic cells into two germ layers (ecto- and endoderm). Photoreceptive eye-spots evolve.
Urbilaterian: the last common ancestor of xenacoelomorphs, protostomes (including the arthropod [insect, crustacean, spider], mollusc [squid, snail, clam] and annelid [earthworm] lineages) and the deuterostomes (including the vertebrate [human] lineage) (the last two are more related to each other and called Nephrozoa). Xenacoelomorphs all have a gonopore to expel gametes but nephrozoans merged it with their anus. Earliest development of bilateral symmetry, mesoderm, head (anterior cephalization) and various gut muscles (and thus peristalsis) and, in the Nephrozoa, nephridia (kidney precursors), coelom (or maybe pseudocoelom), distinct mouth and anus (evolution of through-gut), and possibly even nerve cords and blood vessels.[8] Reproductive tissue probably concentrates into a pair of gonads connecting just before the posterior orifice. "Cup-eyes" and balance organs evolve (the function of hearing added later as the more complex inner ear evolves in vertebrates). The nephrozoan through-gut had a wider portion in the front, called the pharynx. The integument or skin consists of an epithelial layer (epidermis) and a connective layer.
Most known animal phyla appeared in the fossil record as marine species during the Ediacaran-Cambrian explosion, probably caused by long scale oxygenation since around 585 Ma (sometimes called the Neoproterozoic Oxygenation Event or NOE) and also an influx of oceanic minerals. Deuterostomes, the last common ancestor of the Chordata [human] lineage, Hemichordata (acorn worms and graptolites) and Echinodermata (starfish, sea urchins, sea cucumbers, etc.), probably had both ventral and dorsal nerve cords like modern acorn worms.
An archaic survivor from this stage is the acorn worm, sporting an open circulatory system (with less branched blood vessels) with a heart that also functions as a kidney. Acorn worms have a plexus concentrated into both dorsal and ventral nerve cords. The dorsal cord reaches into the proboscis, and is partially separated from the epidermis in that region. This part of the dorsal nerve cord is often hollow, and may well be homologous with the brain of vertebrates.[9] Deuterostomes also evolved pharyngeal slits, which were probably used for filter feeding like in hemi- and proto-chordates.
The increased amount of oxygen causes many eukaryotes, including most animals, to become obligate aerobes.
The Chordata ancestor gave rise to the lancelets (Amphioxii) and Olfactores. Ancestral chordates evolved a post-anal tail, notochord, and endostyle (precursor of thyroid). The pharyngeal slits (or gills) are now supported by connective tissue and used for filter feeding and possibly breathing. The first of these basal chordates to be discovered by science was Pikaia gracilens.[10] Other, earlier chordate predecessors include Myllokunmingia fengjiaoa,[11] Yunnanozoon lividum,[12] and Haikouichthys ercaicunensis.[13] They probably lost their ventral nerve cord and evolved a special region of the dorsal one, called the brain, with glia becoming permanently associated with neurons. They probably evolved the first blood cells (probably early leukocytes, indicating advanced innate immunity), which they made around the pharynx and gut.[14] All chordates except tunicates sport an intricate, closed circulatory system, with highly branched blood vessels.
Olfactores, last common ancestor of tunicates and vertebrates in which olfaction (smell) evolved. Since lancelets lack a heart, it possibly emerged in this ancestor (previously the blood vessels themselves were contractile) though it could have been lost in lancelets after evolving in early deuterostomes (hemichordates and echinoderms have hearts).
The first vertebrates ("fish") appear: the Agnathans. They were jawless, had seven pairs of pharyngeal arches like their descendants today, and their endoskeletons were cartilaginous (then only consisting of the chondrocranium/braincase and vertebrae). The jawless Cyclostomata diverge at this stage. The connective tissue below the epidermis differentiates into the dermis and hypodermis.[15] They depended on gills for respiration and evolved the unique sense of taste (the remaining sense of the skin now called "touch"), endothelia, camera eyes and inner ears (capable of hearing and balancing; each consists of a lagena, an otolithic organ and two semicircular canals) as well as livers, thyroids, kidneys and two-chambered hearts (one atrium and one ventricle). They had a tail fin but lacked the paired (pectoral and pelvic) fins of more advanced fish. Brain divided into three parts (further division created distinct regions based on function). The pineal gland of the brain penetrates to the level of the skin on the head, making it seem like a third eye. They evolved the first erythrocytes and thrombocytes.[16]
The Placodermi were the first jawed fishes (Gnathostomata); their jaws evolved from the first gill/pharyngeal arch and they largely replaced their endoskeletal cartilage with bone and evolved pectoral and pelvic fins. Bones of the first gill arch became the upper and lower jaw, while those from the second arch became the hyomandibula, ceratohyal and basihyal; this closed two of the seven pairs of gills. The gap between the first and second arches just below the braincase (fused with upper jaw) created a pair of spiracles, which opened in the skin and led to the pharynx (water passed through them and left through gills). Placoderms had competition with the previous dominant animals, the cephalopods and sea scorpions, and rose to dominance themselves. A lineage of them probably evolved into the bony and cartilaginous fish, after evolving scales, teeth (which allowed the transition to full carnivory), stomachs, spleens, thymuses, myelin sheaths, hemoglobin and advanced, adaptive immunity (the latter two occurred independently in the lampreys and hagfish). Jawed fish also have a third, lateral semicircular canal and their otoliths are divided between a saccule and utricle.
Some freshwater lobe-finned fish (sarcopterygii) develop limbs and give rise to the Tetrapodomorpha. These fish evolved in shallow and swampy freshwater habitats, where they evolved large eyes and spiracles.
Primitive tetrapods ("fishapods") developed from tetrapodomorphs with a two-lobed brain in a flattened skull, a wide mouth and a medium snout, whose upward-facing eyes show that it was a bottom-dweller, and which had already developed adaptations of fins with fleshy bases and bones. (The "living fossil" coelacanth is a related lobe-finned fish without these shallow-water adaptations.) Tetrapod fishes used their fins as paddles in shallow-water habitats choked with plants and detritus. The universal tetrapod characteristics of front limbs that bend backward at the elbow and hind limbs that bend forward at the knee can plausibly be traced to early tetrapods living in shallow water.[18]
Panderichthys is a 90–130 cm (35–50 in) long fish from the Late Devonian period (380 Mya). It has a large tetrapod-like head. Panderichthys exhibits features transitional between lobe-finned fishes and early tetrapods.
Trackway impressions made by something that resembles Ichthyostega's limbs were formed 390 Ma in Polish marine tidal sediments. This suggests tetrapod evolution is older than the dated fossils of Panderichthys through to Ichthyostega.
Tiktaalik is a genus of sarcopterygian (lobe-finned) fishes from the late Devonian with many tetrapod-like features. It shows a clear link between Panderichthys and Acanthostega.
Acanthostega is an extinct tetrapod, among the first animals to have recognizable limbs. It is a candidate for being one of the first vertebrates to be capable of coming onto land. It lacked wrists, and was generally poorly adapted for life on land. The limbs could not support the animal's weight. Acanthostega had both lungs and gills, also indicating it was a link between lobe-finned fish and terrestrial vertebrates. The dorsal pair of ribs form a rib cage to support the lungs, while the ventral pair disappears.
Ichthyostega is another extinct tetrapod. Being one of the first animals with only two pairs of limbs (also unique since they end in digits and have bones), Ichthyostega is seen as an intermediate between a fish and an amphibian. Ichthyostega had limbs but these probably were not used for walking. They may have spent very brief periods out of water and would have used their limbs to paw their way through the mud.[19] They both had more than five digits (eight or seven) at the end of each of their limbs, and their bodies were scaleless (except their bellies, where they remained as gastralia). Many evolutionary changes occurred at this stage: eyelids and tear glands evolved to keep the eyes wet out of water and the eyes became connected to the pharynx for draining the liquid; the hyomandibula (now called columella) shrank into the spiracle, which now also connected to the inner ear at one side and the pharynx at another, becoming the Eustachian tube (columella assisted in hearing); an early eardrum (a patch of connective tissue) evolved on the end of each tube (called the otic notch); and the ceratohyal and basihyal merged into the hyoid. These "fishapods" had more ossified and stronger bones to support themselves on land (especially skull and limb bones). Jaw bones fuse together while gill and opercular bones disappear.
Pederpes from around 350 Ma indicates that the standard number of 5 digits evolved at the Early Carboniferous, when modern tetrapods (or "amphibians") split in two directions (one leading to the extant amphibians and the other to amniotes). At this stage, our ancestors evolved vomeronasal organs, salivary glands, tongues, parathyroid glands, three-chambered hearts (with two atria and one ventricle) and bladders, and completely removed their gills by adulthood. The glottis evolves to prevent food going into the respiratory tract. Lungs and thin, moist skin allowed them to breathe; water was also needed to give birth to shell-less eggs and for early development. Dorsal, anal and tail fins all disappeared.
Lissamphibia (extant amphibians) retain many features of early amphibians but they have only four digits (caecilians have none).
From amphibians came the first amniotes: Hylonomus, a primitive reptile, is the earliest amniote known. It was 20 cm (8 in) long (including the tail) and probably would have looked rather similar to modern lizards. It had small sharp teeth and probably ate small millipedes and insects. It is a precursor of later amniotes (including both the reptiles and the ancestors of mammals). Alpha keratin first evolves here; it is used in the claws of modern amniotes, and hair in mammals, indicating claws and a different type of scales evolved in amniotes (complete loss of gills as well).[20]
Evolution of the amniotic egg allows the amniotes to reproduce on land and lay shelled eggs on dry land. They did not need to return to water for reproduction nor breathing. This adaptation and the desiccation-resistant scales gave them the capability to inhabit the uplands for the first time, albeit making them drink water through their mouths. At this stage, adrenal tissue may have concentrated into discrete glands.
Amniotes have advanced nervous systems, with twelve pairs of cranial nerves, unlike lower vertebrates. They also evolved true sternums but lost their eardrums and otic notches (hearing only by columella bone conduction).
The earliest synapsids, or "proto-mammals," are the pelycosaurs. The pelycosaurs were the first animals to have temporal fenestrae. Pelycosaurs were not therapsids but their ancestors. The therapsids were, in turn, the ancestors of mammals.
The therapsids had temporal fenestrae larger and more mammal-like than pelycosaurs, their teeth showed more serial differentiation, their gait was semi-erect and later forms had evolved a secondary palate. A secondary palate enables the animal to eat and breathe at the same time and is a sign of a more active, perhaps warm-blooded, way of life.[21] They had lost gastralia and, possibly, scales.
One subgroup of therapsids, the cynodonts, lose pineal eye and lumbar ribs and very likely became warm-blooded. The lower respiratory tract forms intricate branches in the lung parenchyma, ending in highly vascularized alveoli. Erythrocytes and thrombocytes lose their nuclei while lymphatic systems and advanced immunity emerge. They may have also had thicker dermis like mammals today.
The jaws of cynodonts resembled modern mammal jaws; the anterior portion, the dentary, held differentiated teeth. This group of animals likely contains a species which is the ancestor of all modern mammals. Their temporal fenestrae merged with their orbits. Their hindlimbs became erect and their posterior bones of the jaw progressively shrunk to the region of the columella.[22]
From Eucynodontia came the first mammals. Most early mammals were small shrew-like animals that fed on insects and had transitioned to nocturnality to avoid competition with the dominant archosaurs — this led to the loss of the vision of red and ultraviolet light (ancestral tetrachromacy of vertebrates reduced to dichromacy). Although there is no evidence in the fossil record, it is likely that these animals had a constant body temperature, hair and milk glands for their young (the glands stemmed from the milk line). The neocortex (part of the cerebrum) region of the brain evolves in Mammalia, at the reduction of the tectum (non-smell senses which were processed here became integrated into neocortex but smell became primary sense). Origin of the prostate gland and a pair of holes opening to the columella and nearby shrinking jaw bones; new eardrums stand in front of the columella and Eustachian tube. The skin becomes hairy, glandular (glands secreting sebum and sweat) and thermoregulatory. Teeth fully differentiate into incisors, canines, premolars and molars; mammals become diphyodont and possess developed diaphragms and males have internal penises. All mammals have four chambered hearts (with two atria and two ventricles) and lack cervical ribs (now mammals only have thoracic ribs).
Monotremes are an egg-laying group of mammals represented today by the platypus and echidna. Recent genome sequencing of the platypus indicates that its sex genes are closer to those of birds than to those of the therian (live birthing) mammals. Comparing this to other mammals, it can be inferred that the first mammals to gain sexual differentiation through the existence or lack of SRY gene (found in the y-Chromosome) evolved only in the therians. Early mammals and possibly their eucynodontian ancestors had epipubic bones, which serve to hold the pouch in modern marsupials (in both sexes).
Evolution of live birth (viviparity), with early therians probably having pouches for keeping their undeveloped young like in modern marsupials. Nipples stemmed out of the therian milk lines. The posterior orifice separates into anal and urogenital openings; males possess an external penis.
Monotremes and therians independently detach the malleus and incus from the dentary (lower jaw) and combine them to the shrunken columella (now called stapes) in the tympanic cavity behind the eardrum (which is connected to the malleus and held by another bone detached from the dentary, the tympanic plus ectotympanic), and coil their lagena (cochlea) to advance their hearing, with therians further evolving an external pinna and erect forelimbs. Female placentalian mammals do not have pouches and epipubic bones but instead have a developed placenta which penetrates the uterus walls (unlike marsupials), allowing a longer gestation; they also have separated urinary and genital openings.[23]
A group of small, nocturnal, arboreal, insect-eating mammals called Euarchonta begins a speciation that will lead to the orders of primates, treeshrews and flying lemurs. They reduced the number of mammaries to only two pairs (on the chest). Primatomorpha is a subdivision of Euarchonta including primates and their ancestral stem-primates Plesiadapiformes. An early stem-primate, Plesiadapis, still had claws and eyes on the side of the head, making it faster on the ground than in the trees, but it began to spend long times on lower branches, feeding on fruits and leaves.
The Plesiadapiformes very likely contain the ancestor species of all primates.[24] They first appeared in the fossil record around 66 million years ago, soon after the Cretaceous–Paleogene extinction event that eliminated about three-quarters of plant and animal species on Earth, including most dinosaurs.[25][26]
One of the last Plesiadapiformes is Carpolestes simpsoni, having grasping digits but not forward-facing eyes.
Simians split into infraorders Platyrrhini and Catarrhini. They fully transitioned to diurnality and lacked any claw and tapetum lucidum (which evolved many times in various vertebrates). They possibly evolved at least some of the paranasal sinuses, and transitioned from estrous cycle to menstrual cycle. The number of mammaries is now reduced to only one thoracic pair. Platyrrhines, New World monkeys, have prehensile tails and males are color blind. The individuals whose descendants would become Platyrrhini are conjectured to have migrated to South America either on a raft of vegetation or via a land bridge (the hypothesis now favored[27]). Catarrhines mostly stayed in Africa as the two continents drifted apart. Possible early ancestors of catarrhines include Aegyptopithecus and Saadanius.
Catarrhini splits into 2 superfamilies, Old World monkeys (Cercopithecoidea) and apes (Hominoidea). Human trichromatic color vision had its genetic origins in this period. Catarrhines lost the vomeronasal organ (or possibly reduced it to vestigial status).
Proconsul was an early genus of catarrhine primates. They had a mixture of Old World monkey and ape characteristics. Proconsul's monkey-like features include thin tooth enamel, a light build with a narrow chest and short forelimbs, and an arboreal quadrupedal lifestyle. Its ape-like features are its lack of a tail, ape-like elbows, and a slightly larger brain relative to body size.
Proconsul africanus is a possible ancestor of both great and lesser apes, including humans.
Pierolapithecus catalaunicus is thought to be a common ancestor of humans and the other great apes, or at least a species that brings us closer to a common ancestor than any previous fossil discovery. It had the special adaptations for tree climbing as do present-day humans and other great apes: a wide, flat rib cage, a stiff lower spine, flexible wrists, and shoulder blades that lie along its back.
Hominini: The latest common ancestor of humans and chimpanzees is estimated to have lived between roughly 10 to 5 million years ago. Both chimpanzees and humans have a larynx that repositions during the first two years of life to a spot between the pharynx and the lungs, indicating that the common ancestors have this feature, a precondition for vocalized speech in humans. Speciation may have begun shortly after 10 Ma, but late admixture between the lineages may have taken place until after 5 Ma. Candidates of Hominina or Homininae species which lived in this time period include Graecopithecus (c. 7 Ma), Sahelanthropus tchadensis (c. 7 Ma), Orrorin tugenensis (c. 6 Ma).
Ardipithecus was arboreal, meaning it lived largely in the forest where it competed with other forest animals for food, no doubt including the contemporary ancestor of the chimpanzees. Ardipithecus was probably bipedal as evidenced by its bowl shaped pelvis, the angle of its foramen magnum and its thinner wrist bones, though its feet were still adapted for grasping rather than walking for long distances.
A member of the Australopithecus afarensis left human-like footprints on volcanic ash in Laetoli, northern Tanzania, providing strong evidence of full-time bipedalism. Australopithecus afarensis lived between 3.9 and 2.9 million years ago, and is considered one of the earliest hominins—those species that developed and comprised the lineage of Homo and Homo's closest relatives after the split from the line of the chimpanzees.
It is thought that A. afarensis was ancestral to both the genus Australopithecus and the genus Homo. Compared to the modern and extinct great apes, A. afarensis had reduced canines and molars, although they were still relatively larger than in modern humans. A. afarensis also has a relatively small brain size (380–430 cm3) and a prognathic (anterior-projecting) face.
Australopithecines have been found in savannah environments; they probably developed their diet to include scavenged meat. Analyses of Australopithecus africanus lower vertebrae suggests that these bones changed in females to support bipedalism even during pregnancy.
Early Homo appears in East Africa, speciating from australopithecine ancestors. The Lower Paleolithic is defined by the beginning of use of stone tools. Australopithecus garhi was using stone tools at about 2.5 Ma. Homo habilis is the oldest species given the designation Homo, by Leakey et al. in 1964. H. habilis is intermediate between Australopithecus afarensis and H. erectus, and there have been suggestions to re-classify it within genus Australopithecus, as Australopithecus habilis.
LD 350-1 is now considered the earliest known specimen of the genus Homo, dating to 2.75–2.8 Ma, found in the Ledi-Geraru site in the Afar Region of Ethiopia. It is currently unassigned to a species, and it is unclear if it represents the ancestor to H. habilis and H. rudolfensis, which are estimated to have evolved around 2.4 Ma.[36]
Stone tools found at the Shangchen site in China and dated to 2.12 million years ago are considered the earliest known evidence of hominins outside Africa, surpassing Dmanisi hominins found in Georgia by 300,000 years, although whether these hominins were an early species in the genus Homo or another hominin species is unknown.[37]
Homo erectus derives from early Homo or late Australopithecus.
Homo habilis, although significantly different of anatomy and physiology, is thought to be the ancestor of Homo ergaster, or African Homo erectus; but it is also known to have coexisted with H. erectus for almost half a million years (until about 1.5 Ma). From its earliest appearance at about 1.9 Ma, H. erectus is distributed in East Africa and Southwest Asia (Homo georgicus). H. erectus is the first known species to develop control of fire, by about 1.5 Ma.
H. erectus later migrates throughout Eurasia, reaching Southeast Asia by 0.7 Ma. It is described in a number of subspecies.[38] Early humans were social and initially scavenged, before becoming active hunters. The need to communicate and hunt prey efficiently in a new, fluctuating environment (where the locations of resources need to be memorized and told) may have driven the expansion of the brain from 2 to 0.8 Ma.
Homo antecessor may be a common ancestor of Homo sapiens and Neanderthals.[40][41] At present estimate, humans have approximately 20,000–25,000 genes and share 99% of their DNA with the now extinct Neanderthal[42] and 95–99% of their DNA with their closest living evolutionary relative, the chimpanzees.[43][44] The human variant of the FOXP2 gene (linked to the control of speech) has been found to be identical in Neanderthals.[45]
Divergence of Neanderthal and Denisovan lineages from a common ancestor.[46] Homo heidelbergensis (in Africa also known as Homo rhodesiensis) had long been thought to be a likely candidate for the last common ancestor of the Neanderthal and modern human lineages. However, genetic evidence from the Sima de los Huesos fossils published in 2016 seems to suggest that H. heidelbergensis in its entirety should be included in the Neanderthal lineage, as "pre-Neanderthal" or "early Neanderthal", while the divergence time between the Neanderthal and modern lineages has been pushed back to before the emergence of H. heidelbergensis, to about 600,000 to 800,000 years ago, the approximate age of Homo antecessor.[47][48] Brain expansion (enlargement) between 0.8 and 0.2 Ma may have occurred due to the extinction of most African megafauna (which made humans feed from smaller prey and plants, which required greater intelligence due to greater speed of the former and uncertainty about whether the latter were poisonous or not), extreme climate variability after Mid-Pleistocene Transition (which intensified the situation, and resulted in frequent migrations), and in general selection for more social life (and intelligence) for greater chance of survival, reproductivity, and care for mothers. Solidified footprints dated to about 350 ka and associated with H. heidelbergensis were found in southern Italy in 2003.[49]
H. sapiens lost the brow ridges from their hominid ancestors as well as the snout completely, though their noses evolve to be protruding (possibly from the time of H. erectus). By 200 ka, humans had stopped their brain expansion.
Neanderthals and Denisovans emerge from the northern Homo heidelbergensis lineage around 500-450 ka while sapients emerge from the southern lineage around 350-300 ka.[50]
Fossils attributed to H. sapiens, along with stone tools, dated to approximately 300,000 years ago, found at Jebel Irhoud, Morocco[51] yield the earliest fossil evidence for anatomically modern Homo sapiens. Modern human presence in East Africa (Gademotta), at 276 kya.[52] In July 2019, anthropologists reported the discovery of 210,000 year old remains of what may possibly have been a H. sapiens in Apidima Cave, Peloponnese, Greece.[53][54][55]
Patrilineal and matrilineal most recent common ancestors (MRCAs) of living humans roughly between 200 and 100 kya[56][57] with some estimates on the patrilineal MRCA somewhat higher, ranging up to 250 to 500 kya.[58]
160,000 years ago, Homo sapiens idaltu in the Awash River Valley (near present-day Herto village, Ethiopia) practiced excarnation.[59]
Modern human presence in Southern Africa and West Africa.[60] Appearance of mitochondrial haplogroup (mt-haplogroup) L2.
Early evidence for behavioral modernity.[61] Appearance of mt-haplogroups M and N. Southern Dispersal migration out of Africa, Proto-Australoid peopling of Oceania.[62] Archaic admixture from Neanderthals in Eurasia,[63][64] from Denisovans in Oceania with trace amounts in Eastern Eurasia,[65] and from an unspecified African lineage of archaic humans in Sub-Saharan Africa as well as an interbred species of Neanderthals and Denisovans in Asia and Oceania.[66][67][68][69]
Behavioral modernity develops by this time or earlier, according to the "great leap forward" theory.[70] Extinction of Homo floresiensis.[71] M168 mutation (carried by all non-African males). Appearance of mt-haplogroups U and K. Peopling of Europe, peopling of the North Asian Mammoth steppe. Paleolithic art. Extinction of Neanderthals and other archaic human variants (with possible survival of hybrid populations in Asia and Africa). Appearance of Y-Haplogroup R2; mt-haplogroups J and X.
Last Glacial Maximum; Epipaleolithic / Mesolithic / Holocene. Peopling of the Americas. Appearance of: Y-Haplogroup R1a; mt-haplogroups V and T. Various recent divergence associated with environmental pressures, e.g. light skin in Europeans and East Asians (KITLG, ASIP), after 30 ka;[72] Inuit adaptation to high-fat diet and cold climate, 20 ka.[73]
Extinction of late surviving archaic humans at the beginning of the Holocene (12 ka). Accelerated divergence due to selection pressures in populations participating in the Neolithic Revolution after 12 ka, e.g. East Asian types of ADH1B associated with rice domestication,[74] or lactase persistence.[75][76] A slight decrease in brain size occurred a few thousand years ago.[citation needed]

---

# Archaeogenetics

Archaeogenetics is the study of ancient DNA using various molecular genetic methods and DNA resources. This form of genetic analysis can be applied to human, animal, and plant specimens. Ancient DNA can be extracted from various fossilized specimens including bones, eggshells, and artificially preserved tissues in human and animal specimens. In plants, ancient DNA can be extracted from seeds and tissue. Archaeogenetics provides us with genetic evidence of ancient population group migrations,[1] domestication events, and plant and animal evolution.[2] The ancient DNA cross referenced with the DNA of relative modern genetic populations allows researchers to run comparison studies that provide a more complete analysis when ancient DNA is compromised.[3]
Archaeogenetics receives its name from the Greek word arkhaios, meaning "ancient", and the term genetics, meaning "the study of heredity".[4] The term archaeogenetics was conceived by archaeologist Colin Renfrew.[5]
In February 2021, scientists reported the oldest DNA ever sequenced was successfully retrieved from a mammoth dating back over a million years.[6][7]
Ludwik Hirszfeld was a Polish microbiologist and serologist who was the President of the Blood Group Section of the Second International Congress of Blood Transfusion. He founded blood group inheritance with Erich von Dungern in 1910, and contributed to it greatly throughout his life.[8] He studied ABO blood groups. In one of his studies in 1919, Hirszfeld documented the ABO blood groups and hair color of people at the Macedonian front, leading to his discovery that the hair color and blood type had no correlation. In addition to that he observed that there was a decrease of blood group A from western Europe to India and the opposite for blood group B. He hypothesized that the east-to-west blood group ratio stemmed from two blood groups consisting of mainly A or B mutating from blood group O, and mixing through migration or intermingling.[8] A majority of his work was researching the links of blood types to sex, disease, climate, age, social class, and race. His work led him to discover that peptic ulcer was more dominant in blood group O, and that AB blood type mothers had a high male-to-female birth ratio.[9]
Arthur Mourant was a British hematologist and chemist. He received many awards, most notably Fellowship of the Royal Society. His work included organizing the existing data on blood group gene frequencies, and largely contributing to the genetic map of the world through his investigation of blood groups in many populations. Mourant discovered the new blood group antigens of the Lewis, Henshaw, Kell, and Rhesus systems, and analyzed the association of blood groups and various other diseases. He also focused on the biological significance of polymorphisms. His work provided the foundation for archaeogenetics because it facilitated the separation of genetic evidence for biological relationships between people. This genetic evidence was previously used for that purpose. It also provided material that could be used to appraise the theories of population genetics.[10]
William Boyd was an American immunochemist and biochemist who became famous for his research on the genetics of race in the 1950s.[11] During the 1940s, Boyd and Karl O. Renkonen independently discovered that lectins react differently to various blood types, after finding that the crude extracts of the lima bean and tufted vetch agglutinated the red blood cells from blood type A but not blood types B or O. This ultimately led to the disclosure of thousands of plants that contained these proteins.[12] In order to examine racial differences and the distribution and migration patterns of various racial groups, Boyd systematically collected and classified blood samples from around the world, leading to his discovery that blood groups are not influenced by the environment, and are inherited. In his book Genetics and the Races of Man (1950), Boyd categorized the world population into 13 distinct races, based on their different blood type profiles and his idea that human races are populations with differing alleles.[13][14] One of the most abundant information sources regarding inheritable traits linked to race remains the study of blood groups.[14]
Fossil retrieval starts with selecting an excavation site. Potential excavation sites are usually identified with the mineralogy of the location and visual detection of bones in the area. However, there are more ways to discover excavation zones using technology such as field portable x-ray fluorescence[15] and Dense Stereo Reconstruction.[16] Tools used include knives, brushes, and pointed trowels which assist in the removal of fossils from the earth.[17]
To avoid contaminating the ancient DNA, specimens are handled with gloves and stored in -20 °C immediately after being unearthed. Ensuring that the fossil sample is analyzed in a lab that has not been used for other DNA analysis could prevent contamination as well.[17][18] Bones are milled to a powder and treated with a solution before the polymerase chain reaction (PCR) process.[18] Samples for DNA amplification may not necessarily be fossil bones. Preserved skin, salt-preserved or air-dried, can also be used in certain situations.[19]
DNA preservation is difficult because the bone fossilisation degrades and DNA is chemically modified, usually by bacteria and fungi in the soil. The best time to extract DNA from a fossil is when it is freshly out of the ground as it contains six times the DNA when compared to stored bones. The temperature of extraction site also affects the amount of obtainable DNA, evident by a decrease in success rate for DNA amplification if the fossil is found in warmer regions. A drastic change of a fossil's environment also affects DNA preservation. Since excavation causes an abrupt change in the fossil's environment, it may lead to physiochemical change in the DNA molecule. Moreover, DNA preservation is also affected by other factors such as the treatment of the unearthed fossil like (e.g. washing, brushing and sun drying), pH, irradiation, the chemical composition of bone and soil, and hydrology. There are three perseveration diagenetic phases. The first phase is bacterial putrefaction, which is estimated to cause a 15-fold degradation of DNA. Phase 2 is when bone chemically degrades, mostly by depurination. The third diagenetic phase occurs after the fossil is excavated and stored, in which bone DNA degradation occurs most rapidly.[18]
Once a specimen is collected from an archaeological site, DNA can be extracted through a series of processes.[20] One of the more common methods utilizes silica and takes advantage of polymerase chain reactions in order to collect ancient DNA from bone samples.[21]
There are several challenges that add to the difficulty when attempting to extract ancient DNA from fossils and prepare it for analysis. DNA is continuously being split up. While the organism is alive these splits are repaired; however, once an organism has died, the DNA will begin to deteriorate without repair. This results in samples having strands of DNA measuring around 100 base pairs in length. Contamination is another significant challenge at multiple steps throughout the process. Often other DNA, such as bacterial DNA, will be present in the original sample. To avoid contamination it is necessary to take many precautions such as separate ventilation systems and workspaces for ancient DNA extraction work.[22] The best samples to use are fresh fossils as uncareful washing can lead to mold growth.[20] DNA coming from fossils also occasionally contains a compound that inhibits DNA replication.[23] Coming to a consensus on which methods are best at mitigating challenges is also difficult due to the lack of repeatability caused by the uniqueness of specimens.[22]
Silica-based DNA extraction is a method used as a purification step to extract DNA from archaeological bone artifacts and yield DNA that can be amplified using polymerase chain reaction (PCR) techniques.[23] This process works by using silica as a means to bind DNA and separate it from other components of the fossil process that inhibit PCR amplification. However, silica itself is also a strong PCR inhibitor, so careful measures must be taken to ensure that silica is removed from the DNA after extraction.[24] The general process for extracting DNA using the silica-based method is outlined by the following:[21]
One of the main advantages of silica-based DNA extraction is that it is relatively quick and efficient, requiring only a basic laboratory setup and chemicals. It is also independent of sample size, as the process can be scaled to accommodate larger or smaller quantities. Another benefit is that the process can be executed at room temperature. However, this method does contain some drawbacks. Mainly, silica-based DNA extraction can only be applied to bone and teeth samples; they cannot be used on soft tissue. While they work well with a variety of different fossils, they may be less effective in fossils that are not fresh (e.g. treated fossils for museums). Also, contamination poses a risk for all DNA replication in general, and this method may result in misleading results if applied to contaminated material.[21]
Polymerase chain reaction is a process that can amplify segments of DNA and is often used on extracted ancient DNA. It has three main steps: denaturation, annealing, and extension. Denaturation splits the DNA into two single strands at high temperatures. Annealing involves attaching primer strands of DNA to the single strands that allow Taq polymerase to attach to the DNA. Extension occurs when Taq polymerase is added to the sample and matches base pairs to turn the two single strands into two complete double strands.[20] This process is repeated many times, and is usually repeated a higher number of times when used with ancient DNA.[25] Some issues with PCR is that it requires overlapping primer pairs for ancient DNA due to the short sequences. There can also be “jumping PCR” which causes recombination during the PCR process which can make analyzing the DNA more difficult in inhomogeneous samples.
DNA extracted from fossil remains is primarily sequenced using Massive parallel sequencing,[26] which allows simultaneous amplification and sequencing of all DNA segments in a sample, even when it is highly fragmented and of low concentration.[25] It involves attaching a generic sequence to every single strand that generic primers can bond to, and thus all of the DNA present is amplified. This is generally more costly and time intensive than PCR but due to the difficulties involved in ancient DNA amplification it is cheaper and more efficient.[25] One method of massive parallel sequencing, developed by Margulies et al., employs bead-based emulsion PCR and pyrosequencing,[27] and was found to be powerful in analyses of aDNA because it avoids potential loss of sample, substrate competition for templates, and error propagation in replication.[28]
The most common way to analyze an aDNA sequence is to compare it with a known sequence from other sources, and this could be done in different ways for different purposes.
The identity of the fossil remain can be uncovered by comparing its DNA sequence with those of known species using software such as BLASTN.[28] This archaeogenetic approach is especially helpful when the morphology of the fossil is ambiguous.[29] Apart from that, species identification can also be done by finding specific genetic markers in an aDNA sequence. For example, the American indigenous population is characterized by specific mitochondrial RFLPs and deletions defined by Wallace et al.[30]
aDNA comparison study can also reveal the evolutionary relationship between two species. The number of base differences between DNA of an ancient species and that of a closely related extant species can be used to estimate the divergence time of those two species from their last common ancestor.[26] The phylogeny of some extinct species, such as Australian marsupial wolves and American ground sloths, has been constructed by this method.[26] Mitochondrial DNA in animals and chloroplast DNA in plants are usually used for this purpose because they have hundreds of copies per cell and thus are more easily accessible in ancient fossils.[26]
Another method to investigate relationship between two species is through DNA hybridization. Single-stranded DNA segments of both species are allowed to form complementary pair bonding with each other. More closely related species have a more similar genetic makeup, and thus a stronger hybridization signal. Scholz et al. conducted southern blot hybridization on Neanderthal aDNA (extracted from fossil remain W-NW and Krapina). The results showed weak ancient human-Neanderthal hybridization and strong ancient human-modern human hybridization. The human-chimpanzee and Neanderthal-chimpanzee hybridization are of similarly weak strength. This suggests that humans and Neanderthals are not as closely related as two individuals of the same species are, but they are more related to each other than to chimpanzees.[18]
There have also been some attempts to decipher aDNA to provide valuable phenotypic information of ancient species. This is always done by mapping aDNA sequence onto the karyotype of a well-studied closely related species, which share a lot of similar phenotypic traits.[28] For example, Green et al. compared the aDNA sequence from Neanderthal Vi-80 fossil with modern human X and Y chromosome sequence, and they found a similarity in 2.18 and 1.62 bases per 10,000 respectively, suggesting Vi-80 sample was from a male individual.[28] Other similar studies include finding of a mutation associated with dwarfism in Arabidopsis in ancient Nubian cotton,[29] and investigation on the bitter taste perception locus in Neanderthals.[31]
Modern humans are thought to have evolved in Africa at least 200 kya (thousand years ago),[32] with some evidence suggesting a date of over 300 kya.[33] Examination of mitochondrial DNA (mtDNA), Y-chromosome DNA, and X-chromosome DNA indicate that the earliest population to leave Africa consisted of approximately 1500 males and females.[32] It has been suggested by various studies that populations were geographically “structured” to some degree prior to the expansion out of Africa; this is suggested by the antiquity of shared mtDNA lineages.[32] One study of 121 populations from various places throughout the continent found 14 genetic and linguistic “clusters,” suggesting an ancient geographic structure to African populations.[32] In general, genotypic and phenotypic analysis have shown “large and subdivided throughout much of their evolutionary history.”[32]
Genetic analysis has supported archaeological hypotheses of a large-scale migrations of Bantu speakers into Southern Africa approximately 5 kya.[32] Microsatellite DNA, single nucleotide polymorphisms (SNPs), and insertion/deletion polymorphisms (INDELS) have shown that Nilo-Saharan speaking populations originate from Sudan.[32] Furthermore, there is genetic evidence that Chad-speaking descendants of Nilo-Saharan speakers migrated from Sudan to Lake Chad about 8 kya.[32] Genetic evidence has also indicated that non-African populations made significant contributions to the African gene pool.[32] For example, the Saharan African Beja people have high levels of Middle-Eastern as well as East African Cushitic DNA.[32]
Analysis of mtDNA shows that modern humans occupied Eurasia in a single migratory event between 60 and 70 kya.[1] Genetic evidence shows that occupation of the Near East and Europe happened no earlier than 50 kya.[1] Studying haplogroup U has shown separate dispersals from the Near East both into Europe and into North Africa.[1]
Much of the work done in archaeogenetics focuses on the Neolithic transition in Europe.[34] Cavalli-Svorza's analysis of genetic-geographic patterns led him to conclude that there was a massive influx of Near Eastern populations into Europe at the start of the Neolithic.[34] This view led him “to strongly emphasize the expanding early farmers at the expense of the indigenous Mesolithic foraging populations.”[34] mtDNA analysis in the 1990s, however, contradicted this view. M.B. Richards estimated that 10–22% of extant European mtDNA's had come from Near Eastern populations during the Neolithic.[34] Most mtDNA's were “already established” among existing Mesolithic and Paleolithic groups.[34] Most “control-region lineages” of modern European mtDNA are traced to a founder event of reoccupying northern Europe towards the end of the Last Glacial Maximum (LGM).[1] One study of extant European mtDNA's suggest this reoccupation occurred after the end of the LGM, although another suggests it occurred before.[1][34] Analysis of haplogroups V, H, and U5 support a “pioneer colonization” model of European occupation, with incorporation of foraging populations into arriving Neolithic populations.[34] Furthermore, analysis of ancient DNA, not just extant DNA, is shedding light on some issues. For instance, comparison of Neolithic and mesolithic DNA has indicated that the development of dairying preceded widespread lactose tolerance.[34]
South Asia has served as the major early corridor for geographical dispersal of modern humans from out-of-Africa.[35] Based on studies of mtDNA line M, some have suggested that the first occupants of India were Austro-Asiatic speakers who entered about 45–60 kya.[35] The Indian gene pool has contributions from earliest settlers, as well as West Asian and Central Asian populations from migrations no earlier than 8 kya.[35] The lack of variation in mtDNA lineages compared to the Y-chromosome lineages indicate that primarily males partook in these migrations.[35] The discovery of two subbranches U2i and U2e of the U mtDNA lineage, which arose in Central Asia has “modulated” views of a large migration from Central Asia into India, as the two branches diverged 50 kya.[35] Furthermore, U2e is found in large percentages in Europe but not India, and vice versa for U2i, implying U2i is native to India.[35]
Analysis of mtDNA and NRY (non-recombining region of Y chromosome) sequences have indicated that the first major dispersal out of Africa went through Arabia and the Indian coast 50–100 kya, and a second major dispersal occurred 15–50 kya north of the Himalayas.[36]
Much work has been done to discover the extent of north-to-south and south-to-north migrations within Eastern Asia.[36] Comparing the genetic diversity of northeastern groups with southeastern groups has allowed archaeologists to conclude many of the northeast Asian groups came from the southeast.[36] The Pan-Asian SNP (single nucleotide polymorphism) study found “a strong and highly significant correlation between haplotype diversity and latitude,” which, when coupled with demographic analysis, supports the case for a primarily south-to-north occupation of East Asia.[36] Archaeogenetics has also been used to study hunter-gatherer populations in the region, such as the Ainu from Japan and Negrito groups in the Philippines.[36] For example, the Pan-Asian SNP study found that Negrito populations in Malaysia and the Negrito populations in the Philippines were more closely related to non-Negrito local populations than to each other, suggesting Negrito and non-Negrito populations are linked by one entry event into East Asia; although other Negrito groups do share affinities, including with Indigenous Australians.[36] A possible explanation of this is a recent admixture of some Negrito groups with their local populations.
Archaeogenetics has been used to better understand the populating of the Americas from Asia.[37] Native American mtDNA haplogroups have been estimated to be between 15 and 20 kya, although there is some variation in these estimates.[37] Genetic data has been used to propose various theories regarding how the Americas were colonized.[37] Although the most widely held theory suggests “three waves” of migration after the LGM through the Bering Strait, genetic data have given rise to alternative hypotheses.[37] For example, one hypothesis proposes a migration from Siberia to South America 20–15 kya and a second migration that occurred after glacial recession.[37] Y-chromosome data has led some to hold that there was a single migration starting from the Altai Mountains of Siberia between 17.2 and 10.1 kya, after the LGM.[37] Analysis of both mtDNA and Y-chromosome DNA reveals evidence of “small, founding populations.”[37] Studying haplogroups has led some scientists to conclude that a southern migration into the Americas from one small population was impossible, although separate analysis has found that such a model is feasible if such a migration happened along the coasts.[37]
Finally, archaeogenetics has been used to study the occupation of Australia and New Guinea.[38] The Indigenous people of Australia and New Guinea are phenotypically very similar, but mtDNA has shown that this is due to convergence from living in similar conditions.[38] Non-coding regions of mt-DNA have shown “no similarities” between the aboriginal populations of Australia and New Guinea.[38] Furthermore, no major NRY lineages are shared between the two populations. The high frequency of a single NRY lineage unique to Australia coupled with “low diversity of lineage-associated Y-chromosomal short tandem repeat (Y-STR) haplotypes” provide evidence for a “recent founder or bottleneck” event in Australia.[38] But there is relatively large variation in mtDNA, which would imply that the bottleneck effect impacted males primarily.[38] Together, NRY and mtDNA studies show that the splitting event between the two groups was over 50 kya, casting doubt on recent common ancestry between the two.[38]
Archaeogenetics has been used to understand the development of domestication of plants and animals.
The combination of genetics and archeological findings have been used to trace the earliest signs of plant domestication around the world. However, since the nuclear, mitochondrial, and chloroplast genomes used to trace domestication's moment of origin have evolved at different rates, its use to trace genealogy have been somewhat problematic.[39] Nuclear DNA in specific is used over mitochondrial and chloroplast DNA because of its faster mutation rate as well as its intraspecific variation due to a higher consistency of polymorphism genetic markers.[39] Findings in crop 'domestication genes' (traits that were specifically selected for or against) include
Through the study of archaeogenetics in plant domestication, signs of the first global economy can also be uncovered. The geographical distribution of new crops highly selected in one region found in another where it would have not originally been introduced serve as evidence of a trading network for the production and consumption of readily available resources.[39]
Archaeogenetics has been used to study the domestication of animals.[40] By analyzing genetic diversity in domesticated animal populations researchers can search for genetic markers in DNA to give valuable insight about possible traits of progenitor species.[40] These traits are then used to help distinguish archaeological remains between wild and domesticated specimens.[40] The genetic studies can also lead to the identification of ancestors for domesticated animals.[40] The information gained from genetics studies on current populations helps guide the Archaeologist's search for documenting these ancestors.[40]
Archaeogenetics has been used to trace the domestication of pigs throughout the old world.[41] These studies also reveal evidence about the details of early farmers.[41] Methods of Archaeogenetics have also been used to further understand the development of domestication of dogs.[42] Genetic studies have shown that all dogs are descendants from the gray wolf, however, it is currently unknown when, where, and how many times dogs were domesticated.[42] Some genetic studies have indicated multiple domestications while others have not.[42] Archaeological findings help better understand this complicated past by providing solid evidence about the progression of the domestication of dogs.[42] As early humans domesticated dogs the archaeological remains of buried dogs became increasingly more abundant.[42] Not only does this provide more opportunities for archaeologists to study the remains, it also provides clues about early human culture.[42]

---

# Origin of language

The origin of language, its relationship with human evolution, and its consequences have been subjects of study for centuries. Scholars wishing to study the origins of language draw inferences from evidence such as the fossil record, archaeological evidence, and contemporary language diversity. They may also study language acquisition as well as comparisons between human language and systems of animal communication (particularly other primates).[1] Many argue for the close relation between the origins of language and the origins of modern human behavior, but there is little agreement about the facts and implications of this connection.
The shortage of direct, empirical evidence has caused many scholars to regard the entire topic as unsuitable for serious study; in 1866, the Linguistic Society of Paris banned any existing or future debates on the subject, a prohibition which remained influential across much of the Western world until the late twentieth century.[2] Various hypotheses have been developed on the emergence of language.[3] While Charles Darwin's theory of evolution by natural selection had provoked a surge of speculation on the origin of language over a century and a half ago, the speculations had not resulted in a scientific consensus by 1996.[4] Despite this, academic interest had returned to the topic by the early 1990s. Linguists, archaeologists, psychologists, and anthropologists have renewed the investigation into the origin of language with modern methods.[5]
Attempts to explain the origin of language take a variety of forms:[6]
Most linguistic scholars as of 2024[update] favor continuity-based theories, but they vary in how they hypothesize language development.[citation needed] Some among those who consider language as mostly innate avoid speculating about specific precursors in nonhuman primates, stressing simply that the language faculty must have evolved gradually.[7]
Those who consider language as learned socially, such as Michael Tomasello, consider it developing from the cognitively controlled aspects of primate communication, mostly gestural rather than vocal.[8][9] Where vocal precursors are concerned, many continuity theorists envisage language as evolving from early human capacities for song.[10][11][12][13]
Noam Chomsky, a proponent of discontinuity theory, argues that a single change occurred in humans before leaving Africa, coincident with the Great Leap approximately 100,000 years ago, in which a common language faculty developed in a group of humans and their descendants. Chomsky bases his argument on the observation that any human baby of any culture can be raised in a different culture and will completely assimilate the language and behavior of the new culture in which they were raised. This implies that no major change to the human language faculty has occurred since they left Africa.[14]
Transcending the continuity-versus-discontinuity divide, some scholars view the emergence of language as the consequence of some kind of social transformation[15] that, by generating unprecedented levels of public trust, liberated a genetic potential for linguistic creativity that had previously lain dormant.[16][17][18] "Ritual/speech coevolution theory" exemplifies this approach.[19][20] Scholars in this intellectual camp point to the fact that even chimpanzees and bonobos have latent symbolic capacities that they rarely—if ever—use in the wild.[21] Objecting to the sudden mutation idea, these authors argue that even if a chance mutation were to install a language organ in an evolving bipedal primate, it would be adaptively useless under all known primate social conditions. A very specific social structure – one capable of upholding unusually high levels of public accountability and trust – must have evolved before or concurrently with language to make reliance on "cheap signals" (e.g. words) an evolutionarily stable strategy.
Since the emergence of language lies so far back in human prehistory, the relevant developments have left no direct historical traces, and comparable processes cannot be observed today. Despite this, the emergence of new sign languages in modern times—Nicaraguan Sign Language, for example—may offer insights into the developmental stages and creative processes necessarily involved.[22] Another approach inspects early human fossils, looking for traces of physical adaptation to language use.[23][24] In some cases, when the DNA of extinct humans can be recovered, the presence or absence of genes considered to be language-relevant—FOXP2, for example—may prove informative.[25] Another approach, this time archaeological, involves invoking symbolic behavior (such as repeated ritual activity) that may leave an archaeological trace—such as mining and modifying ochre pigments for body-painting—while developing theoretical arguments to justify inferences from symbolism in general to language in particular.[26][27][28]
The time range for the evolution of language or its anatomical prerequisites extends, at least in principle, from the phylogenetic divergence of Homo from Pan to the emergence of full behavioral modernity some 50,000–150,000 years ago. Few dispute that Australopithecus probably lacked vocal communication significantly more sophisticated than that of great apes in general,[29] but scholarly opinions vary as to the developments since the appearance of Homo some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as Homo habilis, while others place the development of symbolic communication only with Homo erectus (1.8 million years ago) or with Homo heidelbergensis (0.6 million years ago) and the development of language proper with Homo sapiens, currently estimated at less than 200,000 years ago.
Using statistical methods to estimate the time required to achieve the current spread and diversity in modern languages, Johanna Nichols—a linguist at the University of California, Berkeley—argued in 1998 that vocal languages must have begun diversifying in the human species at least 100,000 years ago.[30] Estimates of this kind are not universally accepted, but jointly considering genetic, archaeological, palaeontological, and much other evidence indicates that language likely emerged somewhere in sub-Saharan Africa during the Middle Stone Age, roughly contemporaneous with the speciation of Homo sapiens.[31]
I cannot doubt that language owes its origin to the imitation and modification, aided by signs and gestures, of various natural sounds, the voices of other animals, and man's own instinctive cries.
In 1861, historical linguist Max Müller published a list of speculative theories concerning the origins of spoken language:[33]
Most scholars today consider all such theories not so much wrong—they occasionally offer peripheral insights—as naïve and irrelevant.[35][36] The problem with these theories is that they rest on the assumption that once early humans had discovered a workable mechanism for linking sounds with meanings, language would automatically have evolved.[citation needed]
Much earlier, medieval Muslim scholars developed theories on the origin of language.[37][38] Their theories were of five general types:[39]
From the perspective of signalling theory, the main obstacle to the evolution of language-like communication in nature is not a mechanistic one. Rather, it is the fact that symbols—arbitrary associations of sounds or other perceptible forms with corresponding meanings—are unreliable and may as well be false.[40][41][42] The problem of reliability was not recognized at all by Darwin, Müller or the other early evolutionary theorists.
Animal vocal signals are, for the most part, intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. The signal is trusted, not because the cat is inclined to be honest, but because it just cannot fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason—because they are hard to fake.[43] Primate social intelligence is "Machiavellian"; that is, self-serving and unconstrained by moral scruples. Monkeys, apes and particularly humans often attempt to deceive each other, while at the same time remaining constantly on guard against falling victim to deception themselves.[44][45] Paradoxically, it is theorized that primates' resistance to deception is what blocks the evolution of their signalling systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.[19]
Words are easy to fake. Should they turn out to be lies, listeners will adapt by ignoring them in favor of hard-to-fake indices or cues. For language to work, listeners must be confident that those with whom they are on speaking terms are generally likely to be honest.[46] A peculiar feature of language is displaced reference, which means reference to topics outside the currently perceptible situation. This property prevents utterances from being corroborated in the immediate "here" and "now". For this reason, language presupposes relatively high levels of mutual trust in order to become established over time as an evolutionarily stable strategy. This stability is born of a longstanding mutual trust and is what grants language its authority. A theory of the origins of language must therefore explain why humans could begin trusting cheap signals in ways that other animals apparently cannot.
The "mother tongues" hypothesis was proposed in 2004 as a possible solution to this problem.[47] W. Tecumseh Fitch suggested that the Darwinian principle of "kin selection"[48]—the convergence of genetic interests between relatives—might be part of the answer. Fitch suggests that languages were originally "mother tongues". If language evolved initially for communication between mothers and their own biological offspring, extending later to include adult relatives as well, the interests of speakers and listeners would have tended to coincide. Fitch argues that shared genetic interests would have led to sufficient trust and cooperation for intrinsically unreliable signals—words—to become accepted as trustworthy and so begin evolving for the first time.[49]
Critics of this theory point out that kin selection is not unique to humans.[50] So even if one accepts Fitch's initial premises, the extension of the posited "mother tongue" networks from close relatives to more distant relatives remains unexplained.[50] Fitch argues, however, that the extended period of physical immaturity of human infants and the postnatal growth of the human brain give the human-infant relationship a different and more extended period of intergenerational dependency than that found in any other species.[47]
Ib Ulbæk[6] invokes another standard Darwinian principle—"reciprocal altruism"[51]—to explain the unusually high levels of intentional honesty necessary for language to evolve. "Reciprocal altruism" can be expressed as the principle that if you scratch my back, I'll scratch yours. In linguistic terms, it would mean that if you speak truthfully to me, I'll speak truthfully to you. Ordinary Darwinian reciprocal altruism, Ulbæk points out, is a relationship established between frequently interacting individuals. For language to prevail across an entire community, however, the necessary reciprocity would have needed to be enforced universally instead of being left to individual choice. Ulbæk concludes that for language to evolve, society as a whole must have been subject to moral regulation.
Critics point out that this theory fails to explain when, how, why or by whom "obligatory reciprocal altruism" could possibly have been enforced.[20] Various proposals have been offered to remedy this defect.[20] A further criticism is that language does not work on the basis of reciprocal altruism anyway. Humans in conversational groups do not withhold information to all except listeners likely to offer valuable information in return. On the contrary, they seem to want to advertise to the world their access to socially relevant information, broadcasting that information without expectation of reciprocity to anyone who will listen.[52]
Gossip, according to Robin Dunbar in his book Grooming, Gossip and the Evolution of Language, language does for group-living humans what manual grooming does for other primates—it allows individuals to service their relationships and so maintain their alliances on the basis of the principle: if you scratch my back, I'll scratch yours. Dunbar argues that as humans began living in increasingly larger social groups, the task of manually grooming all one's friends and acquaintances became so time-consuming as to be unaffordable.[53] In response to this problem, humans developed "a cheap and ultra-efficient form of grooming"—vocal grooming. To keep allies happy, one now needs only to "groom" them with low-cost vocal sounds, servicing multiple allies simultaneously while keeping both hands free for other tasks. Vocal grooming then evolved gradually into vocal language—initially in the form of "gossip".[53] Dunbar's hypothesis seems to be supported by adaptations, in the structure of language, to the function of narration in general.[54]
Critics of this theory point out that the efficiency of "vocal grooming"—the fact that words are so cheap—would have undermined its capacity to signal commitment of the kind conveyed by time-consuming and costly manual grooming.[55] A further criticism is that the theory does nothing to explain the crucial transition from vocal grooming—the production of pleasing but meaningless sounds—to the cognitive complexities of syntactical speech.
The ritual/speech coevolution theory was originally proposed by social anthropologist Roy Rappaport[56] before being elaborated by anthropologists such as Chris Knight,[57] Jerome Lewis,[58] Nick Enfield,[59] Camilla Power[60] and Ian Watts.[61] Cognitive scientist and robotics engineer Luc Steels[62] is another prominent supporter of this general approach, as is biological anthropologist and neuroscientist Terrence Deacon.[63] A more recent champion of the approach is the Chomskyan specialist in linguistic syntax, Cedric Boeckx.[64]
These scholars argue that there can be no such thing as a "theory of the origins of language". This is because language is not a separate adaptation, but an internal aspect of something much wider—namely, the entire domain known to anthropologists as human symbolic culture.[65] Attempts to explain language independently of this wider context have failed, say these scientists, because they are addressing a problem with no solution. Language would not work outside its necessary environment of confidence-building social mechanisms and institutions. For example, it would not work for a nonhuman ape communicating with others of its kind in the wild. Not even the cleverest nonhuman ape could make language work under such conditions.
Lie and alternative, inherent in language ... pose problems to any society whose structure is founded on language, which is to say all human societies. I have therefore argued that if there are to be words at all it is necessary to establish The Word, and that The Word is established by the invariance of liturgy.
Advocates of this school of thought point out that words are cheap. Should an especially clever nonhuman ape, or even a group of articulate nonhuman apes, try to use words in the wild, they would carry no conviction. The primate vocalizations that do carry conviction—those they actually use—are unlike words, in that they are emotionally expressive, intrinsically meaningful, and reliable because they are relatively costly and hard to fake.
Oral and gestural languages consist of pattern-making whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world—they are a theoretical impossibility.[67] Being intrinsically unreliable, language works only if one can build up a reputation for trustworthiness within a certain kind of society—namely, one where symbolic cultural facts (sometimes called "institutional facts") can be established and maintained through collective social endorsement.[68] In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective ritual.[69] Therefore, the task facing researchers into the origins of language is more multidisciplinary than is usually supposed. It involves addressing the evolutionary emergence of human ritual, kinship, religion and symbolic culture taken as a whole, with language an important but subsidiary component.
In a 2023 article, Cedric Boeckx[64] endorses the Rappaport/Searle/Knight way of capturing the "special" nature of human words. Words are symbols. This means that, from a standpoint in Darwinian signal evolution theory, they are "patently false signals." Words are facts, but "facts whose existence depends entirely on subjective belief".[70] In philosophical terms, they are "institutional facts": fictions that are granted factual status within human social institutions[71] From this standpoint, according to Boeckx, linguistic utterances are symbolic to the extent that they are patent falsehoods serving as guides to communicative intentions. "They are communicatively useful untruths, as it were."[64] The reason why words can survive among humans despite being false is largely down to a matter of trust. The corresponding origins theory is that language can only have begun to evolve from the moment humans started reciprocally faking in communicatively helpful ways, i.e., when they became capable of upholding the levels of trust necessary for linguistic communication to work.
The point here is that an ape or other nonhuman must always carry at least some of the burden of generating the trust necessary for communication to work. That is, in order to be taken seriously, each signal it emits must be a patently reliable one, trusted because it is rooted in some way in the real world. But now imagine what might happen under social conditions where trust could be taken for granted. The signaller could stop worrying about reliability and concentrate instead on perceptual discriminability. Carried to its conclusion, this should permit digital signaling—the cheapest and most efficient kind of communication.
From this philosophical standpoint, animal communication cannot be digital because it does not have the luxury of being patently false. Costly signals of any kind can only be evaluated on an analog scale. Put differently, truly symbolic, digital signals become socially acceptable only under highly unusual conditions—such as those internal to a ritually bonded community whose members are not tempted to lie.[citation needed]
Critics of the speech/ritual co-evolution idea theory include Noam Chomsky, who terms it the "non-existence" hypothesis—a denial of the very existence of language as an object of study for natural science.[72] Chomsky's own theory is that language emerged in an instant and in perfect form,[73] prompting his critics in turn, to retort that only something that does not exist—a theoretical construct or convenient scientific fiction—could possibly emerge in such a miraculous way.[17] The controversy remains unresolved.
Acheulean tool use began during the Lower Paleolithic approximately 1.75 million years ago. Studies focusing on the lateralization of Acheulean tool production and language production have noted similar areas of blood flow when engaging in these activities separately; this theory suggests that the brain functions needed for the production of tools across generations is consistent with the brain systems required for producing language. Researchers used functional transcranial Doppler ultrasonography (fTDC) and had participants perform activities related to the creation of tools using the same methods during the Lower Paleolithic as well as a task designed specifically for word generation.[74] The purpose of this test was to focus on the planning aspect of Acheulean tool making and cued word generation in language (an example of cued word generation would be trying to list all words beginning with a given letter). Theories of language developing alongside tool use has been theorized by multiple individuals;[75][76][77] however, until recently, there has been little empirical data to support these hypotheses. Focusing on the results of the study performed by Uomini et al. evidence for the usage of the same brain areas has been found when looking at cued word generation and Acheulean tool use. The relationship between tool use and language production is found in working and planning memory respectively and was found to be similar across a variety of participants, furthering evidence that these areas of the brain are shared.[74] This evidence lends credibility to the theory that language developed alongside tool use in the Lower Paleolithic.
The humanistic tradition considers language as a human invention. Renaissance philosopher Antoine Arnauld gave a detailed description of his idea of the origin of language in Port-Royal Grammar. According to Arnauld, people are social and rational by nature, and this urged them to create language as a means to communicate their ideas to others. Language construction would have occurred through a slow and gradual process.[78] In later theory, especially in functional linguistics, the primacy of communication is emphasised over psychological needs.[79]
The exact way language evolved is however not considered as vital to the study of languages. Structural linguist Ferdinand de Saussure abandoned evolutionary linguistics after having come to the firm conclusion that it would not be able to provide any further revolutionary insight after the completion of the major works in historical linguistics by the end of the 19th century. Saussure was particularly sceptical of the attempts of August Schleicher and other Darwinian linguists to access prehistorical languages through series of reconstructions of proto-languages.[80]
Saussure's solution to the problem of language evolution involves dividing theoretical linguistics in two. Evolutionary and historical linguistics are renamed as diachronic linguistics. It is the study of language change, but it has only limited explanatory power due to the inadequacy of all of the reliable research material that could ever be made available. Synchronic linguistics, in contrast, aims to widen scientists' understanding of language through a study of a given contemporary or historical language stage as a system in its own right.[81]
Although Saussure put much focus on diachronic linguistics, later structuralists who equated structuralism with the synchronic analysis were sometimes criticised of ahistoricism. According to structural anthropologist Claude Lévi-Strauss, language and meaning—in opposition to "knowledge, which develops slowly and progressively"—must have appeared in an instant.[82]
Structuralism, as first introduced to sociology by Émile Durkheim, is nonetheless a type of humanistic evolutionary theory which explains diversification as necessitated by growing complexity.[83] There was a shift of focus to functional explanation after Saussure's death. Functional structuralists including the Prague Circle linguists and André Martinet explained the growth and maintenance of structures as being necessitated by their functions.[79] For example, novel technologies make it necessary for people to invent new words, but these may lose their function and be forgotten as the technologies are eventually replaced by more modern ones.
According to Chomsky's single-mutation theory, the emergence of language resembled the formation of a crystal; with digital infinity as the seed crystal in a super-saturated primate brain, on the verge of blossoming into the human mind, by physical law, once evolution added a single small but crucial keystone.[84][85] Thus, in this theory, language appeared rather suddenly within the history of human evolution. Chomsky, writing with computational linguist and computer scientist Robert C. Berwick, suggests that this scenario is completely compatible with modern biology. They note that "none of the recent accounts of human language evolution seem to have completely grasped the shift from conventional Darwinism to its fully stochastic modern version—specifically, that there are stochastic effects not only due to sampling like directionless drift, but also due to directed stochastic variation in fitness, migration, and heritability—indeed, all the "forces" that affect individual or gene frequencies ... All this can affect evolutionary outcomes—outcomes that as far as we can make out are not brought out in recent books on the evolution of language, yet would arise immediately in the case of any new genetic or individual innovation, precisely the kind of scenario likely to be in play when talking about language's emergence."
Citing evolutionary geneticist Svante Pääbo, they concur that a substantial difference must have occurred to differentiate Homo sapiens from Neanderthals to "prompt the relentless spread of our species, who had never crossed open water, up and out of Africa and then on across the entire planet in just a few tens of thousands of years. ... What we do not see is any kind of 'gradualism' in new tool technologies or innovations like fire, shelters, or figurative art." Berwick and Chomsky therefore suggest language emerged approximately between 200,000 years ago and 60,000 years ago (between the appearance of the first anatomically modern humans in southern Africa and the last exodus from Africa respectively). "That leaves us with about 130,000 years, or approximately 5,000–6,000 generations of time for evolutionary change. This is not 'overnight in one generation' as some have (incorrectly) inferred—but neither is it on the scale of geological eons. It's time enough—within the ballpark for what Nilsson and Pelger (1994) estimated as the time required for the full evolution of a vertebrate eye from a single cell, even without the invocation of any 'evo-devo' effects."[86]
The single-mutation theory of language evolution has been directly questioned on different grounds. A formal analysis of the probability of such a mutation taking place and going to fixation in the species has concluded that such a scenario is unlikely, with multiple mutations with more moderate fitness effects being more probable.[87] Another criticism has questioned the logic of the argument for single mutation and puts forward that from the formal simplicity of Merge, the capacity Berwick and Chomsky deem the core property of human language that emerged suddenly, one cannot derive the (number of) evolutionary steps that led to it.[88]
The Romulus and Remus hypothesis, proposed by neuroscientist Andrey Vyshedskiy, seeks to address the question as to why the modern speech apparatus originated over 500,000 years before the earliest signs of modern human imagination. This hypothesis proposes that there were two phases that led to modern recursive language. The phenomenon of recursion occurs across multiple linguistic domains, arguably most prominently in syntax and morphology. Thus, by nesting a structure such as a sentence or a word within themselves, it enables the generation of potentially (countably) infinite new variations of that structure. For example, the base sentence [Peter likes apples.] can be nested in irrealis clauses to produce [Mary said [Peter likes apples.]], [Paul believed [Mary said [Peter likes apples.]]] and so forth.[89]
The first phase includes the slow development of non-recursive language with a large vocabulary along with the modern speech apparatus, which includes changes to the hyoid bone, increased voluntary control of the muscles of the diaphragm, and the evolution of the FOXP2 gene, as well as other changes by 600,000 years ago.[90] Then, the second phase was a rapid Chomskian single step, consisting of three distinct events that happened in quick succession around 70,000 years ago and allowed the shift from non-recursive to recursive language in early hominins.
It is not enough for children to have a modern prefrontal cortex (PFC) to allow the development of PFS; the children must also be mentally stimulated and have recursive elements already in their language to acquire PFS. Since their parents would not have invented these elements yet, the children would have had to do it themselves, which is a common occurrence among young children that live together, in a process called cryptophasia.[92] This means that delayed PFC development would have allowed more time to acquire PFS and develop recursive elements.
Delayed PFC development also comes with negative consequences, such as a longer period of reliance on one's parents to survive and lower survival rates. For modern language to have occurred, PFC delay had to have an immense survival benefit in later life, such as PFS ability. This suggests that the mutation that caused PFC delay and the development of recursive language and PFS occurred simultaneously, which lines up with evidence of a genetic bottleneck around 70,000 years ago.[93] This could have been the result of a few individuals who developed PFS and recursive language which gave them significant competitive advantage over all other humans at the time.[91]
The gestural theory states that human language developed from gestures that were used for simple communication.
Research has found strong support for the idea that oral communication and sign language depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language.[96] Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.[97]
Primate gesture is at least partially genetic: different nonhuman apes will perform gestures characteristic of their species, even if they have never seen another ape perform that gesture. For example, gorillas beat their breasts. This shows that gestures are an intrinsic and important part of primate communication, which supports the idea that language evolved from gesture.[98]
Further evidence suggests that gesture and language are linked. In humans, manually gesturing has an effect on concurrent vocalizations, thus creating certain natural vocal associations of manual efforts. Chimpanzees move their mouths when performing fine motor tasks. These mechanisms may have played an evolutionary role in enabling the development of intentional vocal communication as a supplement to gestural communication. Voice modulation could have been prompted by preexisting manual actions.[98]
From infancy, gestures both supplement and predict speech.[99][100] This addresses the idea that gestures quickly change in humans from a sole means of communication (from a very young age) to a supplemental and predictive behavior that is used despite the ability to communicate verbally. This too serves as a parallel to the idea that gestures developed first and language subsequently built upon it.
Two possible scenarios have been proposed for the development of language,[101] one of which supports the gestural theory:
The first perspective that language evolved from the calls of human ancestors seems logical because both humans and animals make sounds or cries. One evolutionary reason to refute this is that, anatomically, the centre that controls calls in monkeys and other animals is located in a completely different part of the brain than in humans. In monkeys, this centre is located in the depths of the brain related to emotions. In the human system, it is located in an area unrelated to emotion. Humans can communicate simply to communicate—without emotions. So, anatomically, this scenario does not work.[101] This suggests that language was derived from gesture[102](humans communicated by gesture first and sound was attached later).
The important question for gestural theories is why there was a shift to vocalization. Various explanations have been proposed:
A comparable hypothesis states that in 'articulate' language, gesture and vocalisation are intrinsically linked, as language evolved from equally intrinsically linked dance and song.[13]
Humans still use manual and facial gestures when they speak, especially when people meet who have no language in common.[106] There are also a great number of sign languages still in existence, commonly associated with Deaf communities. These sign languages are equal in complexity, sophistication, and expressive power, to any oral language.[107] The cognitive functions are similar and the parts of the brain used are similar. The main difference is that the "phonemes" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.[108] (Compare the motor theory of speech perception.)
Critics of gestural theory note that it is difficult to name serious reasons why the initial pitch-based vocal communication (which is present in primates) would be abandoned in favor of the much less effective non-vocal, gestural communication.[109] However, Michael Corballis has pointed out that it is supposed that primate vocal communication (such as alarm calls) cannot be controlled consciously, unlike hand movement, and thus it is not credible as precursor to human language; primate vocalization is rather homologous to and continued in involuntary reflexes (connected with basic human emotions) such as screams or laughter (the fact that these can be faked does not disprove the fact that genuine involuntary responses to fear or surprise exist).[102] Also, gesture is not generally less effective, and depending on the situation can even be advantageous, for example in a loud environment or where it is important to be silent, such as on a hunt. Other challenges to the "gesture-first" theory have been presented by researchers in psycholinguistics, including David McNeill.[110]
Proponents of the motor theory of language evolution have primarily focused on the visual domain and communication through observation of movements. The Tool-use sound hypothesis suggests that the production and perception of sound also contributed substantially, particularly incidental sound of locomotion (ISOL) and tool-use sound (TUS).[111] Human bipedalism resulted in rhythmic and more predictable ISOL. That may have stimulated the evolution of musical abilities, auditory working memory, and abilities to produce complex vocalizations, and to mimic natural sounds.[112] Since the human brain proficiently extracts information about objects and events from the sounds they produce, TUS, and mimicry of TUS, might have achieved an iconic function. The prevalence of sound symbolism in many extant languages supports this idea. Self-produced TUS activates multimodal brain processing (motor neurons, hearing, proprioception, touch, vision), and TUS stimulates primate audiovisual mirror neurons, which is likely to stimulate the development of association chains. Tool use and auditory gestures involve motor-processing of the forelimbs, which is associated with the evolution of vertebrate vocal communication. The production, perception, and mimicry of TUS may have resulted in a limited number of vocalizations or protowords that were associated with tool use.[111] A new way to communicate about tools, especially when out of sight, would have had selective advantage. A gradual change in acoustic properties, meaning, or both could have resulted in arbitrariness and an expanded repertoire of words. Humans have been increasingly exposed to TUS over millions of years, coinciding with the period during which spoken language evolved.
In humans, functional MRI studies have reported finding areas homologous to the monkey mirror neuron system in the inferior frontal cortex, close to Broca's area, one of the language regions of the brain. This has led to suggestions that human language evolved from a gesture performance/understanding system implemented in mirror neurons. Mirror neurons have been said to have the potential to provide a mechanism for action-understanding, imitation-learning, and the simulation of other people's behavior.[113] This hypothesis is supported by some cytoarchitectonic homologies between monkey premotor area F5 and human Broca's area.[114]
Rates of vocabulary expansion link to the ability of children to vocally mirror non-words and so to acquire the new word pronunciations. Such speech repetition occurs automatically, quickly[115] and separately in the brain to speech perception.[116][117] Moreover, such vocal imitation can occur without comprehension such as in speech shadowing[118] and echolalia.[114][119] Further evidence for this link comes from a recent study in which the brain activity of two participants was measured using fMRI while they were gesturing words to each other using hand gestures with a game of charades—a modality that some have suggested might represent the evolutionary precursor of human language. Analysis of the data using Granger Causality revealed that the mirror-neuron system of the observer indeed reflects the pattern of activity of in the motor system of the sender, supporting the idea that the motor concept associated with the words is indeed transmitted from one brain to another using the mirror system.[120]
Not all linguists agree with the above arguments, however. In particular, supporters of Noam Chomsky argue against the possibility that the mirror neuron system can play any role in the hierarchical recursive structures essential to syntax.[121]
According to Dean Falk's "putting-down-the-baby" theory, vocal interactions between early hominid mothers and infants began a sequence of events that led, eventually, to human ancestors' earliest words.[122] The basic idea is that evolving human mothers, unlike their counterparts in other primates, could not move around and forage with their infants clinging onto their backs. Loss of fur in the human case left infants with no means of clinging on. Frequently, therefore, mothers had to put their babies down. As a result, these babies needed to be reassured that they were not being abandoned. Mothers responded by developing 'motherese'—an infant-directed communicative system embracing facial expressions, body language, touching, patting, caressing, laughter, tickling, and emotionally expressive contact calls. The argument is that language developed out of this interaction.[122]
In The Mental and Social Life of Babies, psychologist Kenneth Kaye noted that no usable adult language could have evolved without interactive communication between very young children and adults. "No symbolic system could have survived from one generation to the next if it could not have been easily acquired by young children under their normal conditions of social life."[123]
The "from where to what" model is a language evolution model that is derived primarily from the organization of language processing in the brain into two structures: the auditory dorsal stream and the auditory ventral stream.[124][125] It hypothesizes seven stages of language evolution (see illustration). Speech originated for the purpose of exchanging contact calls between mothers and their offspring to find one another in the event they became separated (illustration part 1). The contact calls could be modified with intonations in order to express either a higher or lower level of distress (illustration part 2). The use of two types of contact calls enabled the first question-answer conversation. In this scenario, the child would emit a low-level distress call to express a desire to interact with an object, and the mother would respond with either another low-level distress call (to express approval of the interaction) or a high-level distress call (to express disapproval) (illustration part 3). Over time, the improved use of intonations and vocal control led to the invention of unique calls (phonemes) associated with distinct objects (illustration part 4). At first, children learned the calls (phonemes) from their parents by imitating their lip-movements (illustration part 5). Eventually, infants were able to encode into long-term memory all the calls (phonemes). Consequentially, mimicry via lip-reading was limited to infancy and older children learned new calls through mimicry without lip-reading (illustration part 6). Once individuals became capable of producing a sequence of calls, this allowed multi-syllabic words, which increased the size of their vocabulary (illustration part 7). The use of words, composed of sequences of syllables, provided the infrastructure for communicating with sequences of words (i.e. sentences).
The theory's name is derived from the two auditory streams, which are both found in the brains of humans and other primates. The auditory ventral stream is responsible for sound recognition, and so it is referred to as the auditory what stream.[126][127][128] In primates, the auditory dorsal stream is responsible for sound localization, and thus it is called the auditory where stream. Only in humans (in the left hemisphere) is it also responsible for other processes associated with language use and acquisition, such as speech repetition and production, integration of phonemes with their lip movements, perception and production of intonations, phonological long-term memory (long-term memory storage of the sounds of words), and phonological working memory (the temporary storage of the sounds of words).[129][130][131][132][133][134][135][136] Some evidence also indicates a role in recognizing others by their voices.[137][138] The emergence of each of these functions in the auditory dorsal stream represents an intermediate stage in the evolution of language.
A contact call origin for human language is consistent with animal studies, as like human language, contact call discrimination in monkeys is lateralised to the left hemisphere.[139][140] Mice with knock-out to language related genes (such as FOXP2 and SRPX2) also resulted in the pups no longer emitting contact calls when separated from their mothers.[141][142] Supporting this model is also its ability to explain unique human phenomena, such as the use of intonations when converting words into commands and questions, the tendency of infants to mimic vocalizations during the first year of life (and its disappearance later on) and the protruding and visible human lips, which are not found in other apes. This theory could be considered an elaboration of the putting-down-the-baby theory of language evolution.
"Grammaticalization" is a continuous historical process in which free-standing words develop into grammatical appendages, while these in turn become ever more specialized and grammatical. An initially "incorrect" usage, in becoming accepted, leads to unforeseen consequences, triggering knock-on effects and extended sequences of change. Paradoxically, grammar evolves because, in the final analysis, humans care less about grammatical niceties than about making themselves understood.[143] If this is how grammar evolves today, according to this school of thought, similar principles at work can be legitimately inferred among distant human ancestors, when grammar itself was first being established.[144][145][146]
In order to reconstruct the evolutionary transition from early language to languages with complex grammars, it is necessary to know which hypothetical sequences are plausible and which are not. In order to convey abstract ideas, the first recourse of speakers is to fall back on immediately recognizable concrete imagery, very often deploying metaphors rooted in shared bodily experience.[147] A familiar example is the use of concrete terms such as "belly" or "back" to convey abstract meanings such as "inside" or "behind". Equally metaphorical is the strategy of representing temporal patterns on the model of spatial ones. For example, English speakers might say "It is going to rain", modelled on "I am going to London." This can be abbreviated colloquially to "It's gonna rain." Even when in a hurry, English speakers do not say "I'm gonna London"—the contraction is restricted to the job of specifying tense. From such examples it can be seen why grammaticalisation is consistently unidirectional—from concrete to abstract meaning, not the other way around.[144]
Grammaticalization theorists picture early language as simple, perhaps consisting only of nouns.[146]p. 111 Even under that extreme theoretical assumption, however, it is difficult to imagine what would realistically have prevented people from using, say, "spear" as if it were a verb ("Spear that pig!"). People might have used their nouns as verbs or their verbs as nouns as occasion demanded. In short, while a noun-only language might seem theoretically possible, grammaticalization theory indicates that it cannot have remained fixed in that state for any length of time.[144][148]
Creativity drives grammatical change.[148] This presupposes a certain attitude on the part of listeners. Instead of punishing deviations from accepted usage, listeners must prioritise imaginative mind-reading. Imaginative creativity—emitting a leopard alarm when no leopard was present, for example—is not the kind of behaviour which, say, vervet monkeys would appreciate or reward.[149] Creativity and reliability are incompatible demands; for "Machiavellian" primates as for animals generally, the overriding pressure is to demonstrate reliability.[150] If humans escape these constraints, it is because in their case, listeners are primarily interested in mental states.
To focus on mental states is to accept fictions—inhabitants of the imagination—as potentially informative and interesting. An example is metaphor: a metaphor is, literally, a false statement.[151] In Romeo and Juliet, Romeo declares "Juliet is the sun!". Juliet is a woman, not a ball of plasma in the sky, but human listeners are not (or not usually) pedants insistent on point-by-point factual accuracy. They want to know what the speaker has in mind. Grammaticalisation is essentially based on metaphor. To outlaw its use would be to stop grammar from evolving and, by the same token, to exclude all possibility of expressing abstract thought.[147][152]
A criticism of all this is that while grammaticalization theory might explain language change today, it does not satisfactorily address the really difficult challenge—explaining the initial transition from primate-style communication to language as it is known today. Rather, the theory assumes that language already exists. As Bernd Heine and Tania Kuteva acknowledge: "Grammaticalisation requires a linguistic system that is used regularly and frequently within a community of speakers and is passed on from one group of speakers to another".[146] Outside modern humans, such conditions do not prevail.
Human language is used for self-expression; however, expression displays different stages. The consciousness of self and feelings represents the stage immediately prior to the external, phonetic expression of feelings in the form of sound (i.e. language). Intelligent animals such as dolphins, Eurasian magpies, and chimpanzees live in communities, wherein they assign themselves roles for group survival and show emotions such as sympathy.[153] When such animals view their reflection (mirror test), they recognize themselves and exhibit self-consciousness.[154] Notably, humans evolved in a quite different environment than that of these animals. Human survival became easier with the development of tools, shelter, and fire, thus facilitating further advancement of social interaction, self-expression, and tool-making, as for hunting and gathering.[155] The increasing brain size allowed advanced provisioning and tools and the technological advances during the Palaeolithic era that built upon the previous evolutionary innovations of bipedalism and hand versatility allowed the development of human language.[citation needed]
According to a study investigating the song differences between white-rumped munias and its domesticated counterpart (Bengalese finch), the wild munias use a highly stereotyped song sequence, whereas the domesticated ones sing a highly unconstrained song. In wild finches, song syntax is subject to female preference—sexual selection—and remains relatively fixed. However, in the Bengalese finch, natural selection is replaced by breeding, in this case for colorful plumage, and thus, decoupled from selective pressures, stereotyped song syntax is allowed to drift. It is replaced, supposedly within 1000 generations, by a variable and learned sequence. Wild finches, moreover, are thought incapable of learning song sequences from other finches.[156] In the field of bird vocalization, brains capable of producing only an innate song have very simple neural pathways: the primary forebrain motor centre, called the robust nucleus of arcopallium, connects to midbrain vocal outputs, which in turn project to brainstem motor nuclei. By contrast, in brains capable of learning songs, the arcopallium receives input from numerous additional forebrain regions, including those involved in learning and social experience. Control over song generation has become less constrained, more distributed, and more flexible.[156]
One way to think about human evolution is that humans are self-domesticated apes. Just as domestication relaxed selection for stereotypic songs in the finches—mate choice was supplanted by choices made by the aesthetic sensibilities of bird breeders and their customers—so might human cultural domestication have relaxed selection on many of their primate behavioural traits, allowing old pathways to degenerate and reconfigure. Given the highly indeterminate way that mammalian brains develop—they basically construct themselves "bottom up", with one set of neuronal interactions preparing for the next round of interactions—degraded pathways would tend to seek out and find new opportunities for synaptic hookups. Such inherited de-differentiations of brain pathways might have contributed to the functional complexity that characterises human language. And, as exemplified by the finches, such de-differentiations can occur in very rapid time-frames.[157]
A distinction can be drawn between speech and language. Language is not necessarily spoken: it might alternatively be written or signed. Speech is among a number of different methods of encoding and transmitting linguistic information, albeit arguably[by whom?] the most natural one.[158]
Some scholars, such as Noam Chomsky, view language as an initially cognitive development, its "externalisation" to serve communicative purposes occurring later in human evolution. According to one such school of thought, the key feature distinguishing human language is recursion,[159] (in this context, the iterative embedding of phrases within phrases). Other scholars—notably Daniel Everett—deny that recursion is universal, citing certain languages (e.g. Pirahã) which allegedly[by whom?] lack this feature.[160]
The ability to ask questions is considered by some[like whom?] to distinguish language from non-human systems of communication.[161] Some captive primates (notably bonobos and chimpanzees), having learned to use rudimentary signing to communicate with their human trainers, proved able to respond correctly to complex questions and requests. Yet they failed to ask even the simplest questions themselves.[162] Conversely, human children are able to ask their first questions (using only question intonation) at the babbling period of their development, long before they start using syntactic structures. Although babies from different cultures acquire native languages from their social environment, all languages of the world without exception—tonal, non-tonal, intonational and accented—use similar rising "question intonation" for yes–no questions.[163][164] Except, of course, the ones that don't.[165] [clarification needed] This fact is a strong evidence of the universality of question intonation. In general, according to some authors[like whom?], sentence intonation/pitch is pivotal in spoken grammar and is the basic information used by children to learn the grammar of whatever language.[13]
Language users have high-level reference (or deixis)—the ability to refer to things or states of being that are not in the immediate realm of the speaker. This ability is often related to theory of mind, or an awareness of the other as a being like the self with individual wants and intentions. According to Chomsky, Hauser and Fitch (2002), there are six main aspects of this high-level reference system:
Simon Baron-Cohen (1999) argues that theory of mind must have preceded language use, based on evidence of use of the following characteristics as much as 40,000 years ago: intentional communication, repairing failed communication, teaching, intentional persuasion, intentional deception, building shared plans and goals, intentional sharing of focus or topic, and pretending. Moreover, Baron-Cohen argues that many primates show some, but not all, of these abilities.[citation needed] Call and Tomasello's research on chimpanzees supports this, in that individual chimps seem to understand that other chimps have awareness, knowledge, and intention, but do not seem to understand false beliefs. Many primates show some tendencies toward a theory of mind, but not a full one as humans have.[166]
Ultimately, there is some consensus within the field that a theory of mind is necessary for language use. Thus, the development of a full theory of mind in humans was a necessary precursor to full language use.[167]
In one particular study, rats and pigeons were required to press a button a certain number of times to get food. The animals showed very accurate distinction for numbers less than four, but as the numbers increased, the error rate increased.[159] In another, the primatologist Tetsuro Matsuzawa attempted to teach chimpanzees Arabic numerals.[168] The difference between primates and humans in this regard was very large, as it took the chimps thousands of trials to learn 1–9, with each number requiring a similar amount of training time; yet, after learning the meaning of 1, 2 and 3 (and sometimes 4), children (after the age of 5.5 to 6) easily comprehend the value of greater integers by using a successor function (i.e. 2 is 1 greater than 1, 3 is 1 greater than 2, 4 is 1 greater than 3; once 4 is reached it seems most children suddenly understand that the value of any integer n is 1 greater than the previous integer).[169] Put simply, other primates learn the meaning of numbers one by one, similar to their approach to other referential symbols, while children first learn an arbitrary list of symbols (1, 2, 3, 4...) and then later learn their precise meanings.[170] These results can be seen as evidence for the application of the "open-ended generative property" of language in human numeral cognition.[159]
Hockett (1966) details a list of features regarded as essential to describing human language.[171] In the domain of the lexical-phonological principle, two features of this list are most important:
The sound system of a language is composed of a finite set of simple phonological items. Under the specific phonotactic rules of a given language, these items can be recombined and concatenated, giving rise to morphology and the open-ended lexicon. A key feature of language is that a simple, finite set of phonological items gives rise to an infinite lexical system wherein rules determine the form of each item, and meaning is inextricably linked with form. Phonological syntax, then, is a simple combination of pre-existing phonological units. Related to this is another essential feature of human language: lexical syntax, wherein pre-existing units are combined, giving rise to semantically novel or distinct lexical items.[This paragraph needs citation(s)]
Certain elements of the lexical-phonological principle are known to exist outside of humans. While all (or nearly all) have been documented in some form in the natural world, very few coexist within the same species. Bird-song, singing nonhuman apes, and the songs of whales all display phonological syntax, combining units of sound into larger structures apparently devoid of enhanced or novel meaning. Certain other primate species do have simple phonological systems with units referring to entities in the world. However, in contrast to human systems, the units in these primates' systems normally occur in isolation, betraying a lack of lexical syntax. There is new[when?] evidence to suggest that Campbell's monkeys also display lexical syntax, combining two calls (a predator alarm call with a "boom", the combination of which denotes a lessened threat of danger), however it is still unclear whether this is a lexical or a morphological phenomenon.[172]
Pidgins are significantly simplified languages with only rudimentary grammar and a restricted vocabulary. In their early stage, pidgins mainly consist of nouns, verbs, and adjectives with few or no articles, prepositions, conjunctions or auxiliary verbs. Often the grammar has no fixed word order and the words have no inflection.[173]
If contact is maintained between the groups speaking the pidgin for long periods of time, the pidgins may become more complex over many generations. If the children of one generation adopt the pidgin as their native language it develops into a creole language, which becomes fixed and acquires a more complex grammar, with fixed phonology, syntax, morphology, and syntactic embedding. The syntax and morphology of such languages may often have local innovations not obviously derived from any of the parent languages.
Studies of creole languages around the world have suggested that they display remarkable similarities in grammar[citation needed] and are developed uniformly from pidgins in a single generation. These similarities are apparent even when creoles do not have any common language origins. In addition, creoles are similar, despite being developed in isolation from each other. Syntactic similarities include subject–verb–object word order. Even when creoles are derived from languages with a different word order they often develop the SVO word order. Creoles tend to have similar usage patterns for definite and indefinite articles, and similar movement rules for phrase structures even when the parent languages do not.[173]
Field primatologists can give useful insights into great ape communication in the wild.[29] One notable finding is that nonhuman primates, including the other great apes, produce calls that are graded, as opposed to categorically differentiated, with listeners striving to evaluate subtle gradations in signallers' emotional and bodily states. Nonhuman apes seemingly find it extremely difficult to produce vocalisations in the absence of the corresponding emotional states.[43] In captivity, nonhuman apes have been taught rudimentary forms of sign language or have been persuaded to use lexigrams—symbols that do not graphically resemble the corresponding words—on computer keyboards. Some nonhuman apes, such as Kanzi, have been able to learn and use hundreds of lexigrams.[174][175]
The Broca's and Wernicke's areas in the primate brain are responsible for controlling the muscles of the face, tongue, mouth, and larynx, as well as recognizing sounds. Primates are known to make "vocal calls", and these calls are generated by circuits in the brainstem and limbic system.[176]
In the wild, the communication of vervet monkeys has been the most extensively studied.[173] They are known to make up to ten different vocalizations. Many of these are used to warn other members of the group about approaching predators. They include a "leopard call", a "snake call", and an "eagle call".[177] Each call triggers a different defensive strategy in the monkeys who hear the call and scientists were able to elicit predictable responses from the monkeys using loudspeakers and prerecorded sounds. Other vocalisations may be used for identification. If an infant monkey calls, its mother turns toward it, but other vervet mothers turn instead toward that infant's mother to see what she will do.[178][179]
Similarly, researchers have demonstrated that chimpanzees (in captivity) use different "words" in reference to different foods. They recorded vocalisations that chimps made in reference, for example, to grapes, and then other chimps pointed at pictures of grapes when they heard the recorded sound.[180][181]
A study published in HOMO: Journal of Comparative Human Biology in 2017 claims that Ardipithecus ramidus, a hominin dated at approximately 4.5 Ma, shows the first evidence of an anatomical shift in the hominin lineage suggestive of increased vocal capability.[182] This study compared the skull of A. ramidus with 29 chimpanzee skulls of different ages and found that in numerous features A. ramidus clustered with the infant and juvenile measures as opposed to the adult measures. Such affinity with the shape dimensions of infant and juvenile chimpanzee skull architecture, it was argued, may have resulted in greater vocal capability. This assertion was based on the notion that the chimpanzee vocal tract ratios that prevent speech are a result of growth factors associated with puberty—growth factors absent in A. ramidus ontogeny. A. ramidus was also found to have a degree of cervical lordosis more conducive to vocal modulation when compared with chimpanzees as well as cranial base architecture suggestive of increased vocal capability.
What was significant in this study, according to the authors,[182] was the observation that the changes in skull architecture that correlate with reduced aggression are the same changes necessary for the evolution of early hominin vocal ability. In integrating data on anatomical correlates of primate mating and social systems with studies of skull and vocal tract architecture that facilitate speech production, the authors argue that paleoanthropologists prior to their study have failed to understand the important relationship between early hominin social evolution and the evolution of our species' capacities for language.
While the skull of A. ramidus, according to the authors, lacks the anatomical impediments to speech evident in chimpanzees, it is unclear what the vocal capabilities of this early hominin were. While they suggest A. ramidus—based on similar vocal tract ratios—may have had vocal capabilities equivalent to a modern human infant or very young child, they concede this is a debatable and speculative hypothesis. However, they do claim that changes in skull architecture through processes of social selection were a necessary prerequisite for language evolution. As they write:
We propose that as a result of paedomorphic morphogenesis of the cranial base and craniofacial morphology Ar. ramidus would have not been limited in terms of the mechanical components of speech production as chimpanzees and bonobos are. It is possible that Ar. ramidus had vocal capability approximating that of chimpanzees and bonobos, with its idiosyncratic skull morphology not resulting in any significant advances in speech capability. In this sense the anatomical features analysed in this essay would have been exapted in later more voluble species of hominin. However, given the selective advantages of pro-social vocal synchrony, we suggest the species would have developed significantly more complex vocal abilities than chimpanzees and bonobos.[182]
Anatomically, some scholars believe that features of bipedalism developed in the australopithecines around 3.5 million years ago. Around this time, these structural developments within the skull led to a more prominently L-shaped vocal tract.[183][page needed] In order to generate the sounds modern Homo sapiens are capable of making, such as vowels, it is vital that Early Homo populations must have a specifically shaped voice track and a lower sitting larynx.[184] Opposing research previously suggested that Neanderthals were physically incapable of creating the full range of vocals seen in modern humans due to the differences in larynx placement. Establishing distinct larynx positions through fossil remains of Homo sapiens and Neanderthals would support this theory; however, modern research has revealed that the hyoid bone was indistinguishable in the two populations. Though research has shown a lower sitting larynx is important to producing speech, another theory states it may not be as important as once thought.[185] Cataldo, Migliano, and Vinicius report speech alone appears inadequate for transmitting stone tool-making knowledge, and suggest that speech may have emerged due to an increase in complex social interactions.[186]
Steven Mithen proposed the term Hmmmmm for the pre-linguistic system of communication posited to have been used by archaic Homo, beginning with Homo ergaster and reaching the highest sophistication in the Middle Pleistocene with Homo heidelbergensis and Homo neanderthalensis. Hmmmmm is an acronym for holistic (non-compositional), manipulative (utterances are commands or suggestions, not descriptive statements), multi-modal (acoustic as well as gestural and facial), musical, and mimetic.[187]
Evidence for Homo erectus potentially using language comes in the form of Acheulean tool usage. The use of abstract thought in the formation of Acheulean hand axes coincides with the symbol creation necessary for simple language.[188] Recent language theories present recursion as the unique facet of human language and theory of mind.[189][190] However, breaking down language into its symbolic parts: separating meaning from the requirements of grammar, it becomes possible to see that language does not depend on either recursion or grammar. This can be evidenced by the Pirahã language users in Brazil that have no myth or creation stories, no numbers and no colors within their language.[191] This is to highlight that even though grammar may have been unavailable, use of foresight, planning and symbolic thought can be evidence of language as early as one million years ago with Homo erectus.
Homo heidelbergensis was a close relative (most probably a migratory descendant) of Homo ergaster. Some researchers believe this species to be the first hominin to make controlled vocalisations, possibly mimicking animal vocalisations,[187] and that as Homo heidelbergensis developed more sophisticated culture, proceeded from this point and possibly developed an early form of symbolic language.
The discovery in 1989 of the (Neanderthal) Kebara 2 hyoid bone suggests that Neanderthals may have been anatomically capable of producing sounds similar to modern humans.[192][193] The hypoglossal nerve, which passes through the hypoglossal canal, controls the movements of the tongue, which may have enabled voicing for size exaggeration (see size exaggeration hypothesis below) or may reflect speech abilities.[24][194][195][196][197][198]
However, although Neanderthals may have been anatomically able to speak, Richard G. Klein in 2004 doubted that they possessed a fully modern language. He largely bases his doubts on the fossil record of archaic humans and their stone tool kit. Bart de Boer in 2017 acknowledges this ambiguity of a universally accepted Neanderthal vocal tract; however, he notes the similarities in the thoracic vertebral canal, potential air sacs, and hyoid bones between modern humans and Neanderthals to suggest the presence of complex speech.[199] For two million years following the emergence of Homo habilis, the stone tool technology of hominins changed very little. Klein, who has worked extensively on ancient stone tools, describes the crude stone tool kit of archaic humans as impossible to break down into categories based on their function, and reports that Neanderthals seem to have had little concern for the final aesthetic form of their tools. Klein argues that the Neanderthal brain may have not reached the level of complexity required for modern speech, even if the physical apparatus for speech production was well-developed.[200][201] The issue of the Neanderthal's level of cultural and technological sophistication remains a controversial one.[citation needed]
Based on computer simulations used to evaluate that evolution of language that resulted in showing three stages in the evolution of syntax, Neanderthals are thought to have been in stage 2, showing they had something more evolved than proto-language but not quite as complex as the language of modern humans.[202]
Some researchers, applying auditory bioengineering models to computerised tomography scans of Neanderthal skulls, have asserted that Neanderthals had auditory capacity very similar to that of anatomically modern humans.[203] These researchers claim that this finding implies that "Neanderthals evolved the auditory capacities to support a vocal communication system as efficient as modern human speech."[203]
Anatomically modern humans begin to appear in the fossil record in Ethiopia some 200,000 years ago.[204] Although there is still much debate as to whether behavioural modernity emerged in Africa at around the same time, a growing number of archaeologists nowadays[when?] invoke the southern African Middle Stone Age use of red ochre pigments—for example at Blombos Cave—as evidence that modern anatomy and behaviour co-evolved.[205] These archaeologists argue strongly that if modern humans at this early stage were using red ochre pigments for ritual and symbolic purposes, they probably had symbolic language as well.[26]
According to the recent African origins hypothesis, from around 60,000 – 50,000 years ago[206] a group of humans left Africa and began migrating to occupy the rest of the world, carrying language and symbolic culture with them.[207]
The larynx (or voice box) is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is descended. The human species is not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls.[208] Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays.[209] Lions, jaguars, cheetahs and domestic cats also do this.[210] However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.[211]
Despite all this, scholars remain divided as to how "special" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent.[212] As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions. He argues that Neanderthals and early anatomically modern humans could not have possessed supralaryngeal vocal tracts capable of producing "fully human speech".[213] Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs—increasing the risk of choking while swallowing food—we are forced to ask what benefits might have outweighed those costs. The obvious benefit—so it is claimed—must have been speech. But this idea has been vigorously contested. One objection is that humans are in fact not seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare.[214] Another objection is that in the view of most scholars, speech as it is known emerged relatively late in human evolution, roughly contemporaneously with the emergence of Homo sapiens.[215] A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was initially driven by selection pressures for speech, thus not excluding that it was selected for e.g. improved singing ability.
To lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds "deeper"—giving an impression of greater size. John Ohala argues that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself.[216] Ohala points out that if the lowered larynx were an adaptation for speech, adult human males would be expected to be better adapted in this respect than adult females, whose larynx is considerably less low. However, females outperform males in verbal tests,[217] falsifying this whole line of reasoning.
W. Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in the human species. Although (according to Fitch) the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalizing to protect their infants would also have benefited from this ability.[218]
In 2011, Quentin Atkinson published a survey of phonemes from 500 different languages as well as language families and compared their phonemic diversity by region, number of speakers and distance from Africa. The survey revealed that African languages had the largest number of phonemes, and Oceania and South America had the smallest number. After allowing for the number of speakers, the phonemic diversity was compared to over 2000 possible origin locations. Atkinson's "best fit" model is that language originated in western, central, or southern Africa between 80,000 and 160,000 years ago. This predates the hypothesized southern coastal peopling of Arabia, India, southeast Asia, and Australia. It would also mean that the origin of language occurred at the same time as the emergence of symbolic culture.[219]
Numerous linguists[220][221][222] have criticized Atkinson's paper as misrepresenting both the phonemic data and processes of linguistic change, as language complexity does not necessarily correspond to age, and of failing to take into account the borrowing of phonemes from neighbouring languages, as some Bantu languages have done with click consonants.[222] Recreations of his method gave possible origins of language in the Caucasus[220] and Turkmenistan,[221] in addition to southern and eastern Africa.
The search for the origin of language has a long history in mythology. Most mythologies do not credit humans with the invention of language but speak of a divine language predating human language. Mystical languages used to communicate with animals or spirits, such as the language of the birds, are also common, and were of particular interest during the Renaissance.
Vāc is the Hindu goddess of speech, or "speech personified". As Brahman's "sacred utterance", she has a cosmological role as the "Mother of the Vedas". The Aztecs' story maintains that only a man, Coxcox, and a woman, Xochiquetzal, survived a flood, having floated on a piece of bark. They found themselves on land and had many children who were at first born unable to speak, but subsequently, upon the arrival of a dove, were endowed with language, although each one was given a different speech such that they could not understand one another.[223]
In the Old Testament, the Book of Genesis (chapter 11) says that God prevented the Tower of Babel from being completed through a miracle that made its construction workers start speaking different languages. After this, they migrated to other regions, grouped together according to which of the newly created languages they spoke, explaining the origins of languages and nations outside of the Fertile Crescent.[224]
History contains a number of anecdotes about people who attempted to discover the origin of language by experiment. The first such tale was told by Herodotus (Histories 2.2). He relates that Pharaoh Psammetichus (probably Psammetichus I, 7th century BC) had two children raised by a shepherd, with the instructions that no one should speak to them, but that the shepherd should feed and care for them while listening to determine their first words. When one of the children cried "bekos" with outstretched arms the shepherd concluded that the word was Phrygian, because that was the sound of the Phrygian word for 'bread'. From this, Psammetichus concluded that the first language was Phrygian. King James IV of Scotland is said to have tried a similar experiment; his children were supposed to have spoken Hebrew.[225]
Both the medieval monarch Frederick II and Akbar are said to have tried similar experiments; the children involved in these experiments did not speak. The current situation of deaf people also points into this direction.[clarification needed]
Modern linguistics did not begin until the late 18th century, and the Romantic or animist theses of Johann Gottfried Herder and Johann Christoph Adelung remained influential well into the 19th century. The question of language origin seemed inaccessible to methodical approaches, and in 1866 the Linguistic Society of Paris famously banned all discussion of the origin of language, deeming it to be an unanswerable problem. An increasingly systematic approach to historical linguistics developed in the course of the 19th century, reaching its culmination in the Neogrammarian school of Karl Brugmann and others.[citation needed]
However, scholarly interest in the question of the origin of language has only gradually been revived from the 1950s on (and then controversially) with ideas such as universal grammar, mass comparison and glottochronology.[citation needed]
The "origin of language" as a subject in its own right emerged from studies in neurolinguistics, psycholinguistics and human evolution. The Linguistic Bibliography introduced "Origin of language" as a separate heading in 1988, as a sub-topic of psycholinguistics. Dedicated research institutes of evolutionary linguistics are a recent phenomenon, emerging only in the 1990s.[226]

---

# Origin of speech

The origin of speech differs from the origin of language because language is not necessarily spoken; it could equally be written or signed. Speech is a fundamental aspect of human communication and plays a vital role in the everyday lives of humans. It allows them to convey thoughts, emotions, and ideas, and providing the ability to connect with others and shape collective reality.[1][2]
Many attempts have been made to explain scientifically how speech emerged in humans, although to date no theory has generated agreement.
Non-human primates, like many other animals, have evolved specialized mechanisms for producing sounds for purposes of social communication.[3] On the other hand, no monkey or ape uses its tongue for such purposes.[4][5] The human species' unprecedented use of the tongue, lips and other moveable parts seems to place speech in a quite separate category, making its evolutionary emergence an intriguing theoretical challenge in the eyes of many scholars.[6]
The term modality means the chosen representational format for encoding and transmitting information. A striking feature of language is that it is modality-independent. Should an impaired child be prevented from hearing or producing sound, its innate capacity to master a language may equally find expression in signing. Sign languages of the deaf are independently invented and have all the major properties of spoken language except for the modality of transmission.[7][8][9][10] From this it appears that the language centres of the human brain must have evolved to function optimally, irrespective of the selected modality.
"The detachment from modality-specific inputs may represent a substantial change in neural organization, one that affects not only imitation but also communication; only humans can lose one modality (e.g. hearing) and make up for this deficit by communicating with complete competence in a different modality (i.e. signing)."
Animal communication systems routinely combine visible with audible properties and effects, but none is modality-independent. For example, no vocally-impaired whale, dolphin, or songbird could express its song repertoire equally in visual display. Indeed, in the case of animal communication, message and modality are not capable of being disentangled. Whatever message is being conveyed stems from the intrinsic properties of the signal.
Modality independence should not be confused with the ordinary phenomenon of multimodality. Monkeys and apes rely on a repertoire of species-specific "gesture-calls" – emotionally-expressive vocalisations inseparable from the visual displays which accompany them.[12][13] Humans also have species-specific gesture-calls – laughs, cries, sobs, etc. – together with involuntary gestures accompanying speech.[14][15][16] Many animal displays are polymodal in that each appears designed to exploit multiple channels simultaneously.
The human linguistic property of modality independence is conceptually distinct from polymodality. It allows the speaker to encode the informational content of a message in a single channel whilst switching between channels as necessary. Modern city-dwellers switch effortlessly between the spoken word and writing in its various forms – handwriting, typing, email, etc. Whichever modality is chosen, it can reliably transmit the full message content without external assistance of any kind. When talking on the telephone, for example, any accompanying facial or manual gestures, however natural to the speaker, are not strictly necessary. When typing or manually signing, conversely, there is no need to add sounds. In many Australian Aboriginal cultures, a section of the population – perhaps women observing a ritual taboo – traditionally restrict themselves for extended periods to a silent (manually-signed) version of their language.[17] Then, when released from the taboo, these same individuals resume narrating stories by the fireside or in the dark, switching to pure sound without sacrifice of informational content.
Speaking is the default modality for language in all cultures. Humans' first recourse is to encode their thoughts in sound – a method which depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus.
The speech organs evolved in the first instance not for speech but for more basic bodily functions such as feeding and breathing. Nonhuman primates have broadly similar organs, but with different neural controls.[6] Non-human apes use their highly-flexible, maneuverable tongues for eating but not for vocalizing. When an ape is not eating, fine motor control over its tongue is deactivated.[4][5] Either it is performing gymnastics with its tongue or it is vocalising; it cannot perform both activities simultaneously. Since this applies to mammals in general, Homo sapiens are exceptional in harnessing mechanisms designed for respiration and ingestion for the radically different requirements of articulate speech.[18]
The word "language" derives from the Latin lingua, "tongue". Phoneticians agree that the tongue is the most important speech articulator, followed by the lips. A natural language can be viewed as a particular way of using the tongue to express thought.
The human tongue has an unusual shape. In most mammals, it is a long, flat structure contained largely within the mouth. It is attached at the rear to the hyoid bone, situated below the oral level in the pharynx. In humans, the tongue has an almost circular sagittal (midline) contour, much of it lying vertically down an extended pharynx, where it is attached to a hyoid bone in a lowered position. Partly as a result of this, the horizontal (inside-the-mouth) and vertical (down-the-throat) tubes forming the supralaryngeal vocal tract (SVT) are almost equal in length (whereas in other species, the vertical section is shorter). As humans move their jaws up and down, the tongue can vary the cross-sectional area of each tube independently by about 10:1, altering formant frequencies accordingly. That the tubes are joined at a right angle permits pronunciation of the vowels [i], [u] and [a], which nonhuman primates cannot do.[19] Even when not performed particularly accurately, in humans the articulatory gymnastics needed to distinguish these vowels yield consistent, distinctive acoustic results, illustrating the quantal[clarification needed] nature of human speech sounds.[20] It may not be coincidental that [i], [u] and [a] are the most common vowels in the world's languages.[21] Human tongues are a lot shorter and thinner than other mammals and are composed of a large number of muscles, which helps shape a variety of sounds within the oral cavity. The diversity of sound production is also increased with the human’s ability to open and close the airway, allowing varying amounts of air to exit through the nose. The fine motor movements associated with the tongue and the airway, make humans more capable of producing a wide range of intricate shapes in order to produce sounds at different rates and intensities.[22]
In humans, the lips are important for the production of stops and fricatives, in addition to vowels. Nothing, however, suggests that the lips evolved for those reasons. During primate evolution, a shift from nocturnal to diurnal activity in tarsiers, monkeys and apes (the haplorhines) brought with it an increased reliance on vision at the expense of olfaction. As a result, the snout became reduced and the rhinarium or "wet nose" was lost. The muscles of the face and lips consequently became less constrained, enabling their co-option to serve purposes of facial expression. The lips also became thicker, and the oral cavity hidden behind became smaller.[22] Hence, according to Ann MacLarnon, "the evolution of mobile, muscular lips, so important to human speech, was the exaptive result of the evolution of diurnality and visual communication in the common ancestor of haplorhines".[23] It is unclear whether human lips have undergone a more recent adaptation to the specific requirements of speech.
Compared with nonhuman primates, humans have significantly enhanced control of breathing, enabling exhalations to be extended and inhalations shortened as we speak. Whilst we are speaking, intercostal and interior abdominal muscles are recruited to expand the thorax and draw air into the lungs, and subsequently to control the release of air as the lungs deflate. The muscles concerned are markedly more innervated in humans than in nonhuman primates.[24] Evidence from fossil hominins suggests that the necessary enlargement of the vertebral canal, and therefore spinal cord dimensions, may not have occurred in Australopithecus or Homo erectus but was present in the Neanderthals and early modern humans.[25][26]
The larynx or voice box is an organ in the neck housing the vocal folds, which are responsible for phonation. In humans, the larynx is descended, it is positioned lower than in other primates. This is because the evolution of humans to an upright position shifted the head directly above the spinal cord, forcing everything else downward. The repositioning of the larynx resulted in a longer cavity called the pharynx, which is responsible for increasing the range and clarity of the sound being produced. Other primates have almost no pharynx; therefore, their vocal power is significantly lower.[22] Humans are not unique in this respect: goats, dogs, pigs and tamarins lower the larynx temporarily, to emit loud calls.[27] Several deer species have a permanently lowered larynx, which may be lowered still further by males during their roaring displays.[28] Lions, jaguars, cheetahs and domestic cats also do this.[29] However, laryngeal descent in nonhumans (according to Philip Lieberman) is not accompanied by descent of the hyoid; hence the tongue remains horizontal in the oral cavity, preventing it from acting as a pharyngeal articulator.[30]
Despite all this, scholars remain divided as to how "special" the human vocal tract really is. It has been shown that the larynx does descend to some extent during development in chimpanzees, followed by hyoidal descent.[31] As against this, Philip Lieberman points out that only humans have evolved permanent and substantial laryngeal descent in association with hyoidal descent, resulting in a curved tongue and two-tube vocal tract with 1:1 proportions.[citation needed] Uniquely in the human case, simple contact between the epiglottis and velum is no longer possible, disrupting the normal mammalian separation of the respiratory and digestive tracts during swallowing. Since this entails substantial costs – increasing the risk of choking whilst swallowing food – we are forced to ask what benefits might have outweighed those costs. Some claim the clear benefit must have been speech, but other contest this. One objection is that humans are in fact not seriously at risk of choking on food: medical statistics indicate that accidents of this kind are extremely rare.[32] Another objection is that in the view of most scholars, speech as we know it emerged relatively late in human evolution, roughly contemporaneously with the emergence of Homo sapiens.[33] A development as complex as the reconfiguration of the human vocal tract would have required much more time, implying an early date of origin. This discrepancy in timescales undermines the idea that human vocal flexibility was initially driven by selection pressures for speech.
At least one orangutan has demonstrated the ability to control the voice box.[34]
To lower the larynx is to increase the length of the vocal tract, in turn lowering formant frequencies so that the voice sounds "deeper" – giving an impression of greater size. John Ohala argued that the function of the lowered larynx in humans, especially males, is probably to enhance threat displays rather than speech itself.[35] Ohala pointed out that if the lowered larynx were an adaptation for speech, we would expect adult human males to be better adapted in this respect than adult females, whose larynx is considerably less low. In fact, females invariably outperform males in verbal tests, falsifying this whole line of reasoning.[citation needed] William Tecumseh Fitch likewise argues that this was the original selective advantage of laryngeal lowering in humans. Although, according to Fitch, the initial lowering of the larynx in humans had nothing to do with speech, the increased range of possible formant patterns was subsequently co-opted for speech. Size exaggeration remains the sole function of the extreme laryngeal descent observed in male deer. Consistent with the size exaggeration hypothesis, a second descent of the larynx occurs at puberty in humans, although only in males. In response to the objection that the larynx is descended in human females, Fitch suggests that mothers vocalising to protect their infants would also have benefited from this ability.[36]
Most specialists credit the Neanderthals with speech abilities not radically different from those of modern Homo sapiens. An indirect line of argument is that their toolmaking and hunting tactics would have been difficult to learn or execute without some kind of speech.[37] A recent extraction of DNA from Neanderthal bones indicates that Neanderthals had the same version of the FOXP2 gene as modern humans. This gene, mistakenly described as the "grammar gene", plays a role in controlling the orofacial movements which (in modern humans) are involved in speech.[38]
During the 1970s, it was widely believed that the Neanderthals lacked modern speech capacities.[39] It was claimed that they possessed a hyoid bone so high up in the vocal tract as to preclude the possibility of producing certain vowel sounds.
The hyoid bone is present in many mammals. It allows a wide range of tongue, pharyngeal and laryngeal movements by bracing these structures alongside each other in order to produce variation.[40] It is now realised that its lowered position is not unique to Homo sapiens, whilst its relevance to vocal flexibility may have been overstated: although men have a lower larynx, they do not produce a wider range of sounds than women or two-year-old babies. There is no evidence that the larynx position of the Neanderthals impeded the range of vowel sounds they could produce.[41] The discovery of a modern-looking hyoid bone of a Neanderthal man in the Kebara Cave in Israel led its discoverers to argue that the Neanderthals had a descended larynx, and thus human-like speech capabilities.[42][43] However, other researchers have claimed that the morphology of the hyoid is not indicative of the larynx's position.[6] It is necessary to take into consideration the skull base, the mandible, the cervical vertebrae and a cranial reference plane.[44][45]
The morphology of the outer and middle ear of Middle Pleistocene hominins from Atapuerca, Spain, believed to be proto-Neanderthal, suggests they had an auditory sensitivity similar to modern humans and very different from chimpanzees. They were probably able to differentiate between many different speech sounds.[46]
The hypoglossal nerve plays an important role in controlling movements of the tongue. In 1998, a research team used the size of the hypoglossal canal in the base of fossil skulls in an attempt to estimate the relative number of nerve fibres, claiming on this basis that Middle Pleistocene hominins and Neanderthals had more fine-tuned tongue control than either Australopithecines or apes.[47] Subsequently, however, it was demonstrated that hypoglossal canal size and nerve sizes are not correlated,[48] and it is now accepted that such evidence is uninformative about the timing of human speech evolution.[49]
According to one influential school,[50][51] the human vocal apparatus is intrinsically digital on the model of a keyboard or digital computer[clarification needed] (see below). Nothing about a chimpanzee's vocal apparatus suggests a digital keyboard[clarification needed], notwithstanding the anatomical and physiological similarities. This poses the question as to when and how, during the course of human evolution, the transition from analog to digital structure and function occurred.
The human supralaryngeal tract is said to be digital in the sense that it is an arrangement of moveable toggles or switches, each of which, at any one time, must be in one state or another. The vocal cords, for example, are either vibrating (producing a sound) or not vibrating (in silent mode). By virtue of simple physics, the corresponding distinctive feature – in this case, "voicing" – cannot be somewhere in between. The options are limited to "off" and "on". Equally digital is the feature known as "nasalisation". At any given moment the soft palate or velum either allows or does not allow sound to resonate in the nasal chamber. In the case of lip and tongue positions, more than two digital states may be allowed.
The theory that speech sounds are composite entities constituted by complexes of binary phonetic features was first advanced in 1938 by the Russian linguist Roman Jakobson.[52] A prominent early supporter of this approach was Noam Chomsky, who went on to extend it from phonology to language more generally, in particular to the study of syntax and semantics.[53][54][55] In his 1965 book, Aspects of the Theory of Syntax,[56] Chomsky treated semantic concepts as combinations of binary-digital atomic elements explicitly on the model of distinctive features theory. The lexical item "bachelor", on this basis, would be expressed as [+ Human], [+ Male], [- Married].
Supporters of this approach view the vowels and consonants recognised by speakers of a particular language or dialect at a particular time as cultural entities of little scientific interest. From a natural science standpoint, the units which matter are those common to Homo sapiens by virtue of biological nature. By combining the atomic elements or "features" with which all humans are innately equipped, anyone may in principle generate the entire range of vowels and consonants to be found in any of the world's languages, whether past, present or future. The distinctive features are in this sense atomic components of a universal language.
In recent years, the notion of an innate "universal grammar" underlying phonological variation has been called into question. The most comprehensive monograph ever written about speech sounds, The Sounds of the World's Languages, by Peter Ladefoged and Ian Maddieson,[21] found virtually no basis for the postulation of some small number of fixed, discrete, universal phonetic features. Examining 305 languages, for example, they encountered vowels that were positioned basically everywhere along the articulatory and acoustic continuum. Ladefoged concluded that phonological features are not determined by human nature: "Phonological features are best regarded as artifacts that linguists have devised in order to describe linguistic systems".[57]
Self-organisation characterises systems where macroscopic structures are spontaneously formed out of local interactions between the many components of the system.[58] In self-organised systems, global organisational properties are not to be found at the local level. In colloquial terms, self-organisation is roughly captured by the idea of "bottom-up" (as opposed to "top-down") organisation. Examples of self-organised systems range from ice crystals to galaxy spirals in the inorganic world.
According to many phoneticians, the sounds of language arrange and re-arrange themselves through self-organisation.[58][59][60] Speech sounds have both perceptual (how one hears them) and articulatory (how one produces them) properties, all with continuous values. Speakers tend to minimise effort, favouring ease of articulation over clarity. Listeners do the opposite, favouring sounds that are easy to distinguish even if difficult to pronounce. Since speakers and listeners are constantly switching roles, the syllable systems actually found in the world's languages turn out to be a compromise between acoustic distinctiveness on the one hand, and articulatory ease on the other.
Agent-based computer models take the perspective of self-organisation at the level of the speech community or population. The two main paradigms are (1) the iterated learning model and (2) the language game model. Iterated learning focuses on transmission from generation to generation, typically with just one agent in each generation.[61] In the language game model, a whole population of agents simultaneously produce, perceive and learn language, inventing novel forms when the need arises.[62][63]
Several models have shown how relatively simple peer-to-peer vocal interactions, such as imitation, can spontaneously self-organise a system of sounds shared by the whole population, and different in different populations. For example, models elaborated by Berrah et al. (1996)[64] and de Boer (2000),[65] and recently reformulated using Bayesian theory,[66] showed how a group of individuals playing imitation games can self-organise repertoires of vowel sounds which share substantial properties with human vowel systems. For example, in de Boer's model, initially vowels are generated randomly, but agents learn from each other as they interact repeatedly over time. Agent A chooses a vowel from her repertoire and produces it, inevitably with some noise. Agent B hears this vowel and chooses the closest equivalent from her own repertoire. To check whether this truly matches the original, B produces the vowel she thinks she has heard, whereupon A refers once again to her own repertoire to find the closest equivalent. If this matches the one she initially selected, the game is successful, otherwise, it has failed. "Through repeated interactions", according to de Boer, "vowel systems emerge that are very much like the ones found in human languages".[67]
In a different model, the phonetician Björn Lindblom[68] was able to predict, on self-organisational grounds, the favoured choices of vowel systems ranging from three to nine vowels on the basis of a principle of optimal perceptual differentiation.
Further models studied the role of self-organisation in the origins of phonemic coding and combinatoriality, which is the existence of phonemes and their systematic reuse to build structured syllables. Pierre-Yves Oudeyer developed models which showed that basic neural equipment for adaptive holistic vocal imitation, coupling directly motor and perceptual representations in the brain, can generate spontaneously shared combinatorial systems of vocalisations, including phonotactic patterns, in a society of babbling individuals.[58][69] These models also characterised how morphological and physiological innate constraints can interact with these self-organised mechanisms to account for both the formation of statistical regularities and diversity in vocalisation systems.
The gestural theory states that speech was a relatively late development, evolving by degrees from a system that was originally gestural. Human ancestors were unable to control their vocalisation at the time when gestures were used to communicate; however, as they slowly began to control their vocalisations, spoken language began to evolve.
Research has found strong support for the idea that spoken language and signing depend on similar neural structures. Patients who used sign language, and who suffered from a left-hemisphere lesion, showed the same disorders with their sign language as vocal patients did with their oral language.[71] Other researchers found that the same left-hemisphere brain regions were active during sign language as during the use of vocal or written language.[72]
Humans spontaneously use hand and facial gestures when formulating ideas to be conveyed in speech.[73][74] There are also, of course, many sign languages in existence, commonly associated with deaf communities; as noted above, these are equal in complexity, sophistication, and expressive power, to any oral language. The main difference is that the "phonemes" are produced on the outside of the body, articulated with hands, body, and facial expression, rather than inside the body articulated with tongue, teeth, lips, and breathing.
Many psychologists and scientists have looked into the mirror system in the brain to answer this theory as well as other behavioural theories. Evidence to support mirror neurons as a factor in the evolution of speech includes mirror neurons in primates, the success of teaching apes to communicate gesturally, and pointing/gesturing to teach young children language. Fogassi and Ferrari (2014)[citation needed] monitored motor cortex activity in monkeys, specifically area F5 in the Broca’s area, where mirror neurons are located. They observed changes in electrical activity in this area when the monkey executed or observed different hand actions performed by someone else. Broca’s area is a region in the frontal lobe responsible for language production and processing. The discovery of mirror neurons in this region, which fire when an action is done or observed specifically with the hand, strongly supports the belief that communication was once accomplished with gestures. The same is true when teaching young children language. When one points at a specific object or location, mirror neurons in the child fire as though they were doing the action, which results in long-term learning[75]
Critics note that for mammals in general, sound turns out to be the best medium in which to encode information for transmission over distances at speed. Given the probability that this applied also to early humans, it is hard to see why they should have abandoned this efficient method in favour of more costly and cumbersome systems of visual gesturing – only to return to sound at a later stage.[76]
By way of explanation, it has been proposed that at a relatively late stage in human evolution, hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language is said to have occurred only at that point.[77] Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this issue – one setting out from considerations of signal reliability and trust – see "from pantomime to speech" below).
Recent insights in human evolution – more specifically, human Pleistocene littoral evolution[78] – may help understand how human speech evolved. One controversial suggestion is that certain pre-adaptations for spoken language evolved during a time when ancestral hominins lived close to river banks and lake shores rich in fatty acids and other brain-specific nutrients. Occasional wading or swimming may also have led to enhanced breath-control (breath-hold diving).
Independent lines of evidence suggest that "archaic" Homo spread intercontinentally along the Indian Ocean shores (they even reached overseas islands such as Flores) where they regularly dived for littoral foods such as shell- and crayfish,[79] which are extremely rich in brain-specific nutrients, explaining Homo's brain enlargement.[80] Shallow diving for seafoods requires voluntary airway control, a prerequisite for spoken language. Seafood such as shellfish generally does not require biting and chewing, but stone tool use and suction feeding. This finer control of the oral apparatus was arguably another biological pre-adaptation to human speech, especially for the production of consonants.[81]
Little is known about the timing of language's emergence in the human species. Unlike writing, speech leaves no material trace, making it archaeologically invisible. Lacking direct linguistic evidence, specialists in human origins have resorted to the study of anatomical features and genes arguably associated with speech production. Whilst such studies may provide information as to whether pre-modern Homo species had speech capacities, it is still unknown whether they actually spoke. Whilst they may have communicated vocally, the anatomical and genetic data lack the resolution necessary to differentiate proto-language from speech.
Using statistical methods to estimate the time required to achieve the current spread and diversity in modern languages today, Johanna Nichols – a linguist at the University of California, Berkeley – argued in 1998 that vocal languages must have begun diversifying at least 100,000 years ago.[82]
In 2012, anthropologists Charles Perreault and Sarah Mathew used phonemic diversity to suggest a date consistent with this.[83] "Phonemic diversity" denotes the number of perceptually distinct units of sound – consonants, vowels and tones – in a language. The current worldwide pattern of phonemic diversity potentially contains the statistical signal of the expansion of modern Homo sapiens out of Africa, beginning around 60-70 thousand years ago. Some scholars argue that phonemic diversity evolves slowly and can be used as a clock to calculate how long the oldest African languages would have to have been around in order to accumulate the number of phonemes they possess today. As human populations left Africa and expanded into the rest of the world, they underwent a series of bottlenecks – points at which only a very small population survived to colonise a new continent or region. Allegedly such a population crash led to a corresponding reduction in genetic, phenotypic and phonemic diversity. African languages today have some of the largest phonemic inventories in the world, whilst the smallest inventories are found in South America and Oceania, some of the last regions of the globe to be colonised. For example, Rotokas, a language of New Guinea, and Pirahã, spoken in South America, both have just 11 phonemes,[84][85] whilst !Xun, a language spoken in Southern Africa has 141 phonemes. The authors use a natural experiment – the colonization of mainland Southeast Asia on the one hand, the long-isolated Andaman Islands on the other – to estimate the rate at which phonemic diversity increases through time. Using this rate, they estimate that the world's languages date back to the Middle Stone Age in Africa, sometime between 350 thousand and 150 thousand years ago. This corresponds to the speciation event which gave rise to Homo sapiens.
These and similar studies have however been criticised by linguists who argue that they are based on a flawed analogy between genes and phonemes, since phonemes are frequently transferred laterally between languages unlike genes, and on a flawed sampling of the world's languages, since both Oceania and the Americas also contain languages with very high numbers of phonemes, and Africa contains languages with very few. They argue that the actual distribution of phonemic diversity in the world reflects recent language contact and not deep language history - since it is well demonstrated that languages can lose or gain many phonemes over very short periods. In other words, there is no valid linguistic reason to expect genetic founder effects to influence phonemic diversity.[86][87]

---

# Evolutionary medicine

Evolutionary medicine or Darwinian medicine is the application of modern evolutionary theory to understanding health and disease. Modern biomedical research and practice have focused on the molecular and physiological mechanisms underlying health and disease, while evolutionary medicine focuses on the question of why evolution has shaped these mechanisms in ways that may leave us susceptible to disease. The evolutionary approach has driven important advances in the understanding of cancer,[1] autoimmune disease,[2] and anatomy.[3] Medical schools have been slower to integrate evolutionary approaches because of limitations on what can be added to existing medical curricula.[4] The International Society for Evolution, Medicine and Public Health coordinates efforts to develop the field. It owns the Oxford University Press journal Evolution, Medicine and Public Health and The Evolution and Medicine Review.
Utilizing the Delphi method, 56 experts from a variety of disciplines, including anthropology, medicine, nursing, and biology agreed upon 14 core principles intrinsic to the education and practice of evolutionary medicine.[5] These 14 principles can be further grouped into five general categories: question framing, evolution I and II (with II involving a higher level of complexity), evolutionary trade-offs, reasons for vulnerability, and culture. Additional information regarding these principles may be found in the table below.
Adaptation works within constraints, makes compromises and trade-offs, and occurs in the context of different forms of competition.[6]
Adaptations can only occur if they are evolvable. Some adaptations which would prevent ill health are therefore not possible.
Other constraints occur as the byproduct of adaptive innovations.
One constraint upon selection is that different adaptations can conflict, which requires a compromise between them to ensure an optimal cost-benefit tradeoff.
Different forms of competition exist and these can shape the processes of genetic change.
Humans evolved to live as simple hunter-gatherers in small tribal bands, while contemporary humans have a more complex life.[13][14] This change may make present-day humans susceptible to lifestyle diseases.
In contrast to the diet of early hunter-gatherers, the modern Western diet often contains high quantities of fat, salt, and simple carbohydrates, such as refined sugars and flours.[15][16][17]
Among different countries, the incidence of colon cancer varies widely, and the extent of exposure to a Western pattern diet may be a factor in cancer incidence.[18]
Examples of aging-associated diseases are atherosclerosis and cardiovascular disease, cancer, arthritis, cataracts, osteoporosis, type 2 diabetes, hypertension and Alzheimer's disease. The incidence of all of these diseases increases rapidly with aging (increases exponentially with age, in the case of cancer).
Of the roughly 150,000 people who die each day across the globe, about two thirds—100,000 per day—die of age-related causes.[19] In industrialized nations, the proportion is much higher, reaching 90%.[19]
Many contemporary humans engage in little physical exercise compared to the physically active lifestyles of ancestral hunter-gatherers.[20][21][22][23][24] Prolonged periods of inactivity may have only occurred in early humans following illness or injury, so a modern sedentary lifestyle may continuously cue the body to trigger life preserving metabolic and stress-related responses such as inflammation, and some theorize that this causes chronic diseases.[25]
Contemporary humans in developed countries are mostly free of parasites, particularly intestinal ones. This is largely due to frequent washing of clothing and the body, and improved sanitation. Although such hygiene can be very important when it comes to maintaining good health, it can be problematic for the proper development of the immune system. The hygiene hypothesis is that humans evolved to be dependent on certain microorganisms that help establish the immune system, and modern hygiene practices can prevent necessary exposure to these microorganisms. "Microorganisms and macroorganisms such as helminths from mud, animals, and feces play a critical role in driving immunoregulation" (Rook, 2012[26]). Essential microorganisms play a crucial role in building and training immune functions that fight off and repel some diseases, and protect against excessive inflammation, which has been implicated in several diseases. For instance, recent studies have found evidence supporting inflammation as a contributing factor in Alzheimer's Disease.[27]
This is a partial list: all links here go to a section describing or debating its evolutionary origin.
As noted in the table below, adaptationist hypotheses regarding the etiology of psychological disorders are often based on analogies with evolutionary perspectives on medicine and physiological dysfunctions (see in particular, Randy Nesse and George C. Williams' book Why We Get Sick).[43] Evolutionary psychiatrists and psychologists suggest that some mental disorders likely have multiple causes.[63]
Summary based on information in Buss (2011),[64] Gaulin & McBurney (2004),[65] Workman & Reader (2004)[66]
See several topic areas, and the associated references, below.
Charles Darwin did not discuss the implications of his work for medicine, though biologists quickly appreciated the germ theory of disease and its implications for understanding the evolution of pathogens, as well as an organism's need to defend against them.
Medicine, in turn, ignored evolution, and instead focused (as done in the hard sciences) upon proximate mechanical causes.
medicine has modelled itself after a mechanical physics, deriving from Galileo, Newton, and Descartes.... As a result of assuming this model, medicine is mechanistic, materialistic, reductionistic, linear-causal, and deterministic (capable of precise predictions) in its concepts. It seeks explanations for diseases, or their symptoms, signs, and cause in single, materialistic— i.e., anatomical or structural (e.g., in genes and their products)— changes within the body, wrought directly (linearly), for example, by infectious, toxic, or traumatic agents.[74] p. 510
George C. Williams was the first to apply evolutionary theory to health in the context of senescence.[32] Also in the 1950s, John Bowlby approached the problem of disturbed child development from an evolutionary perspective upon attachment.
An important theoretical development was Nikolaas Tinbergen's distinction made originally in ethology between evolutionary and proximate mechanisms.[75]
Randolph M. Nesse summarizes its relevance to medicine:
all biological traits need two kinds of explanation, both proximate and evolutionary. The proximate explanation for a disease describes what is wrong in the bodily mechanism of individuals affected by it. An evolutionary explanation is completely different. Instead of explaining why people are different, it explains why we are all the same in ways that leave us vulnerable to disease. Why do we all have wisdom teeth, an appendix, and cells that can divide out of control?[76]
The paper of Paul Ewald in 1980, "Evolutionary Biology and the Treatment of Signs and Symptoms of Infectious Disease",[77] and that of Williams and Nesse in 1991, "The Dawn of Darwinian Medicine"[78] were key developments. The latter paper "draw a favorable reception",[43]page x and led to a book, Why We Get Sick (published as Evolution and healing in the UK). In 2008, an online journal started: Evolution and Medicine Review.
In 2000, Paul Sherman hypothesised that morning sickness could be an adaptation that protects the developing fetus from foodborne illnesses, some of which can cause miscarriage or birth defects, such as listeriosis and toxoplasmosis.[79]

---

# Evolution of morality

The concept of the evolution of morality refers to the emergence of human moral behavior over the course of human evolution. Morality can be defined as a system of ideas about right and wrong conduct. In everyday life, morality is typically associated with human behavior rather than animal behavior. The emerging fields of evolutionary biology, and in particular evolutionary psychology, have argued that, despite the complexity of human social behaviors, the precursors of human morality can be traced to the behaviors of many other social animals. Sociobiological explanations of human behavior remain controversial. Social scientists have traditionally viewed morality as a construct, and thus as culturally relative, although others such as Sam Harris argue that there is an objective science of morality.
Though other animals may not possess what humans may perceive as moral behavior, all social animals have had to modify or restrain their behaviors for group living to be worthwhile. Typical examples of behavioral modification can be found in the societies of ants, bees and termites. Ant colonies may possess millions of individuals. E. O. Wilson argues that the single most important factor that leads to the success of ant colonies is the existence of a sterile worker caste. This caste of females are subservient to the needs of their mother, the queen, and in so doing, have given up their own reproduction in order to raise brothers and sisters. The existence of sterile castes among these social insects significantly restricts the competition for mating and in the process fosters cooperation within a colony. Cooperation among ants is vital, because a solitary ant has an improbable chance of long-term survival and reproduction. However, as part of a group, colonies can thrive for decades. As a consequence, ants are one of the most successful families of species on the planet, accounting for a biomass that rivals that of the human species.[1][2]
The basic reason that social animals live in groups is that opportunities for survival and reproduction are much better in groups than living alone. The social behaviors of mammals are more familiar to humans. Highly social mammals such as primates and elephants have been known to exhibit traits that were once thought to be uniquely human, like empathy and altruism.[3][4]
Humanity's closest living relatives are common chimpanzees and bonobos. These primates share a common ancestor with humans who lived four to six million years ago. It is for this reason that chimpanzees and bonobos are viewed as the best available surrogate for this common ancestor. Barbara King argues that while primates may not possess morality in the human sense, they do exhibit some traits that would have been necessary for the evolution of morality. These traits include high intelligence, a capacity for symbolic communication, a sense of social norms, realization of "self", and a concept of continuity.[5][6] Frans de Waal and Barbara King both view human morality as having grown out of primate sociality. Many social animals such as primates, dolphins, and whales have shown to exhibit what Michael Shermer refers to as premoral sentiments. According to Shermer, the following characteristics are shared by humans and other social animals, particularly the great apes:
Shermer argues that these premoral sentiments evolved in primate societies as a method of restraining individual selfishness and building more cooperative groups. For any social species, the benefits of being part of an altruistic group should outweigh the benefits of individualism. For example, lack of group cohesion could make individuals more vulnerable to attack from outsiders. Being part of a group may also improve the chances of finding food. This is evident among animals that hunt in packs to take down large or dangerous prey.
All social animals have societies in which each member knows its own place.[citation needed] Social order is maintained by certain rules of expected behavior and dominant group members enforce order through punishment. However, higher order primates also have a sense of reciprocity. Chimpanzees remember who did them favors and who did them wrong.[citation needed] For example, chimpanzees are more likely to share food with individuals who have previously groomed them.[9] Vampire bats also demonstrate a sense of reciprocity and altruism. They share blood by regurgitation, but do not share randomly. They are most likely to share with other bats who have shared with them in the past or who are in dire need of feeding.[10]
Animals such as Capuchin monkeys[11] and dogs[12] also display an understanding of fairness, refusing to co-operate when presented unequal rewards for the same behaviors.
Chimpanzees live in fission-fusion groups that average 50 individuals. It is likely that early ancestors of humans lived in groups of similar size. Based on the size of extant hunter gatherer societies, recent paleolithic hominids lived in bands of a few hundred individuals. As community size increased over the course of human evolution, greater enforcement to achieve group cohesion would have been required. Morality may have evolved in these bands of 100 to 200 people as a means of social control, conflict resolution and group solidarity. This numerical limit is theorized to be hard coded in our genes since even modern humans have difficulty maintaining stable social relationships with more than 100–200 people. According to Dr. de Waal, human morality has two extra levels of sophistication that are not found in other primate societies. Humans enforce their society's moral codes much more rigorously with rewards, punishments and reputation building. People also apply a degree of judgment and reason not seen in the animal kingdom.[citation needed]
Some evolutionary biologists and game theorists argue that since gradual evolutionary models of morality require incremental evolution of altruism in populations where egoism and cruelty initially reigned, any sense of occasional altruism otherwise egoistic and cruel individuals being worse than consistent cruelty would have made evolution of morality impossible due to early stages of moral evolution being selected against by such sentiments causing the individuals with some morality to be treated worse than those with no morality. This would have caused low degree morality to become an adaptive valley that would preclude the early steps away from the no morality condition, precluding an early necessary condition for later evolution of higher degrees of morality. These scientists argue that while this rules out evolutionary explanations of the specific type of morality that feels disgust at some empathy from rarely empathic individuals by assuming it to be psychopathic manipulation, it does not rule out evolution of other types of morality that accept a little altruism as better than no altruism at all.[13][14]
While groups may benefit from avoiding certain behaviors, those harmful behaviors have the same effect regardless of whether the offending individuals are aware of them or not.[15] Since the individuals themselves can increase their reproductive success by doing many of them, any characteristics that entail impunity are positively selected by evolution.[16] Specifically punishing individuals aware of their breach of rules would select against the ability to be aware of it, precluding any coevolution of both conscious choice and a sense of it being the basis for moral and penal liability in the same species.[17]
The social brain hypothesis, detailed by R.I.M Dunbar in the article The Social Brain Hypothesis and Its Implications for Social Evolution, supports the fact that the brain originally evolved to process factual information. The brain allows an individual to recognize patterns, perceive speech, develop strategies to circumvent ecologically-based problems such as foraging for food, and also permits the phenomenon of color vision. It is said that in humans and primates the neocortex is responsible for reasoning and consciousness.
Furthermore, having a large brain is a reflection of the large cognitive demands of complex social systems. Therefore, in social animals, the neocortex came under intense selection to increase in size to improve social cognitive abilities. Social animals, such as humans, are capable of two important concepts, coalition formation, or group living, and tactical deception, which is a tactic of presenting false information to others. The fundamental importance of animal social skills lies within the ability to manage relationships and in turn, the ability to not just commit information to memory, but manipulate it as well.[18]
An adaptive response to the challenges of social interaction and living is theory of mind. Theory of mind as defined by Martin Brüne, is the ability to infer another individual's mental states or emotions.[19] Having a strong theory of mind is tied closely with possessing advanced social intelligence. Collectively, group living requires cooperation and generates conflict. Social living puts strong evolutionary selection pressures on acquiring social intelligence because living in groups has advantages. Such advantages include protection from predators and the fact that groups in general outperform the sum of an individual's performance. But, from an objective point of view, group living also has disadvantages, such as competition within the group for resources and mates. This sets the stage for something of an evolutionary arms race within the species.
Within populations of social animals, altruism, or acts of behavior that are disadvantageous to one individual while benefiting other group members, has evolved. This notion seems to be contradictory to evolutionary thought, because an organism's fitness and success is defined by its ability to pass genes on to the next generation. According to E. Fehr, in the article, The Nature of Human Altruism, the evolution of altruism can be accounted for when kin selection and inclusive fitness are taken into account; meaning reproductive success is not just dependent on the number of offspring an individual produces, but also the number of offspring that related individuals produce.[20] Outside of familial relationships altruism is also seen, but in a different manner typically defined by the prisoner's dilemma, theorized by John Nash. The prisoner's dilemma serves to define cooperation and defecting with and against individuals driven by incentive, or in Nash's proposed case, years in jail. In evolutionary terms, the best strategy to use for the prisoner's dilemma is tit-for-tat, where an individual should cooperate as long others are cooperating, and not defect until another individual defects against them. At their core, complex social interactions are driven by the need to distinguish sincere cooperation and defection.
Brune details that theory of mind has been traced back to primates, but it is not observed to the extent that it is in the modern human. The emergence of this unique trait is perhaps where the divergence of the modern human begins, along with our acquisition of language. Humans use metaphors and imply much of what we say. Phrases such as, "You know what I mean?" are not uncommon and are direct results of the sophistication of the human theory of mind. Failure to understand another's intentions and emotions can yield inappropriate social responses and are often associated with human mental conditions such as autism, schizophrenia, bipolar disorder, some forms of dementia, and psychopathy. This is especially true for autism spectrum disorders, where social disconnect is evident, but non-social intelligence can be preserved or even in some cases augmented, such as in the case of a savant.[19] The need for social intelligence surrounding theory of mind is a possible answer to the question as to why morality has evolved as a part of human behavior.
Psychologist Matt J. Rossano muses that religion emerged after morality and built upon morality by expanding the social scrutiny of individual behavior to include supernatural third-party agents. By including ever watchful ancestors, spirits and gods in the social realm, humans discovered an effective strategy for restraining selfishness and building more cooperative groups.[21] The adaptive value of religion would have enhanced group survival.[22][23]
In an experiment where subjects must demonstrate abstract, complex reasoning, researchers have found that humans (as has been seen in other animals) have a strong innate ability to reason about social exchanges. This ability is believed to be intuitive, since the logical rules do not seem to be accessible to the individuals for use in situations without moral overtones.[24]
Disgust, one of the basic emotions, may have an important role in certain forms of morality. Disgust is argued to be a specific response to certain things or behaviors that are dangerous or undesirable from an evolutionary perspective. One example is things that increase the risk of an infectious disease such as spoiled foods, dead bodies, other forms of microbiological decomposition, a physical appearance suggesting sickness or poor hygiene, and various body fluids such as feces, vomit, phlegm, and blood. Another example is disgust against evolutionary disadvantageous mating such as incest (the incest taboo) or unwanted sexual advances.[4] Still another example are behaviors that may threaten group cohesion or cooperation such as cheating, lying, and stealing. MRI studies have found that such situations activate areas in the brain associated with disgust.[25]

---

# Evolutionary neuroscience

Evolutionary neuroscience is the scientific study of the evolution of nervous systems. Evolutionary neuroscientists investigate the evolution and natural history of nervous system structure, functions and emergent properties. The field draws on concepts and findings from both neuroscience and evolutionary biology. Historically, most empirical work has been in the area of comparative neuroanatomy, and modern studies often make use of phylogenetic comparative methods. Selective breeding and experimental evolution approaches are also being used more frequently.[1]
Conceptually and theoretically, the field is related to fields as diverse as cognitive genomics, neurogenetics, developmental neuroscience, neuroethology, comparative psychology, evo-devo, behavioral neuroscience, cognitive neuroscience, behavioral ecology, biological anthropology and sociobiology.
Evolutionary neuroscientists examine changes in genes, anatomy, physiology, and behavior to study the evolution of changes in the brain.[2] They study a multitude of processes including the evolution of vocal, visual, auditory, taste, and learning systems as well as language evolution and development.[2][3] In addition, evolutionary neuroscientists study the evolution of specific areas or structures in the brain such as the amygdala, forebrain and cerebellum as well as the motor or visual cortex.[2]
Studies of the brain began during ancient Egyptian times but studies in the field of evolutionary neuroscience began after the publication of Darwin's On the Origin of Species in 1859. At that time, brain evolution was largely viewed at the time in relation to the incorrect scala naturae. Phylogeny and the evolution of the brain were still viewed as linear. During the early 20th century, there were several prevailing theories about evolution. Darwinism was based on the principles of natural selection and variation, Lamarckism was based on the passing down of acquired traits, Orthogenesis was based on the assumption that tendency towards perfection steers evolution, and Saltationism argued that discontinuous variation creates new species. Darwin's became the most accepted and allowed for people to starting thinking about the way animals and their brains evolve.[4]
The 1936 book The Comparative Anatomy of the Nervous System of Vertebrates Including Man by the Dutch neurologist C.U. Ariëns Kappers (first published in German in 1921) was a landmark publication in the field. Following the Evolutionary Synthesis, the study of comparative neuroanatomy was conducted with an evolutionary view, and modern studies incorporate developmental genetics.[4][5] It is now accepted that phylogenetic changes occur independently between species over time and can not be linear. It is also believed that an increase with brain size correlates with an increase in neural centers and behavior complexity.[4]
Over time, there are several arguments that would come to define the history of evolutionary neuroscience. The first is the argument between E.G. St. Hilaire and G. Cuvier over the topic of "common plan versus diversity".[2] St. Hilaire argued that all animals are built based on a single plan or archetype and he stressed the importance of homologies between organisms, while Cuvier believed that the structure of organs was determined by their function and that knowledge of the function of one organ could help discover the functions of other organs.[2][4] He argued that there were at least four different archetypes. After Darwin, the idea of evolution was more accepted and St. Hilaire's idea of homologous structures was more accepted. The second major argument is that of Aristotle's scala naturae (scale of nature) and the great chain of being versus the phylogenetic bush. The scala naturae, later also called the phylogenetic scale, was based on the premise that phylogenies are linear or like a scale while the phylogenetic bush argument was based on the idea that phylogenies were not linear, and more resembled a bush – the currently accepted view. A third major argument dealt with the size of the brain and whether relative size or absolute size was more relevant in determining function. In the late 18th century, it was determined that brain to body ratio reduces as body size increases. However more recently, there is more focus on absolute brain size as this scales with internal structures and functions, with the degree of structural complexity, and with the amount of white matter in the brain, all suggesting that absolute size is much better predictor of brain function. Finally, a fourth argument is that of natural selection (Darwinism) versus developmental constraints (concerted evolution). It is now accepted that the evolution of development is what causes adult species to show differences and evolutionary neuroscientists maintain that many aspects of brain function and structure are conserved across species.[2]
Throughout history, we see how evolutionary neuroscience has been dependent on developments in biological theory and techniques.[4] The field of evolutionary neuroscience has been shaped by the development of new techniques that allow for the discovery and examination of parts of the nervous system. In 1873, C. Golgi devised the silver nitrate method which allowed for the description of the brain at the cellular level as opposed to simply the gross level. Santiago and Pedro Ramon used this method to analyze numerous parts of brains, broadening the field of comparative neuroanatomy. In the second half of the 19th century, new techniques allowed scientists to identify neuronal cell groups and fiber bundles in brains. In 1885, Vittorio Marchi discovered a staining technique that let scientists see induced axonal degeneration in myelinated axons, in 1950, the "original nauta procedure" allowed for more accurate identification of degenerating fibers, and in the 1970s, there were several discoveries of multiple molecular tracers which would be used for experiments even today. In the last 20 years, cladistics has also become a useful tool for looking at variation in the brain.[4]
Many of Earth's early years were filled with brainless creatures, and among them was the amphioxus, which can be traced as far back as 550 million years ago. Amphioxi had a significantly simpler way of life, which made it not necessary for them to have a brain. To replace its absence of a brain, the prehistoric amphioxi had a limited nervous system, which was composed of only a bunch of cells. These cells optimized their uses because many of the cells for sensing intertwined with the cells used for its very simple system for moving, which allowed it to propel itself through bodies of water and react without much processing while the cells remaining were used for the detection of light to account to the fact that it had no eyes. It also did not need a sense of hearing. Even though the amphioxi had limited senses, they did not need them to survive efficiently, as their life was mainly dedicated to sitting on the seafloor to eat.[6](pp 1–2) Although the amphioxus' "brain" might seem severely underdeveloped compared to their human counterparts, it was set well for its respective environment, which has allowed it to prosper for millions of years.
Although many scientists once assumed that the brain evolved to achieve an ability to think, such a view is today considered a great misconception. 500 million years ago, the Earth entered into the Cambrian period, where hunting became a new concern for survival in an animal's environment. At this point, animals became sensitive to the presence of another, which could serve as food. Although hunting did not inherently require a brain, it was one of the main steps that pushed the development of one, as organisms progressed to develop advanced sensory systems.[6](pp 2, 4–5)
In response to progressively complicated surroundings, where competition between animals with brains started to arise for survival, animals had to learn to manage their energy.[6](pp 5–6) As creatures acquired a variety of senses for perception, animals progressed to develop allostasis, which played the role of an early brain by forcing the body to gather past experiences to improve prediction. Since prediction beat reaction, organisms who planned their manoeuvres were more likely to survive than those who did not. This came with equally managing energy adequately, which nature favoured. Animals that had not developed allostasis would be at a disadvantage for their purpose of exploration, foraging and reproduction, as death was a higher risk factor.[6](pp 7–8)
As allostasis continued to develop in animals, their bodies equally continuously evolved in size and complexity. They progressively started to develop cardiovascular systems, respiratory systems and immune systems to survive in their environments, which required bodies to have something more complex than the limited quality of cells to regulate themselves. This encouraged the nervous systems of many creatures to develop into a brain, which was sizeable and strikingly similar to how most animal brains look today.[6](pp 9–10)
Darwin, in The Descent of Man, stipulated that the mind evolved simultaneously with the body. According to his theory, all humans have a barbaric core that they learn to deal with.[6](p 17) Darwin's theory allowed people to start thinking about the way animals and their brains evolve.[4]
Plato's insight on the evolution of the human brain contemplated the idea that all humans were once lizards, with similar survival needs such as feeding, fighting and mating. In the classical era Plato first described this concept as the "lizard mind" – the deepest layer and one of three parts of his conception of a three-part human mind. In the 20th century P. MacLean developed a similar, modern triune brain theory.[6](pp 14–16)
Recent research in molecular genetics has demonstrated evidence that there is no difference in the neurons that reptiles and nonhuman mammals have when compared to humans. Instead, new research speculates that all mammals, and potentially reptiles, birds and some species of fish, evolve from a common order pattern. This research reinforces the idea that human brains are structurally no t any different from many other organisms.[6](p 19–21)
The cerebral cortex of reptiles resembles that of mammals, although simplified.[2] Although the evolution and function of the human cerebral cortex is still shrouded in mystery, we know that it is the most dramatically changed part of the brain during recent evolution. The reptilian brain, 300 million years ago, was made for all our basic urges and instincts like fighting, reproducing, and mating. The reptile brain evolved 100 million years later and gave us the ability to feel emotion. Eventually, it was able to develop a rational part that controls our inner animal.
Vision allows humans to process the world surrounding them to a certain extent. Through the wavelengths of light, the human brain can associate them to a specific event. Although the brain obviously perceives its surroundings at a specific moment, the brain equally predicts the upcoming changes in the environment.[6](p 66, 72) Once it has noticed them, the brain begins to prepare itself to encounter the new scenario by attempting to develop an adequate response. This is accomplished by using the data the brain has at its access, which can be to use past experiences and memories to form a proper response.[6](pp 66–67)</ref> However, sometimes the brain fails to predict accurately which means that the mind perceives a false illustration. Such an incorrect image occurs when the brain uses an inadequate memory to respond to what it is facing, which means that the memory does not correlate with the real scenario.[6](pp 75–76)
Research about how visual perception has developed in evolution is today best understood through studying present-day primates since the organization of the brain cannot be ascertained only by analyzing fossilized skulls.
The brain interprets visual information in the occipital lobe, a region in the back of the brain. The occipital lobe contains the visual cortex and the thalamus, which are the two main actors in processing visual information. The process of interpreting information has proven to be more complex than "what you see is what you get". Misinterpreting visual information is more common than previously believed.
As knowledge of the human brain has evolved, researchers discover that our visual perception is much closer to a construction of the brain than a direct "photograph" of what is in front of us. This can lead to misperceiving certain situations or elements in the brain's attempt to keep us safe. For example, an on-edge soldier believes a young child with a stick is a grown man with a gun, as the brain's sympathetic system, or fight-or-flight mode, is activated.[6]
An example of this phenomenon can be observed in the rabbit-duck illusion. Depending on how the image is looked at, the brain can interpret the image of a rabbit, or a duck. There is no right or wrong answer, but it is proof that what is seen may not be the reality of the situation.
The organization of the human auditory cortex is divided into core, belt, and parabelt. This closely resembles that of present-day primates.
The concept of auditory perception resembles visual perception very similarly. Our brain is wired to act on what it expects to experience. The sense of hearing helps situate an individual, but it also gives them hints about what else is around them. If something moves, they know approximately where it is and by the tone of it, the brain can predict what moved. If someone were to hear leaves rustling in a forest, the brain might interpret that sound as being an animal which could be a dangerous factor, but it would simply be another person walking.[6] The brain can predict many things based on what it is interpreting, however, those predictions may not all be true.
Evidence of a rich cognitive life in primate relatives of humans is extensive, and a wide range of specific behaviours in line with Darwinian theory is well documented.[7][8][9] However, until recently, research has disregarded nonhuman primates in the context of evolutionary linguistics, primarily because unlike vocal learning birds, our closest relatives seem to lack imitative abilities. Evolutionary speaking, there is great evidence suggesting a genetic groundwork for the concept of languages has been in place for millions of years, as with many other capabilities and behaviours observed today.
While evolutionary linguists agree on the fact that volitional control over vocalizing and expressing language is a quite recent leap in the history of the human race, that is not to say auditory perception is a recent development as well. Research has shown substantial evidence of well-defined neural pathways linking cortices to organize auditory perception in the brain. Thus, the issue lies in our abilities to imitate sounds.[10]
Beyond the fact that primates may be poorly equipped to learn sounds, studies have shown them to learn and use gestures far better. Visual cues and motoric pathways developed millions of years earlier in our evolution, which seems to be one reason for our earlier ability to understand and use gestures.[11]
Evolution shows how certain environments and surroundings will favor the development of specific cognitive functions of the brain to aid an animal or in this case human to successfully live in that environment.
Cognitive specialization in a theory in which cognitive functions, such as the ability to communicate socially, can be passed down genetically through offspring. This would benefit species in the process of natural selection. As for studying this in relation to the human brain, it has been theorized that very specific social skills apart from language, such as trust, vulnerability, navigation, and self-awareness can also be passed by offspring.[12]

---

# Evolutionary origin of religion

The evolutionary origin of religion and religious behavior is a field of study related to evolutionary psychology, the origin of language and mythology, and cross-cultural comparison of the anthropology of religion. Some subjects of interest include Neolithic religion, evidence for spirituality or cultic behavior in the Upper Paleolithic, and similarities in great ape behavior.
Humanity's closest living relatives are common chimpanzees and bonobos.[1][2] These primates share a common ancestor with humans who lived between six and eight million years ago. It is for this reason that chimpanzees and bonobos are viewed as the best available surrogate for this common ancestor. Barbara King argues that while non-human primates are not religious, they do exhibit some traits that would have been necessary for the evolution of religion. These traits include high intelligence, a capacity for symbolic communication, a sense of social norms, and realization of "self" continuity.[3][4]
Elephants perform rituals for their dead. They demonstrate long periods of silence and mourning at the point of death; later, elephants return to grave sites and caress the remains.[5][6] Some evidence suggests that many species grieve death and loss.[7]
In this set of theories, the religious mind is one consequence of a brain that is large enough to formulate religious and philosophical ideas.[8] During human evolution, the hominid brain tripled in size, peaking 500,000 years ago. Much of the brain's expansion took place in the neocortex. The cerebral neocortex is presumed to be responsible for the neural computations underlying complex phenomena such as perception, thought, language, attention, episodic memory and voluntary movement.[9] According to Dunbar's theory, the relative neocortex size of any species correlates with the level of social complexity of the particular species.[10] The neocortex size correlates with a number of social variables that include social group size and complexity of mating behaviors.[11] In chimpanzees the neocortex occupies 50% of the brain, whereas in modern humans it occupies 80% of the brain.[12]
Robin Dunbar argues that the critical event in the evolution of the neocortex took place at the speciation of archaic Homo sapiens about 500,000 years ago. His study indicates that only after the speciation event is the neocortex large enough to process complex social phenomena such as language and religion. The study is based on a regression analysis of neocortex size plotted against a number of social behaviors of living and extinct hominids.[13]
Stephen Jay Gould suggests that religion may have grown out of evolutionary changes that favored larger brains as a means of cementing group coherence among savanna hunters, after that larger brain enabled reflection on the inevitability of personal mortality.[14]
Lewis Wolpert argues that causal beliefs that emerged from tool use played a major role in the evolution of belief. The manufacture of complex tools requires creating a mental image of an object that does not exist naturally before actually making the artifact. Furthermore, one must understand how the tool would be used, that requires an understanding of causality.[15] Accordingly, the level of sophistication of stone tools is a useful indicator of causal beliefs.[16] Wolpert contends use of tools composed of more than one component, such as hand axes, represents an ability to understand cause and effect. However, recent studies of other primates indicate that causality may not be a uniquely human trait. For example, chimpanzees have been known to escape from pens closed with multiple latches, which was previously thought could only have been figured out by humans who understood causality. Chimpanzees are also known to mourn the dead, and notice things that have only aesthetic value, like sunsets, both of which may be considered to be components of religion or spirituality.[17][unreliable source?] The difference between the comprehension of causality by humans and chimpanzees is one of degree. The degree of comprehension in an animal depends upon the size of the prefrontal cortex: the greater the size of the prefrontal cortex the deeper the comprehension.[18][unreliable source?]
Religion requires a system of symbolic communication, such as language, to be transmitted from one individual to another. Philip Lieberman states "human religious thought and moral sense clearly rest on a cognitive-linguistic base".[19] From this premise science writer Nicholas Wade states:
Another view distinguishes individual religious belief from collective religious belief. While the former does not require prior development of language, the latter does. The individual human brain has to explain a phenomenon in order to comprehend and relate to it. This activity predates by far the emergence of language and may have caused it. The theory is, belief in the supernatural emerges from hypotheses arbitrarily assumed by individuals to explain natural phenomena that cannot be explained otherwise. The resulting need to share individual hypotheses with others leads eventually to collective religious belief. A socially accepted hypothesis becomes dogmatic backed by social sanction.
Language consists of digital contrasts whose cost is essentially zero. As pure social conventions, signals of this kind cannot evolve in a Darwinian social world—they are a theoretical impossibility.[21][22] Being intrinsically unreliable, language works only if one can build up a reputation for trustworthiness within a certain kind of society—namely, one where symbolic cultural facts (sometimes called 'institutional facts') can be established and maintained through collective social endorsement.[23] In any hunter-gatherer society, the basic mechanism for establishing trust in symbolic cultural facts is collective ritual.[24]
Transcending the continuity-versus-discontinuity divide, some scholars view the emergence of language as the consequence of some kind of social transformation[25] that, by generating unprecedented levels of public trust, liberated a genetic potential for linguistic creativity that had previously lain dormant.[26][27][28] "Ritual/speech coevolution theory" exemplifies this approach.[29][30] Scholars in this intellectual camp point to the fact that even chimpanzees and bonobos have latent symbolic capacities that they rarely—if ever—use in the wild.[31] Objecting to the sudden mutation idea, these authors argue that even if a chance mutation were to install a language organ in an evolving bipedal primate, it would be adaptively useless under all known primate social conditions. A very specific social structure—one capable of upholding unusually high levels of public accountability and trust—must have evolved before or concurrently with language to make reliance on "cheap signals" (words) an evolutionarily stable strategy. The animistic nature of early human language could serve as the handicap-like cost that helped to ensure the reliability of communication. The attribution of spiritual essence to everything surrounding early humans served as a built-in mechanism that provided instant verification and ensured the inviolability of one's speech.[32]
Animal vocal signals are, for the most part, intrinsically reliable. When a cat purrs, the signal constitutes direct evidence of the animal's contented state. The signal is trusted, not because the cat is inclined to be honest, but because it just cannot fake that sound. Primate vocal calls may be slightly more manipulable, but they remain reliable for the same reason—because they are hard to fake.[33] Primate social intelligence is "Machiavellian"—self-serving and unconstrained by moral scruples. Monkeys and apes often attempt to deceive each other, while at the same time remaining constantly on guard against falling victim to deception themselves.[34][35] Paradoxically, it is theorized that primates' resistance to deception is what blocks the evolution of their signalling systems along language-like lines. Language is ruled out because the best way to guard against being deceived is to ignore all signals except those that are instantly verifiable. Words automatically fail this test.[29]
Frans de Waal and Barbara King both view human morality as having grown out of primate sociality. Although morality awareness may be a unique human trait, many social animals, such as primates, dolphins and whales, have been known to exhibit pre-moral sentiments. According to Michael Shermer, the following characteristics are shared by humans and other social animals, particularly the great apes:
attachment and bonding, cooperation and mutual aid, sympathy and empathy, direct and indirect reciprocity, altruism and reciprocal altruism, conflict resolution and peacemaking, deception and deception detection, community concern and caring about what others think about you, and awareness of and response to the social rules of the group.[36]
De Waal contends that all social animals have had to restrain or alter their behavior for group living to be worthwhile. Pre-moral sentiments evolved in primate societies as a method of restraining individual selfishness and building more cooperative groups. For any social species, the benefits of being part of an altruistic group should outweigh the benefits of individualism. For example, a lack of group cohesion could make individuals more vulnerable to attack from outsiders. Being part of a group may also improve the chances of finding food. This is evident among animals that hunt in packs to take down large or dangerous prey.
All social animals have hierarchical societies in which each member knows its own place. Social order is maintained by certain rules of expected behavior and dominant group members enforce order through punishment. Additionally, higher order primates also have a sense of fairness. [37]
Chimpanzees live in fission-fusion groups that average 50 individuals. It is likely that early ancestors of humans lived in groups of similar size. Based on the size of extant hunter-gatherer societies, recent Paleolithic hominids lived in bands of a few hundred individuals. As community size increased over the course of human evolution, greater enforcement to achieve group cohesion would have been required. Morality may have evolved in these bands of 100 to 200 people as a means of social control, conflict resolution and group solidarity. According to Dr. de Waal, human morality has two extra levels of sophistication that are not found in primate societies.
Psychologist Matt J. Rossano argues that religion emerged after morality and built upon morality by expanding the social scrutiny of individual behavior to include supernatural agents. By including ever-watchful ancestors, spirits and gods in the social realm, humans discovered an effective strategy for restraining selfishness and building more cooperative groups.[38] The adaptive value of religion would have enhanced group survival.[39][40] Rossano is referring here to collective religious belief and the social sanction that institutionalized morality. According to Rossano's teaching, individual religious belief is thus initially epistemological, not ethical, in nature.
Cognitive scientists underlined that religions may be explained as a result of the brain architecture that developed early in the genus Homo in the course of the evolutionary history of life. Nonetheless, there is disagreement on the exact mechanisms that drove the evolution of the religious mind. The two main schools of thought hold:
Stephen Jay Gould, for example, saw religion as an exaptation or a spandrel, in other words: religion evolved as byproduct of psychological mechanisms that evolved for other reasons.[41][42][unreliable source?][43]
Such mechanisms may include the ability to infer the presence of organisms that might do harm (agent detection), the ability to come up with causal narratives for natural events (etiology), and the ability to recognize that other people have minds of their own with their own beliefs, desires and intentions (theory of mind). These three adaptations (among others) allow human beings to imagine purposeful agents behind many observations that could not readily be explained otherwise, e.g. thunder, lightning, movement of planets, complexity of life.[44] The emergence of collective religious belief identified such agents as deities that standardized the explanation.[45]
Some scholars have suggested that religion is genetically "hardwired" into the human condition. One controversial proposal, the God gene hypothesis, states that some variants of a specific gene, the VMAT2 gene, predispose to spirituality.[46]
Another view builds on the concept of the triune brain: the reptilian brain, the limbic system, and the neocortex, proposed by Paul D. MacLean. Collective religious belief draws upon the emotions of love, fear, and gregariousness and is deeply embedded in the limbic system through socio-biological conditioning and social sanction. Individual religious belief utilizes reason based in the neocortex and often varies from collective religion. The limbic system is much older in evolutionary terms than the neocortex and is, therefore, stronger than it – much in the same way as the reptilian is stronger than both the limbic system and the neocortex.
Yet another view is that the behavior of people who participate in a religion makes them feel better and this improves their biological fitness, so that there is a genetic selection in favor of people who are willing to believe in a religion. Specifically, rituals, beliefs, and the social contact typical of religious groups may serve to calm the mind (for example by reducing ambiguity and the uncertainty due to complexity) and allow it to function better when under stress.[47] This would allow religion to be used as a powerful survival mechanism, particularly in facilitating the evolution of hierarchies of warriors, which if true, may be why many modern religions tend to promote fertility and kinship.
Still another view, proposed by Fred H. Previc, sees human religion as a product of an increase in dopaminergic functions in the human brain and of a general intellectual expansion beginning around 80 thousand years ago (kya).[48][49][50] Dopamine promotes an emphasis on distant space and time, which can correlate with religious experience.[51] While the earliest extant shamanic cave-paintings date to around 40 kya, the use of ocher for rock art predates this and there is clear evidence for abstract thinking along the coast of South Africa 80 kya.
Paul Bloom suggests that "certain early emergent cognitive biases ... make it natural to believe in Gods and spirits".[52]
Although the exact time when humans first became religious remains unknown, research in evolutionary archaeology shows credible evidence of religious/ritualistic behavior from around the Middle Paleolithic era (45–200 thousand years ago).[53]
The earliest evidence of religious thought is based on the ritual treatment of the dead. Most animals display only a casual interest in the dead of their own species.[54] Ritual burial thus represents a significant change in human behavior. Ritual burials represent an awareness of life and death and a possible belief in the afterlife. Philip Lieberman states "burials with grave goods clearly signify religious practices and concern for the dead that transcends daily life."[19]
The earliest evidence for treatment of the dead comes from Atapuerca in Spain. At this location the bones of 30 individuals believed to be Homo heidelbergensis have been found in a pit.[55] Neanderthals are also contenders for the first hominids to intentionally bury the dead. They may have placed corpses into shallow graves along with stone tools and animal bones. The presence of these grave goods may indicate an emotional connection with the deceased and possibly a belief in the afterlife. Neanderthal burial sites include Shanidar in Iraq and Krapina in Croatia and Kebara Cave in Israel.[56][57]
The earliest known burial of modern humans is from a cave in Israel located at Qafzeh. Human remains have been dated to 100,000 years ago. Human skeletons were found stained with red ocher. A variety of grave goods were found at the burial site. The mandible of a wild boar was found placed in the arms of one of the skeletons.[58] Philip Lieberman states:
Burial rituals incorporating grave goods may have been invented by the anatomically modern hominids who emigrated from Africa to the Middle East roughly 100,000 years ago
Matt Rossano suggests that the period between 80,000 and 60,000 years before present, following the retreat of humans from the Levant to Africa, was a crucial period in the evolution of religion.[59]
The use of symbolism in religion is a universal established phenomenon. Archeologist Steven Mithen contends that it is common for religious practices to involve the creation of images and symbols to represent supernatural beings and ideas. Because supernatural beings violate the principles of the natural world, there will always be difficulty in communicating and sharing supernatural concepts with others. This problem can be overcome by anchoring these supernatural beings in material form through representational art. When translated into material form, supernatural concepts become easier to communicate and understand.[60][unreliable source?] Due to the association of art and religion, evidence of symbolism in the fossil record is indicative of a mind capable of religious thoughts. Art and symbolism demonstrates a capacity for abstract thought and imagination necessary to construct religious ideas. Wentzel van Huyssteen states that the translation of the non-visible through symbolism enabled early human ancestors to hold beliefs in abstract terms.[61]
Some of the earliest evidence of symbolic behavior is associated with Middle Stone Age sites in Africa. From at least 100,000 years ago, there is evidence of the use of pigments such as red ocher. Pigments are of little practical use to hunter gatherers, thus evidence of their use is interpreted as symbolic or for ritual purposes. Among extant hunter gatherer populations around the world, red ocher is still used extensively for ritual purposes. It has been argued that it is universal among human cultures for the color red to represent blood, sex, life and death.[62]
The use of red ocher as a proxy for symbolism is often criticized as being too indirect. Some scientists, such as Richard Klein and Steven Mithen, only recognize unambiguous forms of art as representative of abstract ideas. Upper Paleolithic cave art provides some of the most unambiguous evidence of religious thought from the Paleolithic. Cave paintings at Chauvet depict creatures that are half human and half animal.
Organized religion traces its roots to the Neolithic Revolution that began 11,000 years ago in the Near East, but may have occurred independently in several other locations around the world. The invention of agriculture transformed many human societies from a hunter-gatherer lifestyle to a sedentary lifestyle. The Neolithic Revolution led to a population explosion and an acceleration in the pace of technological development. The transition from foraging bands to states and empires precipitated more specialized and developed forms of religion that reflected the new social and political environment. While bands and small tribes possess supernatural beliefs, these beliefs do not serve to justify a central authority, justify transfer of wealth or maintain peace between unrelated individuals. Organized religion emerged as a means of providing social and economic stability through the following ways:
The states born out of the Neolithic Revolution, such as those of Ancient Egypt and Mesopotamia, were theocracies with chiefs, kings and emperors playing dual roles of political and spiritual leaders.[36] Anthropologists have found that virtually all state societies and chiefdoms from around the world have been found to justify political power through divine authority. This suggests that political authority co-opts collective religious belief to bolster itself.[36]
Following the Neolithic Revolution, the pace of technological development (cultural evolution) intensified due to the invention of writing 5,000 years ago. Symbols that became words later on made effective communication of ideas possible. Printing, invented only over a thousand years ago, rapidly increased the speed of communication and became the main spring of cultural evolution. Writing is thought to have been first invented in either Sumeria or Ancient Egypt, and was initially used for accounting. Soon after, writing was used to record myth. The first religious texts mark the beginning of religious history. The Pyramid Texts from ancient Egypt form one of the oldest known religious texts in the world, dating to between 2400 and 2300 BCE.[65][66][67][unreliable source?] Writing played a major role in sustaining and spreading organized religion. In pre-literate societies, religious ideas were based on an oral tradition, which was articulated by shamans and remained limited to the collective memories of the society's inhabitants. With the advent of writing, information that was not easy to remember could easily be stored in sacred texts that were maintained by a select group (clergy). Humans could store and process large amounts of information with writing that otherwise would have been forgotten. Writing therefore enabled religions to develop coherent and comprehensive doctrinal systems that remained independent of time and place.[68] Writing also brought a measure of objectivity to human knowledge. Formulation of thoughts in words and the requirement for validation made possible the mutual exchange of ideas and the sifting of generally acceptable from unacceptable ideas. The generally acceptable ideas became objective knowledge reflecting the continuously evolving framework of human awareness of reality that Karl Popper calls 'verisimilitude' – a stage on the human journey to truth.[69]

---

# Evolutionary psychology

Evolutionary psychology is a theoretical approach in psychology that examines cognition and behavior from a modern evolutionary perspective.[1][2] It seeks to identify human psychological adaptations with regard to the ancestral problems they evolved to solve. In this framework, psychological traits and mechanisms are either functional products of natural and sexual selection or non-adaptive by-products of other adaptive traits.[3][4]
Adaptationist thinking about physiological mechanisms, such as the heart, lungs, and the liver, is common in evolutionary biology. Evolutionary psychologists apply the same thinking in psychology, arguing that just as the heart evolved to pump blood, the liver evolved to detoxify poisons, and the kidneys evolved to filter turbid fluids there is modularity of mind in that different psychological mechanisms evolved to solve different adaptive problems.[5] These evolutionary psychologists argue that much of human behavior is the output of psychological adaptations that evolved to solve recurrent problems in human ancestral environments.[6]
Some evolutionary psychologists argue that evolutionary theory can provide a foundational, metatheoretical framework that integrates the entire field of psychology in the same way evolutionary biology has for biology.[5][7][8]
Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations,[9] including the abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, and cooperate with others. Findings have been made regarding human social behaviour related to infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price, and parental investment. The theories and findings of evolutionary psychology have applications in many fields, including economics, environment, health, law, management, psychiatry, politics, and literature.[10][11]
Criticism of evolutionary psychology involves questions of testability, cognitive and evolutionary assumptions (such as modular functioning of the brain, and large uncertainty about the ancestral environment), importance of non-genetic and non-adaptive explanations, as well as political and ethical issues due to interpretations of research results.
Its central assumption is that the human brain is composed of a large number of specialized mechanisms that were shaped by natural selection over a vast period of time to solve the recurrent information-processing problems faced by our ancestors. These problems involve food choices, social hierarchies, distributing resources to offspring, and selecting mates.[2] Proponents suggest that it seeks to integrate psychology into the other natural sciences, rooting it in the organizing theory of biology (evolutionary theory), and thus understanding psychology as a branch of biology. Anthropologist John Tooby and psychologist Leda Cosmides note:
Evolutionary psychology is the long-forestalled scientific attempt to assemble out of the disjointed, fragmentary, and mutually contradictory human disciplines a single, logically integrated research framework for the psychological, social, and behavioral sciences – a framework that not only incorporates the evolutionary sciences on a full and equal basis, but that systematically works out all of the revisions in existing belief and research practice that such a synthesis requires.[12]
Just as human physiology and evolutionary physiology have worked to identify physical adaptations of the body that represent "human physiological nature," the purpose of evolutionary psychology is to identify evolved emotional and cognitive adaptations that represent "human psychological nature." According to Steven Pinker, it is "not a single theory but a large set of hypotheses" and a term that "has also come to refer to a particular way of applying evolutionary theory to the mind, with an emphasis on adaptation, gene-level selection, and modularity." Evolutionary psychology adopts an understanding of the mind that is based on the computational theory of mind. It describes mental processes as computational operations, so that, for example, a fear response is described as arising from a neurological computation that inputs the perceptional data, e.g. a visual image of a spider, and outputs the appropriate reaction, e.g. fear of possibly dangerous animals. Under this view, any domain-general learning is impossible because of the combinatorial explosion. Evolutionary Psychology specifies the domain as the problems of survival and reproduction.[13]
While philosophers have generally considered the human mind to include broad faculties, such as reason and lust, evolutionary psychologists describe evolved psychological mechanisms as narrowly focused to deal with specific issues, such as catching cheaters or choosing mates. The discipline sees the human brain as having evolved specialized functions, called cognitive modules, or psychological adaptations which are shaped by natural selection.[14] Examples include language-acquisition modules, incest-avoidance mechanisms, cheater-detection mechanisms, intelligence and sex-specific mating preferences, foraging mechanisms, alliance-tracking mechanisms, agent-detection mechanisms, and others. Some mechanisms, termed domain-specific, deal with recurrent adaptive problems over the course of human evolutionary history. Domain-general mechanisms, on the other hand, are proposed to deal with evolutionary novelty.[15]
Evolutionary psychology has roots in cognitive psychology and evolutionary biology but also draws on behavioral ecology, artificial intelligence, genetics, ethology, anthropology, archaeology, biology, ecopsycology and zoology. It is closely linked to sociobiology,[9] but there are key differences between them including the emphasis on domain-specific rather than domain-general mechanisms, the relevance of measures of current fitness, the importance of mismatch theory, and psychology rather than behavior.
Nikolaas Tinbergen's four categories of questions can help to clarify the distinctions between several different, but complementary, types of explanations.[16] Evolutionary psychology focuses primarily on the "why?" questions, while traditional psychology focuses on the "how?" questions.[17]
Evolutionary psychology is founded on several core premises.
Evolutionary psychology has its historical roots in Charles Darwin's theory of natural selection.[9] In The Origin of Species, Darwin predicted that psychology would develop an evolutionary basis:
In the distant future I see open fields for far more important researches. Psychology will be based on a new foundation, that of the necessary acquirement of each mental power and capacity by gradation.
Two of his later books were devoted to the study of animal emotions and psychology; The Descent of Man, and Selection in Relation to Sex in 1871 and The Expression of the Emotions in Man and Animals in 1872. Darwin's work inspired William James's functionalist approach to psychology.[9] Darwin's theories of evolution, adaptation, and natural selection have provided insight into why brains function the way they do.[21]
The content of evolutionary psychology has derived from, on the one hand, the biological sciences (especially evolutionary theory as it relates to ancient human environments, the study of paleoanthropology and animal behavior) and, on the other, the human sciences, especially psychology.
Evolutionary biology as an academic discipline emerged with the modern synthesis in the 1930s and 1940s.[22] In the 1930s the study of animal behavior (ethology) emerged with the work of the Dutch biologist Nikolaas Tinbergen and the Austrian biologists Konrad Lorenz and Karl von Frisch.
W.D. Hamilton's (1964) papers on inclusive fitness and Robert Trivers's (1972)[23] theories on reciprocity and parental investment helped to establish evolutionary thinking in psychology and the other social sciences. In 1975, Edward O. Wilson combined evolutionary theory with studies of animal and social behavior, building on the works of Lorenz and Tinbergen, in his book Sociobiology: The New Synthesis.
In the 1970s, two major branches developed from ethology. Firstly, the study of animal social behavior (including humans) generated sociobiology, defined by its pre-eminent proponent Edward O. Wilson in 1975 as "the systematic study of the biological basis of all social behavior"[24] and in 1978 as "the extension of population biology and evolutionary theory to social organization."[25] Secondly, there was behavioral ecology which placed less emphasis on social behavior; it focused on the ecological and evolutionary basis of animal and human behavior.
In the 1970s and 1980s university departments began to include the term evolutionary biology in their titles. The modern era of evolutionary psychology was ushered in, in particular, by Donald Symons' 1979 book The Evolution of Human Sexuality and Leda Cosmides and John Tooby's 1992 book The Adapted Mind.[9] David Buller observed that the term "evolutionary psychology" is sometimes seen as denoting research based on the specific methodological and theoretical commitments of certain researchers from the Santa Barbara school (University of California), thus some evolutionary psychologists prefer to term their work "human ecology", "human behavioural ecology" or "evolutionary anthropology" instead.[26]
From psychology there are the primary streams of developmental, social and cognitive psychology. Establishing some measure of the relative influence of genetics and environment on behavior has been at the core of behavioral genetics and its variants, notably studies at the molecular level that examine the relationship between genes, neurotransmitters and behavior. Dual inheritance theory (DIT), developed in the late 1970s and early 1980s, has a slightly different perspective by trying to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. DIT is seen by some as a "middle-ground" between views that emphasize human universals versus those that emphasize cultural variation.[27]
The theories on which evolutionary psychology is based originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.
Evolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations.[28] As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness.[28][29] Sexual selection provides organisms with adaptations related to mating.[28] For male mammals, which have a relatively high maximal potential reproduction rate, sexual selection leads to adaptations that help them compete for females.[28] For female mammals, with a relatively low maximal potential reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates.[28] Charles Darwin described both natural selection and sexual selection, and he relied on group selection to explain the evolution of altruistic (self-sacrificing) behavior. But group selection was considered a weak explanation, because in any group the less altruistic individuals will be more likely to survive, and the group will become less self-sacrificing as a whole.
In 1964, the evolutionary biologist William D. Hamilton proposed inclusive fitness theory, emphasizing a gene-centered view of evolution. Hamilton noted that genes can increase the replication of copies of themselves into the next generation by influencing the organism's social traits in such a way that (statistically) results in helping the survival and reproduction of other copies of the same genes (most simply, identical copies in the organism's close relatives). According to Hamilton's rule, self-sacrificing behaviors (and the genes influencing them) can evolve if they typically help the organism's close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how altruism can evolve. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories help to explain the development of altruistic behavior, and account for hostility toward cheaters (individuals that take advantage of others' altruism).[30]
Several mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring, while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers–Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture, and culture, in turn, affecting human evolution on a genetic level, in a similar way to the Baldwin effect.
Evolutionary psychology is based on the hypothesis that, just like hearts, lungs, livers, kidneys, and immune systems, cognition has a functional structure that has a genetic basis, and therefore has evolved by natural selection. Like other organs and tissues, this functional structure should be universally shared amongst a species and should solve important problems of survival and reproduction.
Evolutionary psychologists seek to understand psychological mechanisms by understanding the survival and reproductive functions they might have served over the course of evolutionary history.[31][page needed] These might include abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, cooperate with others and follow leaders. Consistent with the theory of natural selection, evolutionary psychology sees humans as often in conflict with others, including mates and relatives. For instance, a mother may wish to wean her offspring from breastfeeding earlier than does her infant, which frees up the mother to invest in additional offspring.[30][32] Evolutionary psychology also recognizes the role of kin selection and reciprocity in evolving prosocial traits such as altruism.[30] Like chimpanzees and bonobos, humans have subtle and flexible social instincts, allowing them to form extended families, lifelong friendships, and political alliances.[30] In studies testing theoretical predictions, evolutionary psychologists have made modest findings on topics such as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price and parental investment.[33]
Another example would be the evolved mechanism in depression. Clinical depression is maladaptive and should have evolutionary approaches so it can become adaptive. Over the centuries animals and humans have gone through hard times to stay alive, which made our fight or flight senses evolve tremendously. For instances, mammalians have separation anxiety from their guardians which causes distress and sends signals to their hypothalamic pituitary adrenal axis, and emotional/behavioral changes. Going through these types of circumstances helps mammals cope with separation anxiety.[34]
Proponents of evolutionary psychology in the 1990s made some explorations in historical events, but the response from historical experts was highly negative and there has been little effort to continue that line of research. Historian Lynn Hunt says that the historians complained that the researchers:
have read the wrong studies, misinterpreted the results of experiments, or worse yet, turned to neuroscience looking for a universalizing, anti-representational and anti-intentional ontology to bolster their claims.[35]
Hunt states that "the few attempts to build up a subfield of psychohistory collapsed under the weight of its presuppositions." She concludes that, as of 2014, the "'iron curtain' between historians and psychology...remains standing."[36]
Not all traits of organisms are evolutionary adaptations. As noted in the table below, traits may also be exaptations, byproducts of adaptations (sometimes called "spandrels"), or random variation between individuals.[37]
Psychological adaptations are hypothesized to be innate or relatively easy to learn and to manifest in cultures worldwide. For example, the ability of toddlers to learn a language with virtually no training is likely to be a psychological adaptation. On the other hand, ancestral humans did not read or write, thus today, learning to read and write requires extensive training, and presumably involves the repurposing of cognitive capacities that evolved in response to selection pressures unrelated to written language.[38] However, variations in manifest behavior can result from universal mechanisms interacting with different local environments. For example, Caucasians who move from a northern climate to the equator will have darker skin. The mechanisms regulating their pigmentation do not change; rather the input to those mechanisms change, resulting in different outputs.
One of the tasks of evolutionary psychology is to identify which psychological traits are likely to be adaptations, byproducts or random variation. George C. Williams suggested that an "adaptation is a special and onerous concept that should only be used where it is really necessary."[39] As noted by Williams and others, adaptations can be identified by their improbable complexity, species universality, and adaptive functionality.
A question that may be asked about an adaptation is whether it is generally obligate (relatively robust in the face of typical environmental variation) or facultative (sensitive to typical environmental variation).[40] The sweet taste of sugar and the pain of hitting one's knee against concrete are the result of fairly obligate psychological adaptations; typical environmental variability during development does not much affect their operation. By contrast, facultative adaptations are somewhat like "if-then" statements. For example, adult attachment style seems particularly sensitive to early childhood experiences. As adults, the propensity to develop close, trusting bonds with others is dependent on whether early childhood caregivers could be trusted to provide reliable assistance and attention.[citation needed] The adaptation for skin to tan is conditional to exposure to sunlight; this is an example of another facultative adaptation. When a psychological adaptation is facultative, evolutionary psychologists concern themselves with how developmental and environmental inputs influence the expression of the adaptation.
Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations.[9] Cultural universals include behaviors related to language, cognition, social roles, gender roles, and technology.[41] Evolved psychological adaptations (such as the ability to learn a language) interact with cultural inputs to produce specific behaviors (e.g., the specific language learned).
Basic gender differences, such as greater eagerness for sex among men and greater coyness among women, are explained as sexually dimorphic psychological adaptations that reflect the different reproductive strategies of males and females. It has been found that both male and female personality traits differ on a large spectrum. Males had a higher rate of traits relating to dominance, tension, and directness. Females had higher rates organizational behavior and more emotional based characteristics.[42]
Evolutionary psychologists contrast their approach to what they term the "standard social science model," according to which the mind is a general-purpose cognition device shaped almost entirely by culture.[43][44]
Evolutionary psychology argues that to properly understand the functions of the brain, one must understand the properties of the environment in which the brain evolved. That environment is often referred to as the "environment of evolutionary adaptedness".[29]
The idea of an environment of evolutionary adaptedness was first explored as a part of attachment theory by John Bowlby.[45] This is the environment to which a particular evolved mechanism is adapted. More specifically, the environment of evolutionary adaptedness is defined as the set of historically recurring selection pressures that formed a given adaptation, as well as those aspects of the environment that were necessary for the proper development and functioning of the adaptation.
Humans, the genus Homo, appeared between 1.5 and 2.5 million years ago, a time that roughly coincides with the start of the Pleistocene 2.6 million years ago. Because the Pleistocene ended a mere 12,000 years ago, most human adaptations either newly evolved during the Pleistocene, or were maintained by stabilizing selection during the Pleistocene. Evolutionary psychology, therefore, proposes that the majority of human psychological mechanisms are adapted to reproductive problems frequently encountered in Pleistocene environments.[46] In broad terms, these problems include those of growth, development, differentiation, maintenance, mating, parenting, and social relationships.
The environment of evolutionary adaptedness is significantly different from modern society.[47] The ancestors of modern humans lived in smaller groups, had more cohesive cultures, and had more stable and rich contexts for identity and meaning.[47] Researchers look to existing hunter-gatherer societies for clues as to how hunter-gatherers lived in the environment of evolutionary adaptedness.[30] Unfortunately, the few surviving hunter-gatherer societies are different from each other, and they have been pushed out of the best land and into harsh environments, so it is not clear how closely they reflect ancestral culture.[30] However, all around the world small-band hunter-gatherers offer a similar developmental system for the young ("hunter-gatherer childhood model," Konner, 2005; "evolved developmental niche" or "evolved nest;" Narvaez et al., 2013). The characteristics of the niche are largely the same as for social mammals, who evolved over 30 million years ago: soothing perinatal experience, several years of on-request breastfeeding, nearly constant affection or physical proximity, responsiveness to need (mitigating offspring distress), self-directed play, and for humans, multiple responsive caregivers. Initial studies show the importance of these components in early life for positive child outcomes.[48][49]
Evolutionary psychologists sometimes look to chimpanzees, bonobos, and other great apes for insight into human ancestral behavior.[30]
Since an organism's adaptations were suited to its ancestral environment, a new and different environment can create a mismatch. Because humans are mostly adapted to Pleistocene environments, psychological mechanisms sometimes exhibit "mismatches" to the modern environment. One example is the fact that although over 20,000 people are murdered by guns in the US annually,[50] whereas spiders and snakes kill only a handful, people nonetheless learn to fear spiders and snakes about as easily as they do a pointed gun, and more easily than an unpointed gun, rabbits or flowers.[51] A potential explanation is that spiders and snakes were a threat to human ancestors throughout the Pleistocene, whereas guns (and rabbits and flowers) were not. There is thus a mismatch between humans' evolved fear-learning psychology and the modern environment.[52][53]
This mismatch also shows up in the phenomena of the supernormal stimulus, a stimulus that elicits a response more strongly than the stimulus for which the response evolved. The term was coined by Niko Tinbergen to refer to non-human animal behavior, but psychologist Deirdre Barrett said that supernormal stimulation governs the behavior of humans as powerfully as that of other animals. She explained junk food as an exaggerated stimulus to cravings for salt, sugar, and fats,[54] and she says that television is an exaggeration of social cues of laughter, smiling faces and attention-grabbing action.[55] Magazine centerfolds and double cheeseburgers pull instincts intended for an environment of evolutionary adaptedness where breast development was a sign of health, youth and fertility in a prospective mate, and fat was a rare and vital nutrient.[56] The psychologist Mark van Vugt recently argued that modern organizational leadership is a mismatch.[57] His argument is that humans are not adapted to work in large, anonymous bureaucratic structures with formal hierarchies. The human mind still responds to personalized, charismatic leadership primarily in the context of informal, egalitarian settings. Hence the dissatisfaction and alienation that many employees experience. Salaries, bonuses and other privileges exploit instincts for relative status, which attract particularly males to senior executive positions.[58]
Evolutionary theory is heuristic in that it may generate hypotheses that might not be developed from other theoretical approaches. One of the main goals of adaptationist research is to identify which organismic traits are likely to be adaptations, and which are byproducts or random variations. As noted earlier, adaptations are expected to show evidence of complexity, functionality, and species universality, while byproducts or random variation will not. In addition, adaptations are expected to be presented as proximate mechanisms that interact with the environment in either a generally obligate or facultative fashion (see above). Evolutionary psychologists are also interested in identifying these proximate mechanisms (sometimes termed "mental mechanisms" or "psychological adaptations") and what type of information they take as input, how they process that information, and their outputs.[40] Evolutionary developmental psychology, or "evo-devo," focuses on how adaptations may be activated at certain developmental times (e.g., losing baby teeth, adolescence, etc.) or how events during the development of an individual may alter life-history trajectories.
Evolutionary psychologists use several strategies to develop and test hypotheses about whether a psychological trait is likely to be an evolved adaptation. Buss (2011)[59] notes that these methods include:
Cross-cultural Consistency. Characteristics that have been demonstrated to be cross-cultural human universals such as smiling, crying, facial expressions are presumed to be evolved psychological adaptations. Several evolutionary psychologists have collected massive datasets from cultures around the world to assess cross-cultural universality. Function to Form (or "problem to solution"). The fact that males, but not females, risk potential misidentification of genetic offspring (referred to as "paternity uncertainty") led evolutionary psychologists to hypothesize that, compared to females, male jealousy would be more focused on sexual, rather than emotional, infidelity. Form to Function (reverse-engineering – or "solution to problem"). Morning sickness, and associated aversions to certain types of food, during pregnancy seemed to have the characteristics of an evolved adaptation (complexity and universality). Margie Profet hypothesized that the function was to avoid the ingestion of toxins during early pregnancy that could damage fetus (but which are otherwise likely to be harmless to healthy non-pregnant women). Corresponding Neurological Modules. Evolutionary psychology and cognitive neuropsychology are mutually compatible – evolutionary psychology helps to identify psychological adaptations and their ultimate, evolutionary functions, while neuropsychology helps to identify the proximate manifestations of these adaptations. Current Evolutionary Adaptiveness. In addition to evolutionary models that suggest evolution occurs across large spans of time, recent research has demonstrated that some evolutionary shifts can be fast and dramatic. Consequently, some evolutionary psychologists have focused on the impact of psychological traits in the current environment. Such research can be used to inform estimates of the prevalence of traits over time. Such work has been informative in studying evolutionary psychopathology.[60]
Evolutionary psychologists also use various sources of data for testing, including experiments, archaeological records, data from hunter-gatherer societies, observational studies, neuroscience data, self-reports and surveys, public records, and human products.[61] Recently, additional methods and tools have been introduced based on fictional scenarios,[62] mathematical models,[63] and multi-agent computer simulations.[64]
Foundational areas of research in evolutionary psychology can be divided into broad categories of adaptive problems that arise from evolutionary theory itself: survival, mating, parenting, family and kinship, interactions with non-kin, and cultural evolution.
Problems of survival are clear targets for the evolution of physical and psychological adaptations. Major problems the ancestors of present-day humans faced included food selection and acquisition; territory selection and physical shelter; and avoiding predators and other environmental threats.[65]
Consciousness meets George Williams' criteria of species universality, complexity,[66] and functionality, and it is a trait that apparently increases fitness.[67]
In his paper "Evolution of consciousness," John Eccles argues that special anatomical and physical adaptations of the mammalian cerebral cortex gave rise to consciousness.[68] In contrast, others have argued that the recursive circuitry underwriting consciousness is much more primitive, having evolved initially in pre-mammalian species because it improves the capacity for interaction with both social and natural environments by providing an energy-saving "neutral" gear in an otherwise energy-expensive motor output machine.[69] Once in place, this recursive circuitry may well have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms, as outlined by Bernard J. Baars.[70] Richard Dawkins suggested that humans evolved consciousness in order to make themselves the subjects of thought.[71] Daniel Povinelli suggests that large, tree-climbing apes evolved consciousness to take into account one's own mass when moving safely among tree branches.[71] Consistent with this hypothesis, Gordon Gallup found that chimpanzees and orangutans, but not little monkeys or terrestrial gorillas, demonstrated self-awareness in mirror tests.[71]
The concept of consciousness can refer to voluntary action, awareness, or wakefulness. However, even voluntary behavior involves unconscious mechanisms. Many cognitive processes take place in the cognitive unconscious, unavailable to conscious awareness. Some behaviors are conscious when learned but then become unconscious, seemingly automatic. Learning, especially implicitly learning a skill, can take place seemingly outside of consciousness. For example, plenty of people know how to turn right when they ride a bike, but very few can accurately explain how they actually do so.[71]
Evolutionary psychology approaches self-deception as an adaptation that can improve one's results in social exchanges.[71]
Sleep may have evolved to conserve energy when activity would be less fruitful or more dangerous, such as at night, and especially during the winter season.[71]
Many experts, such as Jerry Fodor, write that the purpose of perception is knowledge, but evolutionary psychologists hold that its primary purpose is to guide action.[72] For example, they say, depth perception seems to have evolved not to help us know the distances to other objects but rather to help us move around in space.[72] Evolutionary psychologists say that animals from fiddler crabs to humans use eyesight for collision avoidance, suggesting that vision is basically for directing action, not providing knowledge.[72]
Building and maintaining sense organs is metabolically expensive, so these organs evolve only when they improve an organism's fitness.[72] More than half the brain is devoted to processing sensory information, and the brain itself consumes roughly one-fourth of one's metabolic resources, so the senses must provide exceptional benefits to fitness.[72] Perception accurately mirrors the world; animals get useful, accurate information through their senses.[72]
Scientists who study perception and sensation have long understood the human senses as adaptations to their surrounding worlds.[72] Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world.[72] Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects.[72] Sound waves go around corners and interact with obstacles, creating a complex pattern that includes useful information about the sources of and distances to objects.[72] Larger animals naturally make lower-pitched sounds as a consequence of their size.[72] The range over which an animal hears, on the other hand, is determined by adaptation. Homing pigeons, for example, can hear the very low-pitched sound (infrasound) that carries great distances, even though most smaller animals detect higher-pitched sounds.[72] Taste and smell respond to chemicals in the environment that are thought to have been significant for fitness in the environment of evolutionary adaptedness.[72] For example, salt and sugar were apparently both valuable to the human or pre-human inhabitants of the environment of evolutionary adaptedness, so present-day humans have an intrinsic hunger for salty and sweet tastes.[72] The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain.[72] Pain, while unpleasant, is adaptive.[72] An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation.[72] For example, one's eyes automatically adjust to dim or bright ambient light.[72] Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.[72]
Evolutionary psychologists contend that perception demonstrates the principle of modularity, with specialized mechanisms handling particular perception tasks.[72] For example, people with damage to a particular part of the brain have the specific defect of not being able to recognize faces (prosopagnosia).[72] Evolutionary psychology suggests that this indicates a so-called face-reading module.[72]
In evolutionary psychology, learning is said to be accomplished through evolved capacities, specifically facultative adaptations.[73] Facultative adaptations express themselves differently depending on input from the environment.[73] Sometimes the input comes during development and helps shape that development.[73] For example, migrating birds learn to orient themselves by the stars during a critical period in their maturation.[73] Evolutionary psychologists believe that humans also learn language along an evolved program, also with critical periods.[73] The input can also come during daily tasks, helping the organism cope with changing environmental conditions.[73] For example, animals evolved Pavlovian conditioning in order to solve problems about causal relationships.[73] Animals accomplish learning tasks most easily when those tasks resemble problems that they faced in their evolutionary past, such as a rat learning where to find food or water.[73] Learning capacities sometimes demonstrate differences between the sexes.[73] In many animal species, for example, males can solve spatial problems faster and more accurately than females, due to the effects of male hormones during development.[73] The same might be true of humans.[73]
Motivations direct and energize behavior, while emotions provide the affective component to motivation, positive or negative.[74] In the early 1970s, Paul Ekman and colleagues began a line of research which suggests that many emotions are universal.[74] He found evidence that humans share at least five basic emotions: fear, sadness, happiness, anger, and disgust.[74] Social emotions evidently evolved to motivate social behaviors that were adaptive in the environment of evolutionary adaptedness.[74] For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared.[74] Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.[30][74] Motivation has a neurobiological basis in the reward system of the brain. Recently, it has been suggested that reward systems may evolve in such a way that there may be an inherent or unavoidable trade-off in the motivational system for activities of short versus long duration.[75]
Cognition refers to internal representations of the world and internal information processing. From an evolutionary psychology perspective, cognition is not "general purpose". Cognition uses heuristics, or strategies, that generally increase the likelihood of solving problems that the ancestors of present-day humans routinely faced in their lives. For example, present-day humans are far more likely to solve logic problems that involve detecting cheating (a common problem given humans' social nature) than the same logic problem put in purely abstract terms.[76] Since the ancestors of present-day humans did not encounter truly random events and lived under simpler life terms, present-day humans may be cognitively predisposed to incorrectly identify patterns in random sequences. "Gamblers' Fallacy" is one example of this. Gamblers may falsely believe that they have hit a "lucky streak" even when each outcome is actually random and independent of previous trials. Most people believe that if a fair coin has been flipped 9 times and Heads appears each time, that on the tenth flip, there is a greater than 50% chance of getting Tails.[74] Humans find it far easier to make diagnoses or predictions using frequency data than when the same information is presented as probabilities or percentages. This could be due to the ancestors of present-day humans living in relatively small tribes (usually with fewer than 150 people) where frequency information was more readily available and experienced less random occurrences in their lives.[74]
Evolutionary psychology is primarily interested in finding commonalities between people, or basic human psychological nature. From an evolutionary perspective, the fact that people have fundamental differences in personality traits initially presents something of a puzzle.[77] (Note: The field of behavioral genetics is concerned with statistically partitioning differences between people into genetic and environmental sources of variance. However, understanding the concept of heritability can be tricky – heritability refers only to the differences between people, never the degree to which the traits of an individual are due to environmental or genetic factors, since traits are always a complex interweaving of both.)
Personality traits are conceptualized by evolutionary psychologists as due to normal variation around an optimum, due to frequency-dependent selection (behavioral polymorphisms), or as facultative adaptations. Like variability in height, some personality traits may simply reflect inter-individual variability around a general optimum.[77] Or, personality traits may represent different genetically predisposed "behavioral morphs" – alternate behavioral strategies that depend on the frequency of competing behavioral strategies in the population. For example, if most of the population is generally trusting and gullible, the behavioral morph of being a "cheater" (or, in the extreme case, a sociopath) may be advantageous.[78] Finally, like many other psychological adaptations, personality traits may be facultative – sensitive to typical variations in the social environment, especially during early development. For example, later-born children are more likely than firstborns to be rebellious, less conscientious and more open to new experiences, which may be advantageous to them given their particular niche in family structure.[79]
Shared environmental influences do play a role in personality and are not always of less importance than genetic factors. However, shared environmental influences often decrease to near zero after adolescence but do not completely disappear.[80]
According to Steven Pinker, who builds on the work by Noam Chomsky, the universal human ability to learn to talk between the ages of 1 – 4, basically without training, suggests that language acquisition is a distinctly human psychological adaptation (see, in particular, Pinker's The Language Instinct). Pinker and Bloom (1990) argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop.[81]
Pinker follows Chomsky in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction. Chomsky himself does not believe language to have evolved as an adaptation, but suggests that it likely evolved as a byproduct of some other adaptation, a so-called spandrel. But Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.[82]
Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language.[83] In the 1980s, psycholinguist Myrna Gopnik identified a dominant gene that causes language impairment in the KE family of Britain.[83] This gene turned out to be a mutation of the FOXP2 gene.[83] Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history.[83] This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans.[83] However, the once-popular idea that FOXP2 is a 'grammar gene' or that it triggered the emergence of language in Homo sapiens is now widely discredited.[84]
Currently, several competing theories about the evolutionary origin of language coexist, none of them having achieved a general consensus.[85] Researchers of language acquisition in primates and humans such as Michael Tomasello and Talmy Givón, argue that the innatist framework has understated the role of imitation in learning and that it is not at all necessary to posit the existence of an innate grammar module to explain human language acquisition. Tomasello argues that studies of how children and primates actually acquire communicative skills suggest that humans learn complex behavior through experience, so that instead of a module specifically dedicated to language acquisition, language is acquired by the same cognitive mechanisms that are used to acquire all other kinds of socially transmitted behavior.[86]
On the issue of whether language is best seen as having evolved as an adaptation or as a spandrel, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptionist view of evolution, and dismisses Pinker and Bloom's question of whether "Language has evolved as an adaptation" as being misleading. He argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system.[87] A similar argument is made by Terrence Deacon who in The Symbolic Species argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.[88]
If the theory that language could have evolved as a single adaptation is accepted, the question becomes which of its many functions has been the basis of adaptation. Several evolutionary hypotheses have been posited: that language evolved for the purpose of social grooming, that it evolved as a way to show mating potential or that it evolved to form social contracts. Evolutionary psychologists recognize that these theories are all speculative and that much more evidence is required to understand how language might have been selectively adapted.[89]
Given that sexual reproduction is the means by which genes are propagated into future generations, sexual selection plays a large role in human evolution. Human mating, then, is of interest to evolutionary psychologists who aim to investigate evolved mechanisms to attract and secure mates.[90] Several lines of research have stemmed from this interest, such as studies of mate selection[91][92][93] mate poaching,[94] mate retention,[95] mating preferences[96] and conflict between the sexes.[97]
In 1972 Robert Trivers published an influential paper[98] on sex differences that is now referred to as parental investment theory. The size differences of gametes (anisogamy) is the fundamental, defining difference between males (small gametes – sperm) and females (large gametes – ova). Trivers noted that anisogamy typically results in different levels of parental investment between the sexes, with females initially investing more. Trivers proposed that this difference in parental investment leads to the sexual selection of different reproductive strategies between the sexes and to sexual conflict. For example, he suggested that the sex that invests less in offspring will generally compete for access to the higher-investing sex to increase their inclusive fitness. Trivers posited that differential parental investment led to the evolution of sexual dimorphisms in mate choice, intra- and inter- sexual reproductive competition, and courtship displays. In mammals, including humans, females make a much larger parental investment than males (i.e. gestation followed by childbirth and lactation). Parental investment theory is a branch of life history theory.
Buss and Schmitt's (1993) sexual strategies theory[99] proposed that, due to differential parental investment, humans have evolved sexually dimorphic adaptations related to "sexual accessibility, fertility assessment, commitment seeking and avoidance, immediate and enduring resource procurement, paternity certainty, assessment of mate value, and parental investment." Their strategic interference theory[100] suggested that conflict between the sexes occurs when the preferred reproductive strategies of one sex interfere with those of the other sex, resulting in the activation of emotional responses such as anger or jealousy.
Women are generally more selective when choosing mates, especially under long-term mating conditions. However, under some circumstances, engaging in multiple sexual relationships can provide benefits to women as well, such as fertility insurance, trading up to better genes, reducing the risk of inbreeding, and insurance protection of her offspring.[101]
Due to male paternity uncertainty, sex differences have been found in the domains of sexual jealousy.[102][103] Females generally react more adversely to emotional infidelity and males will react more to sexual infidelity. This particular pattern is predicted because the costs involved in mating for each sex are distinct. Women, on average, should prefer a mate who can offer resources (e.g., financial, commitment), thus, a woman risks losing such resources with a mate who commits emotional infidelity. Men, on the other hand, are never certain of the genetic paternity of their children because they do not bear the offspring themselves. This suggests that for men sexual infidelity would generally be more aversive than emotional infidelity because investing resources in another man's offspring does not lead to the propagation of their own genes.[104]
Another interesting line of research is that which examines women's mate preferences across the ovulatory cycle.[105][106] The theoretical underpinning of this research is that ancestral women would have evolved mechanisms to select mates with certain traits depending on their hormonal status. Known as the ovulatory shift hypothesis, the theory posits that, during the ovulatory phase of a woman's cycle (approximately days 10–15 of a woman's cycle),[107] a woman who mated with a male with high genetic quality would have been more likely, on average, to produce and bear a healthy offspring than a woman who mated with a male with low genetic quality. These putative preferences are predicted to be especially apparent for short-term mating domains because a potential male mate would only be offering genes to a potential offspring. This hypothesis allows researchers to examine whether women select mates who have characteristics that indicate high genetic quality during the high fertility phase of their ovulatory cycles. Indeed, studies have shown that women's preferences vary across the ovulatory cycle. In particular, Haselton and Miller (2006) showed that highly fertile women prefer creative but poor men as short-term mates. Creativity may be a proxy for good genes.[108] Research by Gangestad et al. (2004) indicates that highly fertile women prefer men who display social presence and intrasexual competition; these traits may act as cues that would help women predict which men may have, or would be able to acquire, resources.
Reproduction is always costly for women, and can also be for men. Individuals are limited in the degree to which they can devote time and resources to producing and raising their young, and such expenditure may also be detrimental to their future condition, survival and further reproductive output. Parental investment is any parental expenditure (time, energy etc.) that benefits one offspring at a cost to parents' ability to invest in other components of fitness (Clutton-Brock 1991: 9; Trivers 1972). Components of fitness (Beatty 1992) include the well-being of existing offspring, parents' future reproduction, and inclusive fitness through aid to kin (Hamilton, 1964). Parental investment theory is a branch of life history theory.
The benefits of parental investment to the offspring are large and are associated with the effects on condition, growth, survival, and ultimately, on the reproductive success of the offspring. However, these benefits can come at the cost of the parent's ability to reproduce in the future e.g. through the increased risk of injury when defending offspring against predators, the loss of mating opportunities whilst rearing offspring, and an increase in the time to the next reproduction. Overall, parents are selected to maximize the difference between the benefits and the costs, and parental care will likely evolve when the benefits exceed the costs.
The Cinderella effect is an alleged high incidence of stepchildren being physically, emotionally or sexually abused, neglected, murdered, or otherwise mistreated at the hands of their stepparents at significantly higher rates than their genetic counterparts. It takes its name from the fairy tale character Cinderella, who in the story was cruelly mistreated by her stepmother and stepsisters.[109] Daly and Wilson (1996) noted: "Evolutionary thinking led to the discovery of the most important risk factor for child homicide – the presence of a stepparent. Parental efforts and investments are valuable resources, and selection favors those parental psyches that allocate effort effectively to promote fitness. The adaptive problems that challenge parental decision-making include both the accurate identification of one's offspring and the allocation of one's resources among them with sensitivity to their needs and abilities to convert parental investment into fitness increments…. Stepchildren were seldom or never so valuable to one's expected fitness as one's own offspring would be, and those parental psyches that were easily parasitized by just any appealing youngster must always have incurred a selective disadvantage"(Daly & Wilson, 1996, pp. 64–65). However, they note that not all stepparents will "want" to abuse their partner's children, or that genetic parenthood is any insurance against abuse. They see step parental care as primarily "mating effort" towards the genetic parent.[110]
Inclusive fitness is the sum of an organism's classical fitness (how many of its own offspring it produces and supports) and the number of equivalents of its own offspring it can add to the population by supporting others.[111] The first component is called classical fitness by Hamilton (1964).
From the gene's point of view, evolutionary success ultimately depends on leaving behind the maximum number of copies of itself in the population. Until 1964, it was generally believed that genes only achieved this by causing the individual to leave the maximum number of viable offspring. However, in 1964 W. D. Hamilton proved mathematically that, because close relatives of an organism share some identical genes, a gene can also increase its evolutionary success by promoting the reproduction and survival of these related or otherwise similar individuals. Hamilton concluded that this leads natural selection to favor organisms that would behave in ways that maximize their inclusive fitness. It is also true that natural selection favors behavior that maximizes personal fitness.
Hamilton's rule describes mathematically whether or not a gene for altruistic behavior will spread in a population:
The concept serves to explain how natural selection can perpetuate altruism. If there is an "altruism gene" (or complex of genes) that influences an organism's behavior to be helpful and protective of relatives and their offspring, this behavior also increases the proportion of the altruism gene in the population, because relatives are likely to share genes with the altruist due to common descent. Altruists may also have some way to recognize altruistic behavior in unrelated individuals and be inclined to support them. As Dawkins points out in The Selfish Gene (Chapter 6) and The Extended Phenotype,[112] this must be distinguished from the green-beard effect.
Although it is generally true that humans tend to be more altruistic toward their kin than toward non-kin, the relevant proximate mechanisms that mediate this cooperation have been debated (see kin recognition), with some arguing that kin status is determined primarily via social and cultural factors (such as co-residence, maternal association of sibs, etc.),[113] while others have argued that kin recognition can also be mediated by biological factors such as facial resemblance and immunogenetic similarity of the major histocompatibility complex (MHC).[114] The interaction of these social and biological kin recognition factors was discussed in Lieberman, Tooby, and Cosmides (2007)[115]
Whatever the proximate mechanisms of kin recognition there is substantial evidence that humans act generally more altruistically to close genetic kin compared to genetic non-kin.[116][117][118]
Although interactions with non-kin are generally less altruistic compared to those with kin, cooperation can be maintained with non-kin via mutually beneficial reciprocity as was proposed by Robert Trivers.[23] If there are repeated encounters between the same two players in an evolutionary game in which each of them can choose either to "cooperate" or "defect", then a strategy of mutual cooperation may be favored even if it pays each player, in the short term, to defect when the other cooperates. Direct reciprocity can lead to the evolution of cooperation only if the probability, w, of another encounter between the same two individuals exceeds the cost-to-benefit ratio of the altruistic act:
Reciprocity can also be indirect if information about previous interactions is shared. Reputation allows evolution of cooperation by indirect reciprocity. Natural selection favors strategies that base the decision to help on the reputation of the recipient: studies show that people who are more helpful are more likely to receive help. The calculations of indirect reciprocity are complicated and only a tiny fraction of this universe has been uncovered, but again a simple rule has emerged.[119] Indirect reciprocity can only promote cooperation if the probability, q, of knowing someone's reputation exceeds the cost-to-benefit ratio of the altruistic act:
One important problem with this explanation is that individuals may be able to evolve the capacity to obscure their reputation, reducing the probability, q, that it will be known.[120]
Trivers argues that friendship and various social emotions evolved in order to manage reciprocity.[121] Liking and disliking, he says, evolved to help present-day humans' ancestors form coalitions with others who reciprocated and to exclude those who did not reciprocate.[121] Moral indignation may have evolved to prevent one's altruism from being exploited by cheaters, and gratitude may have motivated present-day humans' ancestors to reciprocate appropriately after benefiting from others' altruism.[121] Likewise, present-day humans feel guilty when they fail to reciprocate.[121] These social motivations match what evolutionary psychologists expect to see in adaptations that evolved to maximize the benefits and minimize the drawbacks of reciprocity.[121]
Evolutionary psychologists say that humans have psychological adaptations that evolved specifically to help us identify nonreciprocators, commonly referred to as "cheaters."[121] In 1993, Robert Frank and his associates found that participants in a prisoner's dilemma scenario were often able to predict whether their partners would "cheat", based on a half-hour of unstructured social interaction.[121] In a 1996 experiment, for example, Linda Mealey and her colleagues found that people were better at remembering the faces of people when those faces were associated with stories about those individuals cheating (such as embezzling money from a church).[121]
Humans may have an evolved set of psychological adaptations that predispose them to be more cooperative than otherwise would be expected with members of their tribal in-group, and, more nasty to members of tribal out groups. These adaptations may have been a consequence of tribal warfare.[122] Humans may also have predispositions for "altruistic punishment" – to punish in-group members who violate in-group rules, even when this altruistic behavior cannot be justified in terms of helping those you are related to (kin selection), cooperating with those who you will interact with again (direct reciprocity), or cooperating to better your reputation with others (indirect reciprocity).[123][124]
Though evolutionary psychology has traditionally focused on individual-level behaviors, determined by species-typical psychological adaptations, considerable work has been done on how these adaptations shape and, ultimately govern, culture (Tooby and Cosmides, 1989).[125] Tooby and Cosmides (1989) argued that the mind consists of many domain-specific psychological adaptations, some of which may constrain what cultural material is learned or taught. As opposed to a domain-general cultural acquisition program, where an individual passively receives culturally-transmitted material from the group, Tooby and Cosmides (1989), among others, argue that: "the psyche evolved to generate adaptive rather than repetitive behavior, and hence critically analyzes the behavior of those surrounding it in highly structured and patterned ways, to be used as a rich (but by no means the only) source of information out of which to construct a 'private culture' or individually tailored adaptive system; in consequence, this system may or may not mirror the behavior of others in any given respect." (Tooby and Cosmides 1989).[125]
Biological explanations of human culture also brought criticism to evolutionary psychology: Evolutionary psychologists see the human psyche and physiology as a genetic product and assume that genes contain the information for the development and control of the organism and that this information is transmitted from one generation to the next via genes.[126] Evolutionary psychologists thereby see physical and psychological characteristics of humans as genetically programmed. Even then, when evolutionary psychologists acknowledge the influence of the environment on human development, they understand the environment only as an activator or trigger for the programmed developmental instructions encoded in genes.[126][127] Evolutionary psychologists, for example, believe that the human brain is made up of innate modules, each of which is specialised only for very specific tasks, e. g. an anxiety module. According to evolutionary psychologists, these modules are given before the organism actually develops and are then activated by some environmental event. Critics object that this view is reductionist and that cognitive specialisation only comes about through the interaction of humans with their real environment, rather than the environment of distant ancestors.[126][127] Interdisciplinary approaches are increasingly striving to mediate between these opposing points of view and to highlight that biological and cultural causes need not be antithetical in explaining human behaviour and even complex cultural achievements.[128]
According to Paul Baltes, the benefits granted by evolutionary selection decrease with age. Natural selection has not eliminated many harmful conditions and nonadaptive characteristics that appear among older adults, such as Alzheimer disease. If it were a disease that killed 20-year-olds instead of 70-year-olds this might have been a disease that natural selection could have eliminated ages ago. Thus, unaided by evolutionary pressures against nonadaptive conditions, modern humans suffer the aches, pains, and infirmities of aging and as the benefits of evolutionary selection decrease with age, the need for modern technological mediums against non-adaptive conditions increases.[129]
As humans are a highly social species, there are many adaptive problems associated with navigating the social world (e.g., maintaining allies, managing status hierarchies, interacting with outgroup members, coordinating social activities, collective decision-making). Researchers in the emerging field of evolutionary social psychology have made many discoveries pertaining to topics traditionally studied by social psychologists, including person perception, social cognition, attitudes, altruism, emotions, group dynamics, leadership, motivation, prejudice, intergroup relations, and cross-cultural differences.[130][131][132][133]
When endeavouring to solve a problem humans at an early age show determination while chimpanzees have no comparable facial expression. Researchers suspect the human determined expression evolved because when a human is determinedly working on a problem other people will frequently help.[134]
Adaptationist hypotheses regarding the etiology of psychological disorders are often based on analogies between physiological and psychological dysfunctions,[135] as noted in the table below. Prominent theorists and evolutionary psychiatrists include Michael T. McGuire, Anthony Stevens, and Randolph M. Nesse. They, and others, suggest that mental disorders are due to the interactive effects of both nature and nurture, and often have multiple contributing causes.[17]
Evolutionary psychologists have suggested that schizophrenia and bipolar disorder may reflect a side-effect of genes with fitness benefits, such as increased creativity.[140] (Some individuals with bipolar disorder are especially creative during their manic phases and the close relatives of people with schizophrenia have been found to be more likely to have creative professions.[140]) A 1994 report by the American Psychiatry Association found that people with schizophrenia at roughly the same rate in Western and non-Western cultures, and in industrialized and pastoral societies, suggesting that schizophrenia is not a disease of civilization nor an arbitrary social invention.[140] Sociopathy may represent an evolutionarily stable strategy, by which a small number of people who cheat on social contracts benefit in a society consisting mostly of non-sociopaths.[17] Mild depression may be an adaptive response to withdraw from, and re-evaluate, situations that have led to disadvantageous outcomes (the "analytical rumination hypothesis")[138] (see Evolutionary approaches to depression).
Trofimova reviewed the most consistent psychological and behavioural sex differences in psychological abilities and disabilities and linked them to the Geodakyan's evolutionary theory of sex (ETS).[141] She pointed out that a pattern of consistent sex differences in physical, verbal and social dis/abilities corresponds to the idea of the ETS considering sex dimorphism as a functional specialization of a species. Sex differentiation, according to the ETS, creates two partitions within a species, (1) conservational (females), and (2) variational (males). In females, superiority in verbal abilities, higher rule obedience, socialisation, empathy and agreeableness can be presented as a reflection of the systemic conservation function of the female sex. Male superiority is mostly noted in exploratory abilities - in risk- and sensation seeking, spacial orientation, physical strength and higher rates in physical aggression. In combination with higher birth and accidental death rates this pattern might be a reflection of the systemic variational function (testing the boundaries of beneficial characteristics) of the male sex. As a result, psychological sex differences might be influenced by a global tendency within a species to expand its norm of reaction, but at the same time to preserve the beneficial properties of the species. Moreover, Trofimova[141] suggested a "redundancy pruning" hypothesis as an upgrade of the ETS theory. She pointed out to higher rates of psychopathy, dyslexia, autism and schizophrenia in males, in comparison to females. She suggested that the variational function of the "male partition" might also provide irrelevance/redundancy pruning of an excess in a bank of beneficial characteristics of a species, with a continuing resistance to any changes from the norm-driven conservational partition of species. This might explain a contradictory allocation of a high drive for social status/power in the male sex with the their least (among two sexes) abilities for social interaction. The high rates of communicative disorders and psychopathy in males might facilitate their higher rates of disengagement from normative expectations and their insensitivity to social disapproval, when they deliberately do not follow social norms.
Some of these speculations have yet to be developed into fully testable hypotheses, and a great deal of research is required to confirm their validity.[142][143]
Evolutionary psychology has been applied to explain criminal or otherwise immoral behavior as being adaptive or related to adaptive behaviors. Males are generally more aggressive than females, who are more selective of their partners because of the far greater effort they have to contribute to pregnancy and child-rearing. Males being more aggressive is hypothesized to stem from the more intense reproductive competition faced by them. Males of low status may be especially vulnerable to being childless. It may have been evolutionary advantageous to engage in highly risky and violently aggressive behavior to increase their status and therefore reproductive success. This may explain why males are generally involved in more crimes, and why low status and being unmarried are associated with criminality. Furthermore, competition over females is argued to have been particularly intensive in late adolescence and young adulthood, which is theorized to explain why crime rates are particularly high during this period.[144] Some sociologists have underlined differential exposure to androgens as the cause of these behaviors, notably Lee Ellis in his evolutionary neuroandrogenic (ENA) theory.[145]
Many conflicts that result in harm and death involve status, reputation, and seemingly trivial insults.[144] Steven Pinker in his book The Better Angels of Our Nature argues that in non-state societies without a police it was very important to have a credible deterrence against aggression. Therefore, it was important to be perceived as having a credible reputation for retaliation, resulting in humans developing instincts for revenge as well as for protecting reputation ("honor"). Pinker argues that the development of the state and the police have dramatically reduced the level of violence compared to the ancestral environment. Whenever the state breaks down, which can be very locally such as in poor areas of a city, humans again organize in groups for protection and aggression and concepts such as violent revenge and protecting honor again become extremely important.[144]
Rape is theorized to be a reproductive strategy that facilitates the propagation of the rapist's progeny. Such a strategy may be adopted by men who otherwise are unlikely to be appealing to women and therefore cannot form legitimate relationships, or by high-status men on socially vulnerable women who are unlikely to retaliate to increase their reproductive success even further.[146] The sociobiological theories of rape are highly controversial, as traditional theories typically do not consider rape to be a behavioral adaptation, and objections to this theory are made on ethical, religious, political, as well as scientific grounds.
Adaptationist perspectives on religious belief suggest that, like all behavior, religious behaviors are a product of the human brain. As with all other organ functions, cognition's functional structure has been argued to have a genetic foundation, and is therefore subject to the effects of natural selection and sexual selection. Like other organs and tissues, this functional structure should be universally shared amongst humans and should have solved important problems of survival and reproduction in ancestral environments. However, evolutionary psychologists remain divided on whether religious belief is more likely a consequence of evolved psychological adaptations,[147][148] or a byproduct of other cognitive adaptations.[149]
Coalitional psychology is an approach to explain political behaviors between different coalitions and the conditionality of these behaviors in evolutionary psychological perspective. This approach assumes that since human beings appeared on the earth, they have evolved to live in groups instead of living as individuals to achieve benefits such as more mating opportunities and increased status.[150] Human beings thus naturally think and act in a way that manages and negotiates group dynamics.
Coalitional psychology offers falsifiable ex ante prediction by positing five hypotheses on how these psychological adaptations operate:[151]
Critics of evolutionary psychology accuse it of promoting genetic determinism, pan-adaptationism (the idea that all behaviors and anatomical features are adaptations), unfalsifiable hypotheses, distal or ultimate explanations of behavior when proximate explanations are superior, and malevolent political or moral ideas.[152]
Critics have argued that evolutionary psychology might be used to justify existing social hierarchies and reactionary policies.[153][154] It has also been suggested by critics that evolutionary psychologists' theories and interpretations of empirical data rely heavily on ideological assumptions about race and gender.[155]
In response to such criticism, evolutionary psychologists often caution against committing the naturalistic fallacy – the assumption that "what is natural" is necessarily a moral good.[154][156][page needed][157] However, their caution against committing the naturalistic fallacy has been criticized as means to stifle legitimate ethical discussions.[154]
Some criticisms of evolutionary psychology point at contradictions between different aspects of adaptive scenarios posited by evolutionary psychology. One example is the evolutionary psychology model of extended social groups selecting for modern human brains, a contradiction being that the synaptic function of modern human brains require high amounts of many specific essential nutrients so that such a transition to higher requirements of the same essential nutrients being shared by all individuals in a population would decrease the possibility of forming large groups due to bottleneck foods with rare essential nutrients capping group sizes. It is mentioned that some insects have societies with different ranks for each individual and that monkeys remain socially functioning after the removal of most of the brain as additional arguments against big brains promoting social networking. The model of males as both providers and protectors is criticized for the impossibility of being in two places at once, the male cannot both protect his family at home and be out hunting at the same time. In the case of the claim that a provider male could buy protection service for his family from other males by bartering food that he had hunted, critics point at the fact that the most valuable food (the food that contained the rarest essential nutrients) would be different in different ecologies and as such vegetable in some geographical areas and animal in others, making it impossible for hunting styles relying on physical strength or risk-taking to be universally of similar value in bartered food and instead of making it inevitable that in some parts of Africa, food gathered with no need for major physical strength would be the most valuable to barter for protection. A contradiction between evolutionary psychology's claim of men needing to be more sexually visual than women for fast speed of assessing women's fertility than women needed to be able to assess the male's genes and its claim of male sexual jealousy guarding against infidelity is also pointed at, as it would be pointless for a male to be fast to assess female fertility if he needed to assess the risk of there being a jealous male mate and in that case his chances of defeating him before mating anyway (pointlessness of assessing one necessary condition faster than another necessary condition can possibly be assessed).[158][159]
Evolutionary psychology has been entangled in the larger philosophical and social science controversies related to the debate on nature versus nurture. Evolutionary psychologists typically contrast evolutionary psychology with what they call the standard social science model (SSSM). They characterize the SSSM as the "blank slate", "relativist", "social constructionist", and "cultural determinist" perspective that they say dominated the social sciences throughout the 20th century and assumed that the mind was shaped almost entirely by culture.[156]
Critics have argued that evolutionary psychologists created a false dichotomy between their own view and the caricature of the SSSM.[160][161][162] Other critics regard the SSSM as a rhetorical device or a straw man[157][160][163] and suggest that the scientists whom evolutionary psychologists associate with the SSSM did not believe that the mind was a blank state devoid of any natural predispositions.[157]
Some critics view evolutionary psychology as a form of genetic reductionism and genetic determinism,[164][165] a common critique being that evolutionary psychology does not address the complexity of individual development and experience and fails to explain the influence of genes on behavior in individual cases.[44] Evolutionary psychologists respond that they are working within a nature-nurture interactionist framework that acknowledges that many psychological adaptations are facultative (sensitive to environmental variations during individual development). The discipline is generally not focused on proximate analyses of behavior, but rather its focus is on the study of distal/ultimate causality (the evolution of psychological adaptations). The field of behavioral genetics is focused on the study of the proximate influence of genes on behavior.[166]
A frequent critique of the discipline is that the hypotheses of evolutionary psychology are frequently arbitrary and difficult or impossible to adequately test, thus questioning its status as an actual scientific discipline, for example because many current traits probably evolved to serve different functions than they do now.[9][167] Thus because there are a potentially infinite number of alternative explanations for why a trait evolved, critics contend that it is impossible to determine the exact explanation.[168] While evolutionary psychology hypotheses are difficult to test, evolutionary psychologists assert that it is not impossible.[169] Part of the critique of the scientific base of evolutionary psychology includes a critique of the concept of the Environment of Evolutionary Adaptation (EEA). Some critics have argued that researchers know so little about the environment in which Homo sapiens evolved that explaining specific traits as an adaption to that environment becomes highly speculative.[170] Evolutionary psychologists respond that they do know many things about this environment, including the facts that present day humans' ancestors were hunter-gatherers, that they generally lived in small tribes, etc.[171] Edward Hagen argues that the human past environments were not radically different in the same sense as the Carboniferous or Jurassic periods and that the animal and plant taxa of the era were similar to those of the modern world, as was the geology and ecology. Hagen argues that few would deny that other organs evolved in the EEA (for example, lungs evolving in an oxygen rich atmosphere) yet critics question whether or not the brain's EEA is truly knowable, which he argues constitutes selective scepticism. Hagen also argues that most evolutionary psychology research is based on the fact that females can get pregnant and males cannot, which Hagen observes was also true in the EEA.[172][173]
John Alcock describes this as the "No Time Machine Argument", as critics are arguing that since it is not possible to travel back in time to the EEA, then it cannot be determined what was going on there and thus what was adaptive. Alcock argues that present-day evidence allows researchers to be reasonably confident about the conditions of the EEA and that the fact that so many human behaviours are adaptive in the current environment is evidence that the ancestral environment of humans had much in common with the present one, as these behaviours would have evolved in the ancestral environment. Thus Alcock concludes that researchers can make predictions on the adaptive value of traits.[174] Similarly, Dominic Murphy argues that alternative explanations cannot just be forwarded but instead need their own evidence and predictions - if one explanation makes predictions that the others cannot, it is reasonable to have confidence in that explanation. In addition, Murphy argues that other historical sciences also make predictions about modern phenomena to come up with explanations about past phenomena, for example, cosmologists look for evidence for what we would expect to see in the modern-day if the Big Bang was true, while geologists make predictions about modern phenomena to determine if an asteroid wiped out the dinosaurs. Murphy argues that if other historical disciplines can conduct tests without a time machine, then the onus is on the critics to show why evolutionary psychology is untestable if other historical disciplines are not, as "methods should be judged across the board, not singled out for ridicule in one context."[168]
Evolutionary psychologists generally presume that, like the body, the mind is made up of many evolved modular adaptations,[175] although there is some disagreement within the discipline regarding the degree of general plasticity, or "generality," of some modules.[166] It has been suggested that modularity evolves because, compared to non-modular networks, it would have conferred an advantage in terms of fitness[176] and because connection costs are lower.[177]
In contrast, some academics argue that it is unnecessary to posit the existence of highly domain specific modules, and, suggest that the neural anatomy of the brain supports a model based on more domain general faculties and processes.[178][179] Moreover, empirical support for the domain-specific theory stems almost entirely from performance on variations of the Wason selection task which is extremely limited in scope as it only tests one subtype of deductive reasoning.[180][181]
Psychologist Cecilia Heyes has argued that the picture presented by some evolutionary psychology of the human mind as a collection of cognitive instincts – organs of thought shaped by genetic evolution over very long time periods[182][19] – does not fit research results. She posits instead that humans have cognitive gadgets – "special-purpose organs of thought" built in the course of development through social interaction. Similar criticisms are articulated by Subrena E. Smith of the University of New Hampshire.[183][184][185]
Evolutionary psychologists have addressed many of their critics (e.g. in books by Segerstråle (2000),[186] Barkow (2005),[187][188] and Alcock (2001)[189]). Among their rebuttals are that some criticisms are straw men, or are based on an incorrect nature versus nurture dichotomy or on basic misunderstandings of the discipline.[166][190][191][192][193][194][195]
Robert Kurzban suggested that "...critics of the field, when they err, are not slightly missing the mark. Their confusion is deep and profound. It's not like they are marksmen who can't quite hit the center of the target; they're holding the gun backwards."[196] Many have written specifically to correct basic misconceptions.[5][197][156][198]

---

# Chimpanzee–human last common ancestor

The chimpanzee–human last common ancestor (CHLCA) is the last common ancestor shared by the extant Homo (human) and Pan (chimpanzee and bonobo) genera of Hominini. Estimates of the divergence date vary widely from thirteen to five million years ago.
In human genetic studies, the CHLCA is useful as an anchor point for calculating single-nucleotide polymorphism (SNP) rates in human populations where chimpanzees are used as an outgroup, that is, as the extant species most genetically similar to Homo sapiens.
Despite extensive research, no direct fossil evidence of the CHLCA has been discovered. Fossil candidates like Sahelanthropus tchadensis, Orrorin tugenensis, and Ardipithecus ramidus have been debated as either being early hominins or close to the CHLCA. However, their classification remains uncertain due to incomplete evidence
The taxon tribe Hominini was proposed to separate humans (genus Homo) from chimpanzees (Pan) and gorillas (genus Gorilla) on the notion that the least similar species should be separated from the other two. However, later evidence revealed that Pan and Homo are closer genetically than are Pan and Gorilla; thus, Pan was referred to the tribe Hominini with Homo. Gorilla now became the separated genus and was referred to the new taxon 'tribe Gorillini'.
Mann and Weiss (1996), proposed that the tribe Hominini should encompass Pan and Homo, grouped in separate subtribes.[1] They classified Homo and all bipedal apes in the subtribe Hominina and Pan in the subtribe Panina. (Wood (2010) discussed the different views of this taxonomy.)[2] A "chimpanzee clade" was posited by Wood and Richmond, who referred it to a tribe Panini, which was envisioned from the family Hominidae being composed of a trifurcation of subfamilies.[3]
Richard Wrangham (2001) argued that the CHLCA species was very similar to the common chimpanzee (Pan troglodytes) – so much so that it should be classified as a member of the genus Pan and be given the taxonomic name Pan prior.[4]
All the human-related genera of tribe Hominini that arose after divergence from Pan are members of the subtribe Hominina, including the genera Homo and Australopithecus. This group represents "the human clade" and its members are called "hominins".[5]
No fossil has yet conclusively been identified as the CHLCA.
Sahelanthropus tchadensis is an extinct hominine with some morphology proposed (and disputed) to be as expected of the CHLCA, and it lived some 7 million years ago – close to the time of the chimpanzee–human divergence. But it is unclear whether it should be classified as a member of the tribe Hominini, that is, a hominin, as an ancestor of Homo and Pan and a potential candidate for the CHLCA species itself, or simply a Miocene ape with some convergent anatomical similarity to many later hominins.
Ardipithecus most likely appeared after the human-chimpanzee split, some 5.5 million years ago, at a time when gene flow may still have been ongoing. It has several shared characteristics with chimpanzees, but due to its fossil incompleteness and the proximity to the human-chimpanzee split, the exact position of Ardipithecus in the fossil record is unclear.[6] It is most likely derived from the chimpanzee lineage and thus not ancestral to humans.[7][8] However, Sarmiento (2010), noting that Ardipithecus does not share any characteristics exclusive to humans and some of its characteristics (those in the wrist and basicranium), suggested that it may have diverged from the common human/African ape stock prior to the human, chimpanzee and gorilla divergence.[9]
Another candidate that has been suggested is Graecopithecus, though this claim is disputed as there is insufficient evidence to support the determination of Graecopithecus as hominin.[10] This would put the CHLCA split in Southeast Europe instead of Africa.[11][12]
The earliest fossils clearly in the human but not the chimpanzee lineage appear between about 4.5 to 4 million years ago, with Australopithecus anamensis.
Few fossil specimens on the "chimpanzee-side" of the split have been found; the first fossil chimpanzee, dating between 545 and 284 kyr (thousand years, radiometric), was discovered in Kenya's East African Rift Valley (McBrearty, 2005).[13] All extinct genera listed in the taxobox[which?] are ancestral to Homo, or are offshoots of such. However, both Orrorin and Sahelanthropus existed around the time of the divergence, and so either one or both may be ancestral to both genera Homo and Pan.
Due to the scarcity of fossil evidence for CHLCA candidates, Mounier (2016) presented a project to create a "virtual fossil" by applying digital "morphometrics" and statistical algorithms to fossils from across the evolutionary history of both Homo and Pan, having previously used this technique to visualize a skull of the last common ancestor of Neanderthal and Homo sapiens.[14][15]
An estimate of 10 to 13 million years for the CHLCA was proposed in 1998,[16] and a range of 7 to 10 million years ago is assumed by White and colleagues in 2009.[17] A 2016 study analyzed transitions at CpG sites in genome sequences, which exhibit a more clocklike behavior than other substitutions, arriving at an estimate for human and chimpanzee divergence time of 12.1 million years.[18] Studies in the early 2020s suggest a more recent CHLCA, such as between 9.3 and 6.5 million years ago in 2021,[19] and studies giving even more recent dates are cited in a 2022 article.[20]
A source of confusion in determining the exact age of the Pan–Homo split is evidence of a more complex speciation process than a clean split between the two lineages. Different chromosomes appear to have split at different times, possibly over as much as a 4-million-year period, indicating a long and drawn out speciation process with large-scale gene flow events between the two emerging lineages as recently as 6.3 to 5.4 million years ago, according to Patterson et al. (2006).[21]
Speciation between Pan and Homo occurred over the last 9 million years. Ardipithecus probably branched off of the Pan lineage in the middle Miocene Messinian.[7][8] After the original divergences, there were, according to Patterson (2006), periods of gene flow between population groups and a process of alternating divergence and gene flow that lasted several million years.[21] Some time during the late Miocene or early Pliocene, the earliest members of the human clade completed a final separation from the lineage of Pan – with date estimates ranging from 13 million[16] to as recent as 4 million years ago.[21] The latter date was in particular based on the similarity of the X chromosome in humans and chimpanzees, a conclusion rejected as unwarranted by Wakeley (2008), who suggested alternative explanations, including selection pressure on the X chromosome in the populations ancestral to the CHLCA.[note 1]
Complex speciation and incomplete lineage sorting of genetic sequences seem to also have happened in the split between the human lineage and that of the gorilla, indicating "messy" speciation is the rule rather than the exception in large primates.[23][24] Such a scenario would explain why the divergence age between the Homo and Pan has varied with the chosen method and why a single point has so far been hard to track down.

---

# Gorilla–human last common ancestor

The gorilla–human last common ancestor (GHLCA, GLCA, or G/H LCA) is the last species that the tribes Hominini and Gorillini (i.e. the chimpanzee–human last common ancestor on one hand and gorillas on the other) share as a common ancestor. It is estimated to have lived 8 to 10 million years ago (TGHLCA) during the late Miocene.[1][2][3][4]
The fossil find of Nakalipithecus nakayamai are closest in age to the GHLCA.[3][4]
The GHLCA marks a pivotal evolutionary split within the Homininae subfamily, separating the lineage that led to gorillas (Gorilla gorilla and Gorilla beringei) from the lineage that eventually gave rise to chimpanzees, bonobos and humans.
This ancestor is part of the larger African ape lineage, which also includes the chimpanzee—human last common ancestor (Pan and Homo genera).
The divergence of the gorilla lineage likely coincided with significant environmental changes, such as the shrinking of tropical forests during the Miocene. Based on genomic analysis, this ancestor lived around 10 million years ago. [2]
This prehistoric primate-related article is a stub. You can help Wikipedia by expanding it.

---

# Orangutan–human last common ancestor

The phylogenetic split of Hominidae into the subfamilies Homininae and Ponginae is dated to the middle Miocene, roughly 18 to 14 million years ago. This split is also referenced as the "orangutan–human last common ancestor" by Jeffrey H. Schwartz, professor of anthropology at the University of Pittsburgh School of Arts and Sciences, and John Grehan, director of science at the Buffalo Museum.
Hominoidea (commonly known as apes) are thought to have evolved in Africa by about 18 million years ago. Among the genera thought to be in the ape lineage leading up to the emergence of the great apes (Hominidae) about 13 million years ago are Proconsul, Rangwapithecus, Dendropithecus, Nacholapithecus, Equatorius, Afropithecus and Kenyapithecus, all from East Africa. During the early Miocene, Europe and Africa were connected by land bridges over the Tethys Sea. Apes showed up in Europe in the fossil record beginning 17 million years ago. Great apes show up in the fossil record in Europe and Asia beginning about 12 million years ago.[1] Apart from humans, the only living great ape in Asia is the orangutan.[2][3][4][5][6][7][8]
Various genera of dryopithecines have been identified and are classified as an extinct sister clade of the Homininae.[9]: 226 Possible further members of this tribe, indicated within Ponginae in the cladistic tree above, or of Homininae or else third or more tribes yet unnamed include extinct Pierolapithecus, Hispanopithecus, Lufengpithecus and Khoratpithecus.[10]
Dryopithecines' nominate genus Dryopithecus was first uncovered in France, and it had a large frontal sinus which ties it to the African great apes.[2] Orangutans which are only found in Asia do not.[2] They did have thick dental enamel, another ape-like characteristic.[2] Orangutans do not have a large frontal sinus.[11] The study of Dryopithecini as an outgroup to Hominidae suggests a date earlier than 8 million years ago for the Homininae-Ponginae split. It also suggests that the Homininae group evolved in Africa or Western Eurasia, against the theory that Homininae was originally an Asian line which later recolonized Africa after the Great Apes went extinct in Africa.[12]
During the later Miocene, the climate in Europe started to change as the Himalayas were rising, and Europe became cooler and drier. About 9.5 million years ago, tropical forest in Europe was replaced by woodlands which were less suitable for great apes, and European Homininae (close to the Dryopithecini-Hominini split) appear to have migrated back to Africa where they would have diverged into Gorillini and Hominini.[2]
Plant fossils reveal that forests use to once extend "from southern Europe, through Central Asia, and into China prior to the formation of the Himalayas". This suggests that the ancestral hominoid once lived throughout a vast area and as the Earth's climate and ecosystems changed, the ancestral hominoids ultimately became geographically isolated from one another.[13]
The orangutan–human last common ancestor was tailless and had a broad flat rib cage, a larger body size, larger brain, and in females, the canine teeth had started to shrink like their descendants.[9]: 201 Great apes have sweat glands in the armpits versus in the chest like lesser monkeys.[9]: 195
Orangutans have anterior lingual glands and sparse terminal hair like the hominines.[9]: 193
Terminal hairs are those hairs that are easy to see. As compared to the tiny light-colored hairs called Vellus hairs. Certainly, there is some correlation with the size of the mammal. The larger the mammal, the fewer terminal hairs.[9]: 195
Near the tip of the tongue on the underside are the submandibular glands, which secrete amylase. These are only found in the great apes so it is understood that the last common ancestor would also have these.[9]: 195
Concerning the skull, this is an example where humans share more in common with orangutans than with later great apes. We have two small ridges, one over each eye called superciliary arches.[9]: 229
The great apes other than humans and orangutans have brow ridges.[9]: 229
The orangutan genome has many unique features. Structural evolution of the orangutan genome has proceeded much more slowly than other great apes, evidenced by fewer rearrangements, less segmental duplication, and a lower rate of gene family turnover. Diversity among the orangutan populations may not be maintained with continued habitat loss and population fragmentation.[14] Evolutionary evidence from other species suggests fragmentation will not halt diversity, but their slow reproduction rate and arboreal lifestyle may leave the orangutan species especially vulnerable to rapid dramatic environmental change.[14]
Orangutans represent an older lineage of great apes dating from 12–16 million years ago. Though orangutans are the most phylogenetically distant great apes from humans, they nonetheless share significant similarities: equally large brains, high intelligence, slow lives, hunting, meat-eating, reliance on technology, culture, and language capacity. Experts often argue that orangutans resemble humans the most closely, showing greater bipedalism, subtle intellectual advantages, and the longest childhood growth and period of dependency.[15]

---

# Gibbon–human last common ancestor

The gibbon–human last common ancestor is the last common ancestor of the superfamily Hominoidea (apes), dating to the split of the Hylobatidae (gibbons) and Hominidae (great apes) families. It is dated to the early Miocene, roughly 20 to 16 million years ago.[1]
Hylobatidae has four gibbon genera (Hylobates with 9 species, Hoolock with 3 species, Nomascus with 7 species and Symphalangus with only 1 species) [1][2] containing 20 different species. Hominidae has two subfamilies, Ponginae (orangutans) and Homininae (African apes, including the human lineage).
A 2014 whole-genome molecular dating analysis indicated that the gibbon lineage diverged from that of great apes (Hominidae) around 17 million years ago (16.8±0.9 Mya), based on certain assumptions about the generation time and mutation rate.[1]
The extinct Bunopithecus sericus was a gibbon or gibbon-like ape.[3] Adaptive divergence associated with chromosomal rearrangements led to rapid radiation of the four genera within the Hylobatidae lineage between about 7 to 5 Mya. Each genus comprises a distinct, well-delineated lineage, but the sequence and timing of divergences among these genera have been hard to resolve due to radiative speciations and extensive incomplete lineage sorting.[1][2] Recent coalescent-based analysis of both the coding and noncoding parts of the genome suggests that the most likely sequence of species divergences in the Hylobatidae lineage is (Hylobates, (Nomascus, (Hoolock, Symphalangus))).[2] Though other studies have also found different topology. [4]
Because fossils are so scarce, it is not clear what GHLCA looked like. A 2019 study found that the species was "smaller than previously thought" and about the size of a gibbon.[5]
It is unknown whether GHLCA was tailless and had a broad, flat rib cage like their descendants.[6]: 193 But it is likely that it was a small animal, probably weighing only 12 kilograms (26 lb). This contradicts previous theories that they were the size of chimpanzees and that apes moved to hang and to swing from trees to get off the ground because they were too big. There might have been an arms race in brachiating to reach the best food. Also, the Hominidae, which came later, were smaller than their ancestors, which is contrary to normal evolution where animals get larger over their evolutionary development.[5]

---

# Gathering hypothesis

The gathering hypothesis is a term in evolutionary psychology coined in 1970s feminism[1][2] as the antithesis of "hunting hypothesis", suggesting that gathering rather than hunting was the main factor in the emergence of anatomically modern humans.
David Buss argues that tools were not used for hunting initially, but instead to dig up and gather plants. It is possible the invention of tools explains the transition from a forest habitat to the savanna woodlands and grasslands. Tools made gathering food easier and more economical enabling ancestral humans to live in a sparser environment. It was not until the invention of receptacles to store food that more elaborate tools used to hunt, skin, and butcher were developed. According to the gathering hypothesis hunting had no major role in the evolution of modern humans.[3]
One of the sources of evidence for the gathering hypothesis is in the alleged “Superior Spatial Memory of Women.”[clarification needed] McBurney et al. found that women perform better on memory tasks than males whereas men perform better on rotation tasks.[4] Hawkes furthers the argument for the gathering hypothesis in that women obtain larger fitness benefits by tending to their offspring, thus they provision, because it is simpler to coordinate gathering and offspring care.[5] The "Superior Spatial Memory of Women" has been supported through more recent studies by Neave et al. in which females were both quicker and made few mistakes in gathering and identifying specific plants than their male counterparts.[6]
However, the gathering hypothesis cannot account for sexual division of labor in men hunting and women gathering, high parental investment of human males, male coalition psychology, why humans live in environments without plant resources, human gut structure versus primate gut structure, reciprocal alliances (alliance theory), and why women share food.[citation needed]

---

# Endurance running hypothesis

The endurance running hypothesis is a series of conjectures which presume humans evolved anatomical and physiological adaptations to run long distances[1][2][3] and, more strongly, that "running is the only known behavior that would account for the different body plans in Homo as opposed to apes or australopithecines".[4]
The hypothesis posits a significant role of endurance running in facilitating early hominins' ability to obtain meat. Proponents of this hypothesis assert that endurance running served as a means for hominins to effectively engage in persistence hunting and carcass poaching, thus enhancing their competitive edge in acquiring prey. Consequently, these evolutionary pressures have led to the prominence of endurance running as a primary factor shaping many biomechanical characteristics of modern humans.
No primates other than humans are capable of endurance running, and in fact, Australopithecus did not have structural adaptations for running.[5][6] Instead, forensic anthropology suggests that anatomical features that directly contributed to endurance running capabilities were heavily selected for within the genus Homo dating back to 1.9Ma. Consequently, selecting anatomical features that made endurance running possible radically transformed the hominid body.[7] The general form of human locomotion is markedly distinct from all other animals observed in nature. The unique manner of human locomotion has been described in the Journal of Anatomy:
"… no animal walks or runs as we do. We keep the trunk erect; in walking, our knees are almost straight at mid-stance; the forces our feet exert on the ground are very markedly two-peaked when we walk fast; and in walking and usually in running, we strike the ground initially with the heel alone. No animal walks or runs like that."[8]
More recent research has shown that so-called heelstrike, the tendency of runners to channel all of their weight through the heel as the leading foot touches the ground, is not universal. This may be an artefact of more comfortable shoes, those specifically designed for running.[9] Runners who have only ever gone barefoot tend to land on the front of the foot, on the heads of the fourth and fifth metatarsal bones.[10] When asked to run on a forceplace that records the degree of pressure experienced by the foot over the course of a stride barefoot runners display a markedly flatter and less intense force curve, indicating a reduced impact on the bones of their feet. Researchers whose work has elucidated this detail, such as Harvard's Daniel Lieberman, conclude that this was likely the stride of humanity's earliest upright ancestors. Other authors hold that this "barefoot stride" is both less injurious and more efficient than the "shod stride" typical of runners who wear specialized shoes. Their case rests on the traditional grounds that underlie all arguments from human evolution: namely that millions of years of natural selection have optimized the human body for one mode of locomotion and that modern attempts to surpass this through technological interventions, such as engineered running shoes, cannot compete with human anatomy as delivered by evolution.
From the perspective of natural selection, scientists acknowledge that specialization in endurance running would not have helped early humans avoid faster predators over short distances.[11] Instead, it could have allowed them to traverse shifting habitat zones more effectively in the African savannas during the Pliocene. Endurance running facilitated the timely scavenging of large animal carcasses and enabled the tracking and chasing of prey over long distances. This tactic of exhausting prey was especially advantageous for capturing large quadrupedal mammals struggling to thermoregulate in hot weather and over extended distances. Conversely, humans possess efficient means to dissipate heat, primarily through sweating. Specifically, evaporative heat dissipation from the scalp and face prevents hyperthermia and heat-induced encephalitis by extreme cardiovascular loads.[12] Furthermore, as humans continued to develop, their posture became more upright and subsequently increased vertically with the elongation of limbs and torso, effectively increasing surface area for corporeal heat dissipation.[13]
In work exploring the evolution of the human head, paleontologist Daniel Lieberman suggests that certain adaptations to the Homo skull and neck are correlational evidence of traits selective to endurance running optimization. Specifically, he posits that adaptations such as a flattening face and the development of the nuchal ligament promote improved head balance for cranial stabilization during extended periods of running.[14]
Compared to Australopithecus fossil skeletons, selection for walking by itself would not develop some of these proposed "endurance running" derived traits:
The derived longer hindlimb was already present in Australopithecus along with evidence for foot bones with a stiff arch. Walking and running in Australopithecus may have been the same as early Homo. Research on the former has shown running to be slower and less efficient than in modern humans.[15] Small changes in joint morphology may indicate neutral evolutionary processes rather than selection.[16]
The methodology by which the proposed derived traits were chosen and evaluated does not seem to have been stated, and there were immediate highly technical arguments "dismissing their validity and terming them either trivial or incorrect."[17]
Most of those proposed traits have not been tested for their effect on walking and running efficiency. [16] The new trunk shape counter-rotations, which help control rotations induced by hip-joint motion, seem active during walking.[18][19] Elastic energy storage does occur in the plantar soft tissue of the foot during walking.[18] Relative lower-limb length has a slightly larger effect on the economy of walking than running.[19] The heel-down foot posture makes walking economical but does not benefit running.[20]
Model-based analysis showing that scavengers would reach a carcass within 30 minutes of detection suggests that "endurance running" would not have given earlier access to carcasses and so not result in selection for "endurance running". Earlier access to carcasses may have been selected for running short distances of 5 km or less, with adaptations that generally improved running performance.[21]
The discovery of more fossil evidence resulted in additional detailed descriptions of hindlimb bones with measurable data reported in the literature. From a study of those reports, hindlimb proposed traits were already present in Australopithecus or early Homo. Those hindlimb characteristics most likely evolved to improve walking efficiency with improved running as a by-product.[22]
Gluteus maximus activity was substantially higher in maximal effort jumping and punching than sprinting, and substantially higher in sprinting than in running at speeds that can be sustained. The activity levels are not consistent with the suggestion that the muscle size is a result of selection for sustained endurance running.[23] Additionally, gluteus maximus activity was much greater in sprinting than in running, similar in climbing and running, and greater in running than walking. Increased muscle activity seems related to the speed and intensity of the movement rather than the gait itself. The data suggests that the large size of the gluteus maximus reflects multiple roles during rapid and powerful movements rather than a specific adaptation to submaximal endurance running.[24]

---

# Aquatic ape hypothesis

The aquatic ape hypothesis (AAH), also referred to as aquatic ape theory (AAT) or the waterside hypothesis of human evolution, postulates that the ancestors of modern humans took a divergent evolutionary pathway from the other great apes by becoming adapted to a more aquatic habitat.[1] While the hypothesis has some popularity with the lay public, it is generally ignored or classified as pseudoscience by anthropologists.[2][3][4]
The theory developed before major discoveries of ancient hominin fossils in East Africa.[5][6] The hypothesis was initially proposed by the English marine biologist Alister Hardy in 1960, who argued that a branch of apes was forced by competition over terrestrial habitats to hunt for food such as shellfish on the coast and seabed, leading to adaptations that explained distinctive characteristics of modern humans such as functional hairlessness and bipedalism.[7] The popular science writer Elaine Morgan supported this hypothesis in her 1972 book The Descent of Woman. In it, she contrasted the theory with zoologist and ethnologist Desmond Morris's theories of sexuality, which she believed to be rooted in sexism.[8]
Anthropologists do not take the hypothesis seriously: John Langdon characterized it as an "umbrella hypothesis" (a hypothesis that tries to explain many separate traits of humans as a result of a single adaptive pressure) that was not consistent with the fossil record, and said that its claim that it was simpler and therefore more likely to be true than traditional explanations of human evolution was not true.[9] According to anthropologist John Hawkes, the AAH is not consistent with the fossil record. Traits that the hypothesis tries to explain evolved at vastly different times, and distributions of soft tissue the hypothesis alleges are unique to humans are common among other primates.[5]
In 1942 the German pathologist Max Westenhöfer (1871–1957) discussed various human characteristics (hairlessness, subcutaneous fat, the regression of the olfactory organ, webbed fingers, direction of the body hair etc.) that could have derived from an aquatic past, quoting several other authors who had made similar speculations. As he did not believe human beings were apes, he believed this might have been during the Cretaceous, contrary to what is possible given the geologic and evolutionary biology evidence available at the time.[10] He stated: "The postulation of an aquatic mode of life during an early stage of human evolution is a tenable hypothesis, for which further inquiry may produce additional supporting evidence."[11] He later abandoned the concept.[12]
Independently of Westenhöfer's writings, the marine biologist Alister Hardy had since 1930 also hypothesized that humans may have had ancestors more aquatic than previously imagined, although his work, unlike Westenhöfer's, was rooted in the Darwinian consensus. On the advice of his colleagues, Hardy delayed presenting the hypothesis for approximately thirty years.[13][14] After he had become a respected academic and knighted for contributions to marine biology, Hardy finally voiced his thoughts in a speech to the British Sub-Aqua Club in Brighton on 5 March 1960. Several national newspapers reported sensational presentations of Hardy's ideas, which he countered by explaining them more fully in an article in New Scientist on 17 March 1960: "My thesis is that a branch of this primitive ape-stock was forced by competition from life in the trees to feed on the sea-shores and to hunt for food, shellfish, sea-urchins etc., in the shallow waters off the coast."[14]
The idea was generally ignored by the scientific community after the article was published. Some interest was received, notably from the geographer Carl Sauer whose views on the role of the seashore in human evolution[15] "stimulated tremendous progress in the study of coastal and aquatic adaptations" inside marine archaeology.[16] In 1967, the hypothesis was mentioned in The Naked Ape, a popular book by the zoologist Desmond Morris, who reduced Hardy's phrase "more aquatic ape-like ancestors" to the bare "aquatic ape", commenting that "despite its most appealing indirect evidence, the aquatic theory lacks solid support".[17]
While traditional descriptions of 'savage' existence identified three common sources of sustenance: gathering of fruit and nuts, fishing, and hunting,[18] in the 1950s, the anthropologist Raymond Dart focused on hunting and gathering as the likely organizing concept of human society in prehistory,[19] and hunting was the focus of the screenwriter Robert Ardrey's 1961 best-seller African Genesis. Another screenwriter, Elaine Morgan, responded to this focus in her 1972 Descent of Woman, which parodied the conventional picture of "the Tarzanlike figure of the prehominid who came down from the trees, saw a grassland teeming with game, picked up a weapon and became a Mighty Hunter,"[20] and pictured a more peaceful scene of humans by the seashore. She took her lead from a section in Morris's 1967 book which referred to the possibility of an Aquatic Ape period in evolution, his name for the speculation by the biologist Alister Hardy in 1960. When it aroused no reaction in the academic community, she dropped the feminist criticism and wrote a series of books–The Aquatic Ape (1982), The Scars of Evolution (1990), The Descent of the Child (1994), The Aquatic Ape Hypothesis (1997) and The Naked Darwinist (2008)–which explored the issues in more detail. Books published on the topic since then have avoided the contentious term aquatic and used waterside instead.[21][22]
My thesis is that a branch of this primitive ape-stock was forced by competition from life in the trees to feed on the sea-shores and to hunt for food, shell fish, sea-urchins etc., in the shallow waters off the coast. I suppose that they were forced into the water just as we have seen happen in so many other groups of terrestrial animals. I am imagining this happening in the warmer parts of the world, in the tropical seas where Man could stand being in the water for relatively long periods, that is, several hours at a stretch.[7]
Hardy argued a number of features of modern humans are characteristic of aquatic adaptations. He pointed to humans' lack of body hair as being analogous to the same lack seen in whales and hippopotamuses,[23][24] and noted the layer of subcutaneous fat humans have that Hardy believed other apes lacked, although it has been shown that captive apes with ample access to food have levels of subcutaneous fat similar to humans.[25][26] Additional features cited by Hardy include the location of the trachea in the throat rather than the nasal cavity, the human propensity for front-facing copulation, tears and eccrine sweating, though these claimed pieces of evidence have alternative evolutionary adaptationist explanations that do not invoke an aquatic context.[27]
The diving reflex is sometimes cited as evidence. This is exhibited strongly in aquatic mammals, such as seals, otters and dolphins. It also exists as a lesser response in other animals, including human babies up to 6 months old (see infant swimming). However adult humans generally exhibit a mild response.
Hardy additionally posited that bipedalism evolved first as an aid to wading before becoming the usual means of human locomotion,[28][29] and tool use evolved out of the use of rocks to crack open shellfish.[28][25] These last arguments were cited by later proponents of AAH as an inspiration for their research programs.
Morgan summed up her take on the hypothesis in 2011:
Waterside hypotheses of human evolution assert that selection from wading, swimming and diving and procurement of food from aquatic habitats have significantly affected the evolution of the lineage leading to Homo sapiens as distinct from that leading to Pan.[30]
The AAH is generally ignored by anthropologists, although it has a following outside academia and conferences on the topic have received celebrity endorsement, for example from David Attenborough.[2] Despite being debunked, it returns periodically, being promoted as recently as 2019.[3]
Academics who have commented on the aquatic ape hypothesis include categorical opponents (generally members of the community of academic anthropology) who reject almost all of the claims related to the hypothesis. Other academics have argued that the rejection of Hardy and Morgan is partially unfair given that other explanations which suffer from similar problems are not so strongly opposed. A conference devoted to the subject was held at Valkenburg, Netherlands, in 1987.[31] Its 22 participants included academic proponents and opponents of the hypothesis and several neutral observers headed by the anthropologist Vernon Reynolds of the University of Oxford. His summary at the end was:
Overall, it will be clear that I do not think it would be correct to designate our early hominid ancestors as 'aquatic'. But at the same time there does seem to be evidence that not only did they take to water from time to time but that the water (and by this I mean inland lakes and rivers) was a habitat that provided enough extra food to count as an agency for selection.[32]
The AAH is considered to be a classic example of pseudoscience among the scholarly community,[33][34][35] and has been met with significant skepticism.[36] The Nature editor and paleontologist Henry Gee has argued that the hypothesis has equivalent merit to creationism, and should be similarly dismissed.[6]
In a 1997 critique, anthropologist John Langdon considered the AAH under the heading of an "umbrella hypothesis" and argued that the difficulty of ever disproving such a thing meant that although the idea has the appearance of being a parsimonious explanation, it actually was no more powerful an explanation than the null hypothesis that human evolution is not particularly guided by interaction with bodies of water. Langdon argued that however popular the idea was with the public, the "umbrella" nature of the idea means that it cannot serve as a proper scientific hypothesis. Langdon also objected to Morgan's blanket opposition to the "savannah hypothesis" which he took to be the "collective discipline of paleoanthropology". He observed that some anthropologists had regarded the idea as not worth the trouble of a rebuttal. In addition, the evidence cited by AAH proponents mostly concerned developments in soft tissue anatomy and physiology, whilst paleoanthropologists rarely speculated on evolutionary development of anatomy beyond the musculoskeletal system and brain size as revealed in fossils. After a brief description of the issues under 26 different headings, he produced a summary critique of these with mainly negative judgments. His main conclusion was that the AAH was unlikely ever to be disproved on the basis of comparative anatomy, and that the one body of data that could potentially disprove it was the fossil record.[9]
In a blog post originally published in 2005 and continually updated since, anthropologist John D. Hawks said that anthropologists don't accept the AAH for several reasons. Hardy and Morgan situated the alleged aquatic period of human nature in a period of the fossil record that is now known not to contain any aquatic ancestors. The traits the AAH tries to explain actually evolved at wildly different time periods. The AAH claims that the alleged aquatic nature of humanity is responsible for human patterns of hair, fat, and sweat, but actually all of these things are similar in humans to other primates. To the extent they are exceptional in any primate relative to other primates, or in primates relative to other mammals, they are exceptional for well-understood thermodynamic reasons.[5]
Palaeontologist Riley Black concurred with the pseudoscience label, and described the AAH as a "classic case of picking evidence that fits a preconceived conclusion and ignoring everything else".[37] Physical anthropologist Eugenie Scott has described the aquatic ape hypothesis as an instance of "crank anthropology" akin to other pseudoscientific ideas in anthropology such as alien-human interbreeding and Bigfoot.[38]
In The Accidental Species: Misunderstandings of Human Evolution (2013), Henry Gee remarked on how a seafood diet can aid in the development of the human brain. He nevertheless criticized the AAH because "it's always a problem identifying features [such as body fat and hairlessness] that humans have now and inferring that they must have had some adaptive value in the past." Also "it's notoriously hard to infer habits [such as swimming] from anatomical structures".[39]
Popular support for the AAH has become an embarrassment to some anthropologists, who want to explore the effects of water on human evolution without engaging with the AAH, which they consider "emphasizes adaptations to deep water (or at least underwater) conditions". Foley and Lahr suggest that "to flirt with anything watery in paleoanthropology can be misinterpreted", but argue "there is little doubt that throughout our evolution we have made extensive use of terrestrial habitats adjacent to fresh water, since we are, like many other terrestrial mammals, a heavily water-dependent species." But they allege that "under pressure from the mainstream, AAH supporters tended to flee from the core arguments of Hardy and Morgan towards a more generalized emphasis on fishy things."[40]
In "The Waterside Ape", a pair of 2016 BBC Radio documentaries, David Attenborough discussed what he thought was a "move towards mainstream acceptance" for the AAH in the light of new research findings. He interviewed scientists supportive of the idea, including Kathlyn Stewart and Michael Crawford who had published papers in a special issue of the Journal of Human Evolution[41] on "The Role of Freshwater and Marine Resources in the Evolution of the Human Diet, Brain and Behavior".[42] Responding to the documentaries in a newspaper article, paleoanthropologist Alice Roberts criticized Attenborough's promotion of AAH and dismissed the idea as a distraction "from the emerging story of human evolution that is more interesting and complex". She argued that AAH had become "a theory of everything" that is simultaneously "too extravagant and too simple".[43][44]
Philosopher Daniel Dennett, in his discussion of evolutionary philosophy,[45] commented "During the last few years, when I have found myself in the company of distinguished biologists, evolutionary theorists, paleoanthropologists and other experts, I have often asked them to tell me, please, exactly why Elaine Morgan must be wrong about the aquatic theory. I haven't yet had a reply worth mentioning, aside from those who admit, with a twinkle in their eyes, that they have also wondered the same thing." He challenged both Elaine Morgan and the scientific establishment in that "Both sides are indulging in adapt[at]ionist Just So stories". Along the same lines, historian Erika Lorraine Milam noted that independent of Morgan's work, certain standard explanations of human development in paleoanthropology have been roundly criticized for lacking evidence, while being based on sexist assumptions.[46] Anatomy lecturer Bruce Charlton gave Morgan's book Scars of Evolution an enthusiastic review in the British Medical Journal in 1991, calling it "exceptionally well written" and "a good piece of science".[47]
In 1995, paleoanthropologist Phillip Tobias declared that the savannah hypothesis was dead, because the open conditions did not exist when humanity's precursors stood upright and that therefore the conclusions of the Valkenburg conference were no longer valid. Tobias praised Morgan's book Scars of Evolution as a "remarkable book", though he said that he did not agree with all of it.[48][49] Tobias and his student further criticised the orthodox hypothesis by arguing that the coming out of the forest of man's precursors had been an unexamined assumption of evolution since the days of Lamarck, and followed by Darwin, Wallace and Haeckel, well before Raymond Dart used it.[50]
Alister Hardy was astonished and mortified in 1960 when the national Sunday papers carried banner headlines "Oxford professor says man a sea ape", causing problems with his Oxford colleagues.[51] As he later said to his ex-pupil Desmond Morris, "Of course I then had to write an article to refute this saying no this is just a guess, a rough hypothesis, this isn't a proven fact. And of course we're not related to dolphins."[42]
Elaine Morgan's 1972 book Descent of Woman became an international best-seller, a Book of the Month selection in the United States and was translated into ten languages.[52] The book was praised for its feminism but paleoanthropologists were disappointed with its promotions of the AAH.[53] Morgan removed the feminist critique and left her AAH ideas intact, publishing the book as The Aquatic Ape 10 years later, but it did not garner any more positive reaction from scientists.[53]
AAH proponent Algis Kuliukas, performed experiments to measure the comparative energy used when lacking orthograde posture with using fully upright posture. Although it is harder to walk upright with bent knees on land, this difference gradually diminishes as the depth of water increases[54] and is still practical in thigh-high water.[55]
In a critique of the AAH, Henry Gee questioned any link between bipedalism and diet. Gee writes that early humans have been bipedal for 5 million years, but our ancestors' "fondness for seafood" emerged a mere 200,000 years ago.[56]
Evidence supports aquatic food consumption in Homo as early as the Pliocene,[57] but its linkage to brain evolution remains controversial.[58][59] Further, there is no evidence that humans ate fish in significant amounts earlier than tens to hundreds of thousands of years ago.[60] Supporters argue that the avoidance of taphonomic bias is the problem, as most hominin fossils occur in lake-side environments, and the presence of fish remains is therefore not proof of fish consumption.[61] They also claim that the archaeological record of human fishing and coastal settlement is fundamentally deficient due to postglacial sea level rise.[62]
In their 1989 book The Driving Force: Food, Evolution and The Future, Michael Crawford and David Marsh claimed that omega-3 fatty acids were vital for the development of the brain:[63]
A branch of the line of primitive ancestral apes was forced by competition to leave the trees and feed on the seashore. Searching for oysters, mussels, crabs, crayfish and so on they would have spent much of their time in the water and an upright position would have come naturally.
Crawford and Marsh opined that the brain size in aquatic mammals is similar to humans, and that other primates and carnivores lost relative brain capacity.[64] Cunnane, Stewart, Crawford, and colleagues published works arguing a correlation between aquatic diet and human brain evolution in their "shore-based diet scenario",[65][66][67] acknowledging the Hardy/Morgan's thesis as a foundation work of their model.[68] As evidence, they describe health problems in landlocked communities, such as cretinism in the Alps and goitre in parts of Africa due to salt-derived iodine deficiency,[69][70] and state that inland habitats cannot naturally meet human iodide requirements.[71]
Biologists Caroline Pond and Dick Colby were highly critical, saying that the work provided "no significant new information that would be of interest to biologists" and that its style was "speculative, theoretical and in many places so imprecise as to be misleading."[72] British palaeontologist Henry Gee, who remarked on how a seafood diet can aid in the development of the human brain, nevertheless criticized AAH because inferring aquatic behavior from body fat and hairlessness patterns is an unjustifiable leap.[39]
Professor of animal physiology and experienced scuba and freediver Erika Schagatay researches human diving abilities and oxygen stress. She suggests that such abilities are consistent with selective pressure for underwater foraging during human evolution, and discussed other anatomical traits speculated as diving adaptations by Hardy/Morgan.[73] John Langdon suggested that such traits could be enabled by a human developmental plasticity.[74]
Vernix caseosa, the thin skin found on newborn babies, is also found on seals, beavers, and otters, but not on other primates. Furthermore, human placentophagy seems to have disappeared at a very early point in the human species, as it was practiced by no known culture; placentophagy is common in primates, but not among seals or dolphins. Notably, placentophagy is also not seen in camels, which like aquatic mammals commonly consume salty substances.[75]

---

# Sexual selection in humans

The concept of sexual selection was introduced by Charles Darwin as an element of his theory of natural selection.[1] Sexual selection is a biological way one sex chooses a mate for the best reproductive success. Most compete with others of the same sex for the best mate to contribute their genome for future generations. This has shaped human evolution for many years, but reasons why humans choose their mates are not fully understood. Sexual selection is quite different in non-human animals than humans as they feel more of the evolutionary pressures to reproduce and can easily reject a mate.[2] The role of sexual selection in human evolution has not been firmly established although neoteny has been cited as being caused by human sexual selection.[3] It has been suggested that sexual selection played a part in the evolution of the anatomically modern human brain, i.e. the structures responsible for social intelligence underwent positive selection as a sexual ornamentation to be used in courtship rather than for survival itself,[4] and that it has developed in ways outlined by Ronald Fisher in the Fisherian runaway model.[5][6][7][8][9] Fisher also stated that the development of sexual selection was "more favourable" in humans.[10]
Some hypotheses about the evolution of the human brain argue that it is a sexually selected trait, as it would not confer enough fitness in itself relative to its high maintenance costs (a fifth to a quarter of the energy and oxygen consumed by a human).[11] Current consensus about the evolutionary development of the human brain accepts sexual selection as a potential contributing factor but maintains that human intelligence and the ability to store and share cultural knowledge would have likely carried high survival value as well.[12]
Sexual selection's role in human evolution cannot be definitively established, as features may result from an equilibrium among competing selective pressures, some involving sexual selection, others natural selection, and others pleiotropy. Richard Dawkins argued that
Charles Darwin described sexual selection as depending on "the advantage which certain individuals have over others of the same sex and species, solely in respect of reproduction".[14] Darwin noted that sexual selection is of two kinds and concluded that both kinds had operated on humans:[15] "The sexual struggle is of two kinds; in the one it is between the individuals of the same sex, generally the male sex, in order to drive away or kill their rivals, the females remaining passive; whilst in the other, the struggle is likewise between the individuals of the same sex, in order to excite or charm those of the opposite sex, generally the females, which no longer remain passive, but select the more agreeable partners."[16]
Charles Darwin conjectured that the male beard, as well as the hairlessness of humans compared to nearly all other mammals, were results of sexual selection. He reasoned that since the bodies of females are more nearly hairless, the loss of fur was due to sexual selection of females at a remote prehistoric time when males had overwhelming selective power, and that it nonetheless affected males due to genetic correlation between the sexes. He also hypothesized that contrasts in sexual selection acting along with natural selection were significant factors in the geographical differentiation in human appearance of some isolated groups, as he did not believe that natural selection alone provided a satisfactory answer. Although not explicit, his observation that in Khoisan women "the posterior part of the body projects in a most wonderful manner" (known as steatopygia)[17] implies sexual selection for this characteristic. In The Descent of Man, and Selection in Relation to Sex, Darwin viewed many physical traits which vary around the world as being so trivial to survival[18] that he concluded some input from sexual selection was required to account for their presence. He noted that variation in these features among the various peoples of the world meant human mate-choice criteria would also have to be quite different if the focus was similar, and he himself doubted that, citing[19] reports indicating that ideals of beauty did not, in fact, vary in this way around the world.
The effects on the human brain formation during puberty is directly linked to hormones changes. The mismatch timing between biological puberty and age of social maturity in western society has a psychological expectation on children.[20] With puberty, men are generally hairier than women, and Darwin was of the opinion that hairlessness was related to sexual selection; however, several other explanations have been advanced to explain human hairlessness; a leading one being that loss of body hair facilitated sweating.[21] This idea closely relates to that of the suggested need for increased photoprotection and is part of the most-commonly-accepted scientific explanation for the evolution of pigmentary traits.[22]
Sexual dimorphism suggests the presence of sexual selection. The earliest homininae were highly dimorphic and that this tendency lessened over the course of human evolution, suggesting humans have become more monogamous. In contrast, gorillas living in harems exhibit a much stronger sexual dimorphism (see: homininae).[23]
The theory of sexual selection has been used to explain a number of human anatomical features. These include rounded breasts, facial hair, pubic hair and penis size. The breasts of primates are flat, yet are able to produce sufficient milk for feeding their young. The breasts of non-lactating human females are filled with fatty tissue and not milk. Thus it has been suggested the rounded female breasts are signals of fertility.[24] Richard Dawkins has speculated that the loss of the penis bone in humans, when it is present in other primates, may be due to sexual selection by females looking for a clear sign of good health in prospective mates. Since a human erection relies on a hydraulic pumping system, erection failure is a sensitive early warning of certain kinds of physical and mental ill health.[25]
Homo has a thicker penis than the other great apes, though it is on average no longer than the chimpanzee's.[26] It has been suggested the evolution of the human penis towards larger size was the result of female choice rather than sperm competition, which generally favors large testicles.[27] However, penis size may have been subject to natural selection, rather than sexual selection, due to a larger penis' efficiency in displacing the sperm of rival males during sexual intercourse. A model study showed displacement of semen was directly proportional to the depth of pelvic thrusting, as an efficient semen displacement device.[28]
Several factors drive sexual selection in humans.[29][30] Selection preferences are biologically driven,[31][32] that is, by the display of phenotypic traits that can be both consciously and unconsciously evaluated by the opposite sex to determine the health and fertility of a potential mate.[33] This process can be affected, however, by social factors, including in cultures where arranged marriage is practiced, or psychosocial factors, such as valuing certain cultural traits of a mate, including a person's social status, or what is perceived to be an ideal partner in various cultures.[34]
Some of the factors that affect how females select their potential mates for reproduction include voice pitch, facial shape, muscular appearance, and particularly height;[35][36][37][38] with the possibility of a Fisherian runaway in the making.[39]
Several studies suggest that there is a link between hormone levels and partner selection among humans. In a study measuring female attraction to males with varying levels of masculinity, it was established that women had a general masculinity preference for men's voices, and that the preference for masculinity was greater in the fertile phase of the menstrual cycle than in the non-fertile phase.[36] There is further evidence from the same study that in fertile stages of the menstrual cycle, women also had a preference for other masculine traits such as body size, facial shape, and dominant behavior, which are indicators of both fertility and health.[36] This study did not exclude males with feminine traits from being selected, however, as feminine traits in men indicate a higher probability of long-term relationship commitment,[36] and may be one of several evolutionary strategies.[40] Further research also backs up the idea of using phenotypic traits as a means of assessing a potential mate's fitness for reproduction as well as assessing whether a partner has high genetic quality.[41]
One study proposed a link between Human Development Index levels and female preference for male facial appearance.[42] While women from the United Kingdom preferred the faces of men with low cortisol levels, women from Latvia did not discriminate between men with either high or low levels of cortisol.[42] It was concluded that societal-level ecological factors impact the valuation of traits by combinations of sex- and stress-hormones.[42]
A 2020 study reported that women tend to find a man more attractive if the man's previous relationships ended mutually, and less attractive if the man was dumped.[43]
Like their female counterparts, males also use visual information about a potential mate, as well as voice, body shape, and an assortment of other factors in selecting a partner. Research shows that males tend to prefer feminine women's faces and voices as opposed to women with masculine features in these categories.[44] Furthermore, males also evaluate skin coloration, symmetry, and apparent health, as a means by which they select a partner for reproductive purposes.[44] Males are particularly attracted to femininity in women's faces when their testosterone levels are at their highest, and the level of attraction to femininity may fluctuate as hormone levels fluctuate.[45] Studies on men have also been done to show the effects of exogenous testosterone and its effects on attraction to femininity, and the results concluded that throughout several studies, men have shown decreased preference for feminine female faces in the long-term context, when given exogenous testosterone, but this difference did not occur with placebo.[46]
Sexual selection is in essence a process which favors sexual displays for attraction, aggressiveness, dominance, size, and strength, and the ability to exclude competitors by force if necessary, or by using resources to win.[47] Both male and female use voice, face, and other physical characteristics[34] to assess a potential mate's ability to reproduce, as well as their health.[33] Together with visual and chemical signals, these crucial characteristics which are likely to enhance the ability to produce offspring, as well as long-term survival prospects, can be assessed and selections made.[31][48]
Contest competition is form of sexual selection in which mating is obtained by using force or the threat of force to exclude same-sex competitors from mates.[49] Male contest competition favors large body size, which is seen in the sexual dimorphism of human males and females.[50] In all living hominid species, males are more muscular, allowing them to have more strength and power. Human males have 61% more overall muscle mass compared to females.[51] This greater muscle mass allows males to gain greater acceleration, speed, and more powerful striking movements.[52] Compared to females, human males exhibit more same-sex aggression, which peaks in young adulthood.[53][54][55][56]
Male contest competition also often favors threat displays, which allow one competitor to submit without a costly fight.[57] Low vocalization fundamental frequencies (perceived as vocal pitch) increase the perception of threat among human males.[58][59][60] Controlling for body size, lower male fundamental frequency relative to females tends to evolve in polygynous anthropoid primates, where males compete more intensely for mates.[61] Chimpanzees and humans have the greatest sexual dimorphism in fundamental frequency of all hominids.[61] Males are also more likely to engage in physical risks in front of competitors, and males who take more physical risks are perceived as being stronger.[62] Status badges such as facial hair are generally related to men being perceived as more dominant.[49] Facial hair makes the jaw appear more prominent and shows emotions like anger clearly which makes a male appear more threatening.[63][64] Dominance has been associated with increased male mating success.[65][66][67]
Often contest competition produces anatomical weapons such as antlers or large canine teeth; however, hominids lack canine weaponry typical of other primates.[49] Reduced canine size may be due to bipedalism and adaptations of the hand.[68][69] Bipedalism is not a common trait, yet many species like the great apes stand on their hind legs when fighting, which increases power behind blows.[70][49] Hominin hands are adapted for gripping tools or hurling objects like stones.[71][72][73][74] Bipedalism and utilizing handheld objects such as weapons may have aided early hominins in contest competition, reducing sexual selection pressures of maintaining large canine teeth.[68][73][75]
Several other traits in human males may have been selected for contest competition. Males exhibit a more robust face compared to females.[49] This may have provided protection against blows to the face during contest competitions as the areas on the skull that have increased robusticity are parts that are more likely to suffer from injury.[76] Additionally, there are 23% more lefthanded males than females.[77] Although left-handedness is heritable and associated with survival disadvantages, the rarity of left-handedness may have given ancestral males a fighting advantage in competitions keeping this trait in the gene pool via negative frequency-dependent selection.[49][78][79][80] Many combat sports such as boxing have higher-than-chance frequencies of left-handed individuals among the top competitors.[81] Human males are also able to tolerate pain longer than females, especially during competition.[49][82][83] A higher pain tolerance allows for males to remain aggressive during contests along with an increased aerobic capacity.[49] Males have an oxygen capacity rate that is 25–30% higher than females.[84][85] This aerobic capacity increases during puberty when males are sexually maturing and preparing to mate.[49]
Human males engage in both within-group contest competition and coalitional aggression.[49] The latter form competition may be supported by males tending to contribute more to a group task when competing against other groups and to discriminate more strongly against outgroup members.[86][87][88][89]
Traits that evolve during contest competition, such as large body size and physical aggression, are often costly to produce and maintain.[90] These traits may therefore be indicators of male genetic quality and/or ability to provide resources and other direct benefits.[90] Consequently, human females may evolve preferences for these traits, which then comprise an additional selection pressure. However, secondary sexual characteristics in human males do not always enhance overall attractiveness to females.[91][92][93] Some traits of human males that function in contests, such as body size, strength, and weaponry usage, may also have been selected to aid in hunting.[49] However, contest competition is observed in all great apes and thus likely preceded hunting as a selective pressure.[49]
Human female mating competition is complex and multifaceted and varies across cultures, societies, and individuals.[94] Females may compete for high-quality mates who possess traits that indicate underlying genetic quality, possibly including physical attractiveness and intelligence,[95] or material resources that can enhance the survival and reproductive success of the female and her offspring.[96][95]
Females may also compete for leadership and reputation in social alliances and networks that can provide support, protection, and mating opportunities.[97][98] Human females compete with other females, sometimes including co-wives, to obtain and retain investment from mates, while managing cooperative same-sex relationships.[99]
Over human evolution, the cost of aggressive and physical contests in females may have been high given that females were the primary caregivers and protectors of offspring, so a mother's death greatly impacts infant mortality.[97][98] Some behaviors from mothers competing with other females at a similar life stage over resources include self-promotion and competitor derogation.[98] However, maternal competition remains understudied. Compared to male aggression, female aggression tends to be more indirect. Females tend to engage in more subtle and indirect aggression, such as gossip, as a competitive tool to harm same-sex rivals' social opportunities[100] and partake in competitor derogation to prevent female rivals from getting male attention.[98] Gossip, derogation, and social exclusion grant the aggressor the chance to go undetected and avoid retaliation. Derogation, for example, can eliminate same-sex rivals by reducing their ability to compete; it was found that girls' suicide attempts were associated with any amount of indirect peer victimization, whereas only frequent indirect peer victimization was associated with boys' suicide attempts.[98] Furthermore, same-sex harassment in some nonhuman animals impacted females' ovulation capabilities, which suggests that human females' reproductive success could be influenced by the stress induced by indirect or direct peer victimization.[98] Males pursue both sexually attractive and faithful long-term partners, which might be the source of female mating competition greatly revolving around denigrating same-sex rivals' attractiveness and reputation through accusations of promiscuity and infidelity.[98] Competitive women are more likely to spread reputation-harming information about other women, suggesting that reputation manipulation is a form of female competition for romantic partners.[101] Women are more likely to compete for desirable mates when maternal investment levels are high, and their social groups are largely composed of mothers,[102] as more women living closer together are looking for similar resources that benefit their own survival and that of their children.[98]
Competition for mates among human females may take multiple forms. Contests tend to be less frequent, aggressive, and injurious than male-male contests.[103] This leads to a difference in the traits selected. The indirect aggression in which females engage can take the form of damaging the reputation of other women (e.g., via gossip), potentially influencing their sexual behavior and opportunities.[104] Additionally, females compete with one another through male mate choice, e.g., by enhancing their own physical attractiveness.[104] Some female anatomical traits are targets of male mate choice and possibly represent female sexual ornaments shaped by selection. Femininity in the female face and voice provide cues to female reproductive hormones and reproductive potential.[105] Males tend to have lower pitched voices than females, likely due to male intrasexual competition,[106] but some evidence suggests that high female voice pitch may also be favored by male mate choice and function in intrasexual competition among females.[93]
Deposition of fat on the hips, buttocks, and breasts in human females may also be an outcome of female sexual selection, signaling the ability to support gestation and lactation for offspring in environments where resources may be low.[107][108] However, in the Western World, women with larger breasts are seen as more likely to commit infidelity and more likely to participate in intra-sexual competition with other females.[107] Greater overall body fat percentage in human females appears to be unique among primates and may function in storing resources needed to gestate and support large-brained offspring[109] as well as in sexual selection.[110] For example, higher female body mass index (BMI) is associated with increased fertility in young women, particularly those in subsistence societies.[111] Lower WHR, lower BMI, and smaller waist sizes are also associated with lower birth weights and higher infant mortality.[112] Such traits, particularly body fat distribution, may represent sexual ornamentation, which is important in mating throughout the animal kingdom, for example, in birds.[113][114] Humans also use bodily decoration, including jewelry, tattoos, scarification, and makeup to enhance appearance and desirability to potential mates.[107][115]
It has also been suggested that women who are nearing ovulation were more likely to be judged as more attractive than their counterparts who were in different stages of their cycle.[116] Facial and vocal attractiveness have been observed to change with estradiol and progesterone in pattens consistent with fertility-related increases,[117] although some data challenge this interpretation.[118] In general, ovulatory cycle changes are more subtle than in non-human primates, perhaps representing leakage of information on fertility and hormonal status rather than signals functioning to convey this information.[119]
John Manning[120] suggests that where polygyny is common, there is also a higher disease burden, resulting in selection for antimicrobial resistance. In this view, the antimicrobial properties of melanin help mitigate the susceptibility to disease in sub-Saharan Africa. According to this argument, the anti-infective qualities of melanin were more important than protection from ultraviolet light in the evolution of the darkest skin types. Manning asserts that skin color is more correlated with the occurrence of polygyny – because melanin has an antimicrobial function – than the latitudinal gradient in intensity of ultraviolet radiation.[120][121]
Research seems to contradict Manning's explanation about skin color. The analysis of indigenous populations from more than 50 countries has shown that the strongest correlation with light skin is upper latitude.[122] Rogers et al. (2004) concluded that dark skin evolved as a result of the loss of body hair among the earliest primate ancestors of humans.[123][124][125] and protect from folate depletion due to the increased exposure to sunlight.[126] When humans started to migrate away from the tropics, where there is less-intense sunlight, lighter skin is able to generate more vitamin D than darker skin, so it would have represented a health benefit in reduced sunlight, leading to natural selection for lighter skin.[124][127]
Anthropologist Peter Frost has proposed that sexual selection for women with unusual hair or eye color was responsible for the evolution of pigmentary traits in European populations,[128] however this theory has since been refuted by data-based evidence from genetics and spectrophotometry,[129][130] and multiple studies have shown that women with the facial features and pigmentary characteristics of East Asian women are considered more attractive than European women.[131][132][133]
Geoffrey Miller, drawing on some of Darwin's largely neglected ideas about human behavior, has hypothesized that many human behaviors not clearly tied to survival benefits, such as humor, music, visual art, some forms of altruism, verbal creativity, or the fact that most humans have a far greater vocabulary than that which is required for survival, can nevertheless play a role.[134] Miller (2000) has proposed that this apparent redundancy is due to individuals using vocabulary to demonstrate their intelligence, and consequently their "fitness", to potential mates. This has been tested experimentally, and it appears that males do make greater use of lower-frequency (more unusual) words when in a romantic mindset compared to a non-romantic mindset, suggesting that vocabulary is likely to be used as a sexual display (Rosenberg & Tunney, 2008). All these qualities are considered courtship adaptations that have been favored through sexual selection.[135]
Miller is critical of theories that imply that human culture arose as accidents or by-products of human evolution. He believes that human culture arose through sexual selection for creative traits. In that view, many human artifacts could be considered subject to sexual selection as part of the extended phenotype, for instance clothing that enhances sexually selected traits.[2] During human evolution, on at least two occasions, hominid brain size increased rapidly over a short period of time followed by a period of stasis. The first period of brain expansion occurred 2.5 million years ago, when Homo habilis first began using stone tools. The second period occurred 500,000 years ago, with the emergence of archaic Homo sapiens. Miller argues that the rapid increases in brain size would have occurred by a positive feedback loop resulting in a Fisherian runaway selection for larger brains. Tor Nørretranders, in The Generous Man conjectures how intelligence, musicality, artistic and social skills, and language might have evolved as an example of the handicap principle, analogously with the peacock's tail, the standard example of that principle.
The role of sexual selection in human evolution has been considered controversial from the moment of publication of Darwin's book on sexual selection (1871). Among his vocal critics were some of Darwin's supporters, such as Alfred Wallace, a believer in spiritualism and a non-material origin of the human mind, who argued that animals and birds do not choose mates based on sexual selection, and that the artistic faculties in humans belong to their spiritual nature and therefore cannot be connected to natural selection, which only affects the animal nature.[10] Darwin was accused of looking to the evolution of early human ancestors through the moral codes of the 19th century Victorian society.

---

# Bipedalism#Evolution of human bipedalism

Bipedalism is a form of terrestrial locomotion where an animal moves by means of its two rear (or lower) limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped /ˈbaɪpɛd/, meaning 'two feet' (from Latin bis 'double' and pes 'foot'). Types of bipedal movement include walking or running (a bipedal gait) and hopping.
Several groups of modern species are habitual bipeds whose normal method of locomotion is two-legged. In the Triassic period some groups of archosaurs (a group that includes crocodiles and dinosaurs) developed bipedalism; among the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds are members of a clade of exclusively bipedal dinosaurs, the theropods. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare,[4] hopping mice, pangolins and hominin apes (australopithecines, including humans) as well as various other extinct groups evolving the trait independently. A larger number of modern species intermittently or briefly use a bipedal gait. Several lizard species move bipedally when running, usually to escape from threats.[5] Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment, though there are a few cases where they walk on their hind limbs only. Several arboreal primate species, such as gibbons and indriids, exclusively walk on two legs during the brief periods they spend on the ground. Many animals rear up on their hind legs while fighting or copulating. Some animals commonly stand on their hind legs to reach food, keep watch, threaten a competitor or predator, or pose in courtship, but do not move bipedally.
The word is derived from the Latin words bi(s) 'two' and ped- 'foot', as contrasted with quadruped 'four feet'.
Limited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in the giant pangolin), combat (in bears, great apes and the large monitor lizard) or camouflage.
The maximum bipedal speed appears slower than the maximum speed of quadrupedal movement with a flexible backbone – both the ostrich and the red kangaroo can reach speeds of 70 km/h (43 mph), while the cheetah can exceed 100 km/h (62 mph).[6][7] Even though bipedalism is slower at first, over long distances, it has allowed humans to outrun most other animals according to the endurance running hypothesis.[8] Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, [clarification needed] which could aid in escaping from predators.[9][10]
Zoologists often label behaviors, including bipedalism, as "facultative" (i.e. optional) or "obligate" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut — for example, humans other than infants normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome — very rare genetic neurological disorders rather than normal behavior.[11] Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that "normal" humans can crawl on hands and knees. This article therefore avoids the terms "facultative" and "obligate", and focuses on the range of styles of locomotion normally used by various groups of animals. Normal humans may be considered "obligate" bipeds because the alternatives are very uncomfortable and usually only resorted to when walking is impossible.
There are a number of states of movement commonly associated with bipedalism.
The great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand, most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to walk or hop, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.
Many species of lizards become bipedal during high-speed, sprint locomotion,[5] including the world's fastest lizard, the spiny-tailed iguana (genus Ctenosaura).
The first known biped is the bolosaurid Eudibamus whose fossils date from 290 million years ago.[12][13] Its long hind-legs, short forelegs, and distinctive joints all suggest bipedalism. The species became extinct in the early Permian.
All birds are bipeds, as is the case for all theropod dinosaurs. However, hoatzin chicks have claws on their wings which they use for climbing.
Bipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians.[14] All dinosaurs are thought to be descended from a fully bipedal ancestor, perhaps similar to Eoraptor.
Dinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95 percent of all life on Earth.[15][16] Radiometric dating of fossils from the early dinosaur genus Eoraptor establishes its presence in the fossil record at this time. Paleontologists suspect Eoraptor resembles the common ancestor of all dinosaurs;[17] if this is true, its traits suggest that the first dinosaurs were small, bipedal predators.[18] The discovery of primitive, dinosaur-like ornithodirans such as Marasuchus and Lagerpeton in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.
Bipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodonts. Some extinct members of Pseudosuchia, a sister group to the avemetatarsalians (the group including dinosaurs and relatives), also evolved bipedal forms – a poposauroid from the Triassic, Effigia okeeffeae, is thought to have been bipedal.[19] Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion.
A number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion – for example, humans, ground pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied, are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.[20]
Bipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few non-primate mammals commonly move bipedally with an alternating leg gait. Exceptions are the ground pangolin and in some circumstances the tree kangaroo.[21] One black bear, Pedals, became famous locally and on the internet for having a frequent bipedal gait, although this is attributed to injuries on the bear's front paws. A two-legged fox was filmed in a Derbyshire garden in 2023, most likely having been born that way.[22]
Most bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans), and the tail may be absent entirely. Many primates can stand upright on their hind legs without any support. Chimpanzees, bonobos, gorillas, gibbons[23] and baboons[24] exhibit forms of bipedalism. On the ground sifakas move like all indrids with bipedal sideways hopping movements of the hind legs, holding their forelimbs up for balance.[25] Geladas, although usually quadrupedal, will sometimes move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion.[26] However, they can only do so for brief amounts, as their bodies are not adapted for constant bipedal locomotion.
Humans are the only primates who are normally biped, due to an extra curve in the spine which stabilizes the upright position, as well as shorter arms relative to the legs than is the case for the nonhuman great apes. The evolution of human bipedalism began in primates about four million years ago,[27] or as early as seven million years ago with Sahelanthropus[28][29] or about 12 million years ago with Danuvius guggenmosi. One hypothesis for human bipedalism is that it evolved as a result of differentially successful survival from carrying food to share with group members,[30] although there are alternative hypotheses.
Injured chimpanzees and bonobos have been capable of sustained bipedalism.[31]
Three captive primates, one macaque Natasha[32] and two chimps, Oliver and Poko[33] (chimpanzee), were found to move bipedally. Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage.[34][35] Oliver reverted to knuckle-walking after developing arthritis. Non-human primates often use bipedal locomotion when carrying food, or while moving through shallow water.
Other mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver will move bipedally if transporting wood for their dams, as will the raccoon when holding food). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs (e.g. Faith) can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will walk on its front legs when threatened, rearing up on its front legs while facing the attacker so that its anal glands, capable of spraying an offensive oil, face its attacker.
Bipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the "reared-up" running of lizards such as agamids and monitor lizards.[5] Many reptile species will also temporarily adopt bipedalism while fighting.[36] One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds.[37] Bipedalism is rarely found outside terrestrial animals, though at least two species of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.[38]
There are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools.[39] Bipedal specializations are found in Australopithecus fossils from 4.2 to 3.9 million years ago and recent studies have suggested that obligate bipedal hominid species were present as early as 7 million years ago.[28][40] Nonetheless, the evolution of bipedalism was accompanied by significant evolutions in the spine including the forward movement in position of the foramen magnum, where the spinal cord leaves the cranium.[41] Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as Australopithecus africanus. This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make.[42][43] Adapting bipedalism would have required less shoulder stability, which allowed the shoulder and other limbs to become more independent of each other and adapt for specific suspensory behaviors. In addition to the change in shoulder stability, changing locomotion would have increased the demand for shoulder mobility, which would have propelled the evolution of bipedalism forward.[44] The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.
The form and function of modern-day humans' upper bodies appear to have evolved from living in a more forested setting. Living in this kind of environment would have made it so that being able to travel arboreally would have been advantageous at the time. Although different to human walking, bipedal locomotion in trees was thought to be advantageous.[45] It has also been proposed that, like some modern-day apes, early hominins had undergone a knuckle-walking stage prior to adapting the back limbs for bipedality while retaining forearms capable of grasping.[46] Numerous causes for the evolution of human bipedalism involve freeing the hands for carrying and using tools, sexual dimorphism in provisioning, changes in climate and environment (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun.[47] It is possible that bipedalism provided a variety of benefits to the hominin species, and scientists have suggested multiple reasons for evolution of human bipedalism.[48] There is also not only the question of why the earliest hominins were partially bipedal but also why hominins became more bipedal over time. For example, the postural feeding hypothesis describes how the earliest hominins became bipedal for the benefit of reaching food in trees while the savanna-based theory describes how the late hominins that started to settle on the ground became increasingly bipedal.[49]
Napier (1963) argued that it is unlikely that a single factor drove the evolution of bipedalism. He stated "It seems unlikely that any single factor was responsible for such a dramatic change in behaviour. In addition to the advantages of accruing from ability to carry objects – food or otherwise – the improvement of the visual range and the freeing of the hands for purposes of defence and offence may equally have played their part as catalysts."[50] Sigmon (1971) demonstrated that chimpanzees exhibit bipedalism in different contexts, and one single factor should be used to explain bipedalism: preadaptation for human bipedalism.[51] Day (1986) emphasized three major pressures that drove evolution of bipedalism: food acquisition, predator avoidance, and reproductive success.[52] Ko (2015) stated that there are two main questions regarding bipedalism 1. Why were the earliest hominins partially bipedal? and 2. Why did hominins become more bipedal over time? He argued that these questions can be answered with combination of prominent theories such as Savanna-based, Postural feeding, and Provisioning.[53]
According to the Savanna-based theory, hominines came down from the tree's branches and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. One of the proposed mechanisms was the knuckle-walking hypothesis, which states that human ancestors used quadrupedal locomotion on the savanna, as evidenced by morphological characteristics found in Australopithecus anamensis and Australopithecus afarensis forelimbs, and that it is less parsimonious to assume that knuckle walking developed twice in genera Pan and Gorilla instead of evolving it once as synapomorphy for Pan and Gorilla before losing it in Australopithecus.[54] The evolution of an orthograde posture would have been very helpful on a savanna as it would allow the ability to look over tall grasses in order to watch out for predators, or terrestrially hunt and sneak up on prey.[55] It was also suggested in P. E. Wheeler's "The evolution of bipedality and loss of functional body hair in hominids", that a possible advantage of bipedalism in the savanna was reducing the amount of surface area of the body exposed to the sun, helping regulate body temperature.[56] In fact, Elizabeth Vrba's turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.[57]
Others state hominines had already achieved the bipedal adaptation that was used in the savanna. The fossil evidence reveals that early bipedal hominins were still adapted to climbing trees at the time they were also walking upright.[58] It is possible that bipedalism evolved in the trees, and was later applied to the savanna as a vestigial trait. Humans and orangutans are both unique to a bipedal reactive adaptation when climbing on thin branches, in which they have increased hip and knee extension in relation to the diameter of the branch, which can increase an arboreal feeding range and can be attributed to a convergent evolution of bipedalism evolving in arboreal environments.[59] Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence—the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.[57]
Some of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous Australopithecus afarensis, found in Hadar in Ethiopia, which may have been forested at the time of Lucy's death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. "Little Foot", a nearly-complete specimen of Australopithecus africanus, has a divergent big toe as well as the ankle strength to walk upright. "Little Foot" could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be much more wet and covered in thick vegetation and has only recently become the arid desert it is now.[57]
An alternative explanation is that the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism.[60][61] In an experiment monitoring chimpanzee metabolic rate via oxygen consumption, it was found that the quadrupedal and bipedal energy costs were very similar, implying that this transition in early ape-like ancestors would not have been very difficult or energetically costing.[62] This increased travel efficiency is likely to have been selected for as it assisted foraging across widely dispersed resources.
The postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University.[63] This hypothesis asserts that chimpanzees were only bipedal when they eat. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was used to reach up to grab for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt's hypotheses states that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, Australopithecus afarensis has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the Australopithecus hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.[63]
A related study conducted by University of Birmingham, Professor Susannah Thorpe examined the most arboreal great ape, the orangutan, holding onto supporting branches in order to navigate branches that were too flexible or unstable otherwise. In more than 75 percent of observations, the orangutans used their forelimbs to stabilize themselves while navigating thinner branches. Increased fragmentation of forests where A. afarensis as well as other ancestors of modern humans and other apes resided could have contributed to this increase of bipedalism in order to navigate the diminishing forests. Findings also could shed light on discrepancies observed in the anatomy of A. afarensis, such as the ankle joint, which allowed it to "wobble" and long, highly flexible forelimbs. If bipedalism started from upright navigation in trees, it could explain both increased flexibility in the ankle as well as long forelimbs which grab hold of branches.[64][65][66][67][68][69]
One theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as "male provisioning".[70] Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction ("feminization") of the male canine teeth in early hominids such as Sahelanthropus tchadensis[71] and Ardipithecus ramidus,[72] which along with low body size dimorphism in Ardipithecus[73] and Australopithecus,[74][75][76] suggests a reduction in inter-male antagonistic behavior in early hominids.[77] In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.[77]
However, this model has been debated, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that Australopithecus afarensis males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk.[78] Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother – hampering the mother's freedom[79] and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that Australopithecus afarensis lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a pair-bonded though group living primate.
Recent studies of 4.4 million years old Ardipithecus ramidus suggest bipedalism. It is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. Other recent studies of the foot structure of Ardipithecus ramidus suggest that the species was closely related to African-ape ancestors. This possibly provides a species close to the true connection between fully bipedal hominins and quadruped apes.[80] According to Richard Dawkins in his book "The Ancestor's Tale", chimps and bonobos are descended from Australopithecus gracile type species while gorillas are descended from Paranthropus. These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines from whom eventually evolved hominins. Early hominines such as Ardipithecus ramidus may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas[81] and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.
Joseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects.[82] Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.
There are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski & Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others (e.g. Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins (e.g. 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested (e.g. Tanner 1981:165) that male phallic display could have been the initial incentive, as well as increased sexual signaling in upright female posture.[55]
The thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation.[83][84][85] When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus. A feedback mechanism from the advantages of bipedality in hot and open habitats would then in turn make a forest preadaptation solidify as a permanent state.[86]
Charles Darwin wrote that "Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will". Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat "over considerable distances" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea, as indeed did Lovejoy (1981) with his "provisioning model" described above. Others, such as Nancy Tanner (1981), have suggested that infant carrying was key, while others again have suggested stone tools and weapons drove the change.[87] This stone-tools theory is very unlikely, as though ancient humans were known to hunt, the discovery of tools was not discovered for thousands of years after the origin of bipedalism, chronologically precluding it from being a driving force of evolution. (Wooden tools and spears fossilize poorly and therefore it is difficult to make a judgment about their potential usage.)
The observation that large primates, including especially the great apes, that predominantly move quadrupedally on dry land, tend to switch to bipedal locomotion in waist deep water, has led to the idea that the origin of human bipedalism may have been influenced by waterside environments. This idea, labelled "the wading hypothesis",[88] was originally suggested by the Oxford marine biologist Alister Hardy who said: "It seems to me likely that Man learnt to stand erect first in water and then, as his balance improved, he found he became better equipped for standing up on the shore when he came out, and indeed also for running."[89] It was then promoted by Elaine Morgan, as part of the aquatic ape hypothesis, who cited bipedalism among a cluster of other human traits unique among primates, including voluntary control of breathing, hairlessness and subcutaneous fat.[90] The "aquatic ape hypothesis", as originally formulated, has not been accepted or considered a serious theory within the anthropological scholarly community.[91] Others, however, have sought to promote wading as a factor in the origin of human bipedalism without referring to further ("aquatic ape" related) factors. Since 2000 Carsten Niemitz has published a series of papers and a book[92] on a variant of the wading hypothesis, which he calls the "amphibian generalist theory" (German: Amphibische Generalistentheorie).
Other theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution[93] or critical fallback foods[94]) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism. It has also been thought that consistent water-based food sources had developed early hominid dependency and facilitated dispersal along seas and rivers.[95]
Prehistoric fossil records show that early hominins first developed bipedalism before being followed by an increase in brain size.[96] The consequences of these two changes in particular resulted in painful and difficult labor due to the increased favor of a narrow pelvis for bipedalism being countered by larger heads passing through the constricted birth canal. This phenomenon is commonly known as the obstetrical dilemma.
Non-human primates habitually deliver their young on their own, but the same cannot be said for modern-day humans. Isolated birth appears to be rare and actively avoided cross-culturally, even if birthing methods may differ between said cultures. This is due to the fact that the narrowing of the hips and the change in the pelvic angle caused a discrepancy in the ratio of the size of the head to the birth canal. The result of this is that there is greater difficulty in birthing for hominins in general, let alone to be doing it by oneself.[97]
Bipedal movement occurs in a number of ways and requires many mechanical and neurological adaptations. Some of these are described below.
Energy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.
Shoulder stability would decrease with the evolution of bipedalism. Shoulder mobility would increase because the need for a stable shoulder is only present in arboreal habitats. Shoulder mobility would support suspensory locomotion behaviors which are present in human bipedalism. The forelimbs are freed from weight-bearing requirements, which makes the shoulder a place of evidence for the evolution of bipedalism.[98]
Unlike non-human apes that are able to practice bipedality such as Pan and Gorilla, hominins have the ability to move bipedally without the utilization of a bent-hip-bent-knee (BHBK) gait, which requires the engagement of both the hip and the knee joints. This human ability to walk is made possible by the spinal curvature humans have that non-human apes do not.[99] Rather, walking is characterized by an "inverted pendulum" movement in which the center of gravity vaults over a stiff leg with each step.[100] Force plates can be used to quantify the whole-body kinetic & potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two.[100] This model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.[101]
In humans, walking is composed of several separate processes:[100]
Early hominins underwent post-cranial changes in order to better adapt to bipedality, especially running. One of these changes is having longer hindlimbs proportional to the forelimbs and their effects. As previously mentioned, longer hindlimbs assist in thermoregulation by reducing the total surface area exposed to direct sunlight while simultaneously allowing for more space for cooling winds. Additionally, having longer limbs is more energy-efficient, since longer limbs mean that overall muscle strain is lessened. Better energy efficiency, in turn, means higher endurance, particularly when running long distances.[102]
Running is characterized by a spring-mass movement.[100] Kinetic and potential energy are in phase, and the energy is stored & released from a spring-like limb during foot contact,[100] achieved by the plantar arch and the Achilles tendon in the foot and leg, respectively.[102] Again, the whole-body kinetics are similar to animals with more limbs.[101]
Bipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms. In addition to the leg muscles, the increased size of the gluteus maximus in humans is an important adaptation as it provides support and stability to the trunk and lessens the amount of stress on the joints when running.[102]
Quadrupeds, have more restrictive breathing respire while moving than do bipedal humans.[103] "Quadrupedal species normally synchronize the locomotor and respiratory cycles at a constant ratio of 1:1 (strides per breath) in both the trot and gallop. Human runners differ from quadrupeds in that while running they employ several phase-locked patterns (4:1, 3:1, 2:1, 1:1, 5:2, and 3:2), although a 2:1 coupling ratio appears to be favored. Even though the evolution of bipedal gait has reduced the mechanical constraints on respiration in man, thereby permitting greater flexibility in breathing pattern, it has seemingly not eliminated the need for the synchronization of respiration and body motion during sustained running."[104]
Respiration through bipedality means that there is better breath control in bipeds, which can be associated with brain growth. The modern brain utilizes approximately 20% of energy input gained through breathing and eating, as opposed to species like chimpanzees who use up twice as much energy as humans for the same amount of movement. This excess energy, leading to brain growth, also leads to the development of verbal communication. This is because breath control means that the muscles associated with breathing can be manipulated into creating sounds. This means that the onset of bipedality, leading to more efficient breathing, may be related to the origin of verbal language.[103]
For nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO, MABEL and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot,[105] those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.

---

# Human skeletal changes due to bipedalism

The evolution of human bipedalism, which began in primates approximately four million years ago,[1] or as early as seven million years ago with Sahelanthropus,[2][3] or approximately twelve million years ago with Danuvius guggenmosi, has led to morphological alterations to the human skeleton including changes to the arrangement, shape, and size of the bones of the foot, hip, knee, leg, and the vertebral column. These changes allowed for the upright gait to be overall more energy efficient in comparison to quadrupeds. The evolutionary factors that produced these changes have been the subject of several theories that correspond with environmental changes on a global scale.[4]
Human walking is about 75% less costly than both quadrupedal and bipedal walking in chimpanzees. Some hypotheses have supported that bipedalism increased the energetic efficiency of travel and that this was an important factor in the origin of bipedal locomotion. Humans save more energy than quadrupeds when walking but not when running. Human running is 75% less efficient than walking. A 1980 study reported that walking in living hominin bipeds is noticeably more efficient than walking in living hominin quadrupeds, but the costs of quadrupedal and bipedal travel are the same.[5]
Human feet evolved enlarged heels.[6] The human foot evolved as a platform to support the entire weight of the body, rather than acting as a grasping structure (like hands), as it did in early hominids. Humans therefore have smaller toes than their bipedal ancestors. This includes a non-opposable hallux, which is relocated in line with the other toes.[7] The push off would also require all the toes to be slightly bent up.[8]
Humans have a foot arch rather than being flat footed.[7] When non-human hominids walk upright, weight is transmitted from the heel, along the outside of the foot, and then through the middle toes while a human foot transmits weight from the heel, along the outside of the foot, across the ball of the foot and finally through the big toe. This transference of weight contributes to energy conservation during locomotion.[1][9] The muscles that work along with the hallux has evolved to provide efficient push off. The long arch has also evolved to provide efficient push-off. The stiffening of the arch would be required of an upward gait, all considered that modern bipedalism does not include grasping of tree branches, which also explains the hallux evolving to line up with the rest of the toes.[8]
Human knee joints are enlarged for the same reason as the hip – to better support an increased amount of body weight.[7] The degree of knee extension (the angle between the thigh and shank in a walking cycle) has decreased. The changing pattern of the knee joint angle of humans shows a small extension peak, called the "double knee action," in the midstance phase. Double knee action decreases energy lost by vertical movement of the center of gravity.[1] Humans walk with their knees kept straight and the thighs bent inward so that the knees are almost directly under the body, rather than out to the side, as is the case in ancestral hominids. This type of gait also aids balance.[7]
An increase in leg length since the evolution of bipedalism changed how leg muscles functioned in upright gait. In humans, the push for walking comes from the leg muscles acting at the ankle. A longer leg allows the use of the natural swing of the limb so that, when walking, humans do not need to use muscle to swing the other leg forward for the next step.[7] As a consequence, since the human forelimbs are not needed for locomotion, they are instead optimized for carrying, holding, and manipulating objects with great precision.[10] This results in decreased strength in the forelimbs relative to body size for humans compared to apes.[11]
Having long hind limbs and short forelimbs allows humans to walk upright, while orangutans and gibbons had the adaptation of longer arms to swing on branches.[12] Apes can stand on their hindlimbs, but they cannot do so for long periods of time without getting tired. This is because their femurs are not adapted for bipedalism. Apes have vertical femurs, while humans have femurs that are slightly angled medially from the hip to the knee, thus making human knees closer together and under the body's center of gravity. This adaptation lets humans lock their knees and stand up straight for long periods of time without much effort from muscles.[13] The gluteus maximus became a major role in walking and is one of the largest muscles in humans. This muscle is much smaller in chimps, which shows that it has an important role in bipedalism. When humans run, our upright posture tends to flex forward as each foot strikes the ground creating momentum forward. The gluteus muscle helps to prevent the upper trunk of the body from "pitching forward" or falling over.[14]
Modern human hip joints are larger than in quadrupedal ancestral species to better support the greater amount of body weight passing through them.[7] They also have a shorter, broader shape. This alteration in shape brought the vertebral column closer to the hip joint, providing a stable base for support of the trunk while walking upright.[15] Because bipedal walking requires humans to balance on a relatively unstable ball and socket joint, the placement of the vertebral column closer to the hip joint allows humans to invest less muscular effort in balancing.[7]
Change in the shape of the hip may have led to the decrease in the degree of hip extension, an energy efficient adaptation.[1][14] The ilium changed from a long and narrow shape to a short and broad one and the walls of the pelvis modernized to face laterally. These combined changes provide increased area for the gluteus muscles to attach; this helps to stabilize the torso while standing on one leg. The sacrum has also become more broad, increasing the diameter of the birth canal and making birthing easier. To increase surface for ligament attachment to help support the abdominal viscera during erect posture, the ischial spines became more prominent and shifted towards the middle of the body.[16]
The vertebral column of humans takes a forward bend in the lumbar (lower) region and a backward bend in the thoracic (upper) region. Without the lumbar curve, the vertebral column would always lean forward, a position that requires much more muscular effort for bipedal animals. With a forward bend, humans use less muscular effort to stand and walk upright.[15] Together the lumbar and thoracic curves bring the body's center of gravity directly over the feet.[7] Specifically, the S-shaped curve in the spine brings the center of gravity closer to the hips by bringing the torso back. Balance of the whole vertebral column over the hip joints is a major contribution for efficient bipedalism.[17] The degree of body erection (the angle of body incline to a vertical line in a walking cycle) is significantly smaller[1] to conserve energy.
The Angle of Sacral Incidence was a concept developed by G. Duval-Beaupère and his team at the University of René Descartes. It combines both the pelvic tilt and sacral slope to determine approximately how much lordosis is required for the upright gait to eliminate strain and fatigue on the torso. Lordosis, which the inward curvature of the spine, is normal for an upright gait as long as it is not too excessive or minimal. If the inward curvature of the spine is not enough, the center of balance would be offset causing the body to essentially tip forward, which is why some apes that have the ability to be bipedal require large amounts of energy to stand up. In addition to sacral angles, the sacrum has also evolved to be more flexible in comparison to the stiff sacrum that apes possess.[17]
The human skull is balanced on the vertebral column. The foramen magnum is located inferiorly under the skull, which puts much of the weight of the head behind the spine. The flat human face helps to maintain balance on the occipital condyles. Because of this, the erect position of the head is possible without the prominent supraorbital ridges and the strong muscular attachments found in, for example, apes. As a result, in humans the muscles of the forehead (the occipitofrontalis) are only used for facial expressions.[10]
Increasing brain size has also been significant in human evolution. It began to increase approximately 2.4 million years ago, but modern levels of brain size were not attained until after 500,000 years ago. Zoological analyses have shown that the size of human brains is significantly larger than what anatomists would expect for their size. The human brain is three to four times larger than its closest relative, which is the chimpanzee.[16]
Even with much modification, some features of the human skeleton remain poorly adapted to bipedalism, leading to negative implications prevalent in humans today. The lower back and knee joints are plagued by osteological malfunction, lower back pain being a leading cause of lost working days,[18] because the joints support more weight. Arthritis has been an obstacle since hominids became bipedal: scientists have discovered its traces in the vertebrae of prehistoric hunter-gatherers.[18] Physical constraints have made it difficult to modify the joints for further stability while maintaining efficiency of locomotion.[7]
There have been multiple theories as to why bipedalism was favored, thus leading to skeletal changes that aided the upward gait. The savannah hypothesis describes how the transition from arboreal habits to a savannah lifestyle favored an upright, bipedal gait. This would also change the diet of hominins, more specifically a shift from primarily plant-based to a higher protein, meat-based diet. This would eventually increase the size of the brain, changing the skeletal structure of the skull.[19] Transitions from the forests to the savannah meant that sunlight and heat would require major changes in lifestyle. Being a biped on an open field is also an advantage because of heat dispersal. Walking upright reduces the amount of direct sun exposure and radiation in comparison to being a quadruped which have more body surface on top for the sun to hit.[20] Increased capabilities of postural/locomotor neural control is hypothesis suggesting that the transition from quadrupedal to habitual upright bipedal locomotion was caused by qualitative changes in the nervous system that allowed controlling the more demanding type of posture/locomotion. Only after the more demanding posture was enabled by changes in the nervous system, could advantages of bipedal over quadrupedal locomotion be utilized, including better scanning of the environment, carrying food and infants, simultaneous upper extremity movements and observation of the environment, limitless manipulation of objects with upper extremities, and less space for rotating around the Z-axis.[21]

---

# Muscular evolution in humans

Muscular evolution in humans is an overview of the muscular adaptations made by humans from their early ancestors to the modern man. Humans are believed to be predisposed to develop muscle density as early humans depended on muscle structures to hunt and survive. Modern man's need for muscle is not as dire, but muscle development is still just as rapid if not faster due to new muscle building techniques and knowledge of the human body.[1]
DNA and anthropologic data consider modern humans (Homo sapiens) a primate and the descendants of ape-like species. Species of the genus ‘Homo’ are all extinct except humans, which are thought to have evolved from australopithecine ancestors originating in East Africa.[2] The development of the modern human has taken place over some 300,000 years and unique adaptations have resulted from ecological pressures that Homo Sapiens has faced. Due prominently to ecological and behavioral factors, the modern human muscular system differs greatly from that of our early primate ancestors.[3] These adaptations and changes have allowed Homo sapiens to function as they do today.
As is the standard for all evolutionary adaptations, the human muscle system evolved in its efforts to increase survivability. Since muscles and the accompanying ligaments and tendons are present all throughout the body aiding in many functions, it is apparent that our behavior and decisions are based upon what we are and how we can operate. It is believed that our ancestor's original habitat was not on the ground but in the trees and we developed new habits that eventually allowed us to thrive on the ground, such as changes in diet, gathering of food, energy expenditure, social interactions, and predators. Life in the canopy meant a food supply similar to that of herbivores: leaves, fruits, berries; mostly low-protein foods that did not require a large amount of energy to find. However, if any could be found, meat was also consumed. At this time our ancestors had not yet switched to full-time bipedalism and so searching for food on the ground did not make sense because there was too much energy and risk involved. This habitat also lacked the predators found on the ground that our chimp-like ancestors would have been poor defenders against. As they became bipedal, they began to live in groups that used weapons to fend off predators and hunt down prey. Running became a key aspect to the survival of the species.[4] Even with all this, it is the development of the brain that has guided the development of the muscle functions and structures in humans.
It is suspected that H. sapiens ancestors’ did not initially forage on the forest floor; instead they migrated from the trees for various reasons. In that environment, they survived on a diet high in plant matter with some insects and little amounts of meat. They were not very formidable opponents to more dominant mammals such as large ancient cats (lions, leopards) but their ability to be better hunters and gatherers along with their corresponding brain development, gave them the advantage to add high-calorie nutrient supplies such as meat to their diet. Analysis of the jaws and skull of the supposed human ancestors show that they had larger, stronger jaw muscles attached to the skull which would be expected with a diet rich in fruit and plants. The back set of molars were much larger for this reason also. The dependence on these higher-calorie foods came from the inefficiency of bipedalism and the growing energy costs of climbing tall trees.[5] Human ancestors are thought to have had more muscles connecting the skull, neck, and shoulders/back area (similar to apes) which caused their neck and skull regions to appear to sag, such as non-human primate species do. These diminished muscles allow the human head to be held in its current ‘upright’ position and lets the occipitofrontalis muscle, or the forehead, to function as an aid to expressions.[6]
Humans became taller as the years passed after becoming bipedal which lengthened back muscles at the base of the tail bone and hips which in effect made them weigh more, further hampering their abilities in the trees. Early human ancestors had a tail where modern humans’ tail bone is located. This aided in balance when in the trees but lost its prominence when bipedalism was adapted. The arms also became shorter (opposite in comparison to legs) for carrying objects and using them as multi-tasking agents instead of climbing and swinging in trees. It is well known that the Homo sapiens line of primates developed the opposable thumb which opened the door to many muscle functions not yet possible in the hand and other upper body regions.[7] The stretching muscles of the forearms whose tendons allowed the human to concentrate its force and abilities within his/her hands and fingers contributed to great new abilities.[8] Overall, upper body muscles developed to deal with more activities that involved the concentration of strength in those muscles such as: holding, throwing, lifting, running with something to assist in escaping danger, hunting, and the construction of habitats and shelters.
The conversion to full-time bipedalism in our distant ancestors is the main argument for the adaptations our muscle structure and function have made. By having to center the force of gravity on two feet, the human thigh bone developed an inward slope down to the knee which may have allowed their gluteal abductors to adapt to the stress and build the necessary muscle. This allows the human to manage their balance on a single foot and when “in-stride” during walking. Muscles near the ankle helped provide the push during walking and running. There are many advantages and disadvantages to this altered posture and gait. The ability to grab something with four appendages was lost but what was gained was the ability to hold a club or throw a spear and use the other free hand for another task.[9] This adaptation also helped humans stand up straight with locked knees for longer periods of time.[10] The plantaris muscle in the foot which helped our ancestors grab and manipulate objects like chimps do, has adapted to its new evolutionary role appropriately, becoming so underdeveloped that it cannot grip or grab anything, the foot has grown more elongated as a result and now 9% of humans are born without it. Homo sapiens benefitted by becoming a better defender and hunter. An increase in running as a hunting and survival activity was perhaps fundamental to this development.[11]
Compared to our closest living relatives, chimpanzees and bonobos, Homo sapiens' skeletal muscle is on average about 1.35 to 1.5 times weaker when normalized for size. As little biomechanical difference was found between individual muscle fibers from the different species, this strength difference is likely the result of different muscle fiber type composition. Humans' limb muscles tend to be more biased toward fatigue-resistant, slow twitch Type I muscle fibers.[12] While there is no proof that modern humans have become physically weaker than past generations of humans, inferences from such things as bone robusticity and long bone cortical thickness can be made as a representation of physical strength. Taking such factors into account, there has been a rapid decrease in overall robusticity in those populations that take to sedentism.[13] For instance, bone shaft thickness since the 17th and 18th centuries have decreased in the United States, indicating a less physically stressful life.[14] This is not, however, the case for current hunter gatherer and foraging populations, such as the Andaman Islanders, who retain overall robusticity.[15] In general, though, hunter gatherers tend to be robust in the legs and farmers tend to be robust in the arms, representing different physical load (i.e., walking many miles a day versus grinding wheat).

---

# Hunting hypothesis

In paleoanthropology, the hunting hypothesis is the hypothesis that human evolution was primarily influenced by the activity of hunting for relatively large and fast animals, and that the activity of hunting distinguished human ancestors from other hominins.
While it is undisputed that early humans were hunters, the importance of this fact for the final steps in the emergence of the genus Homo out of earlier australopithecines, with its bipedalism and production of stone tools (from about 2.5 million years ago), and eventually also control of fire (from about 1.5 million years ago), is emphasized in the "hunting hypothesis", and de-emphasized in scenarios that stress the omnivore status of humans as their recipe for success, and social interaction, including mating behaviour as essential in the emergence of language and culture.
Advocates of the hunting hypothesis tend to believe that tool use and toolmaking essential to effective hunting were an extremely important part of human evolution, and trace the origin of language and religion to a hunting context.
As societal evidence David Buss cites that modern tribal population deploy hunting as their primary way of acquiring food.[1] The Aka pygmies in the Central African Republic spend 56% of their quest for nourishment hunting, 27% gathering, and 17% processing food. Additionally, the !Kung in Botswana retain 40% of their calories from hunting and this percentage varies from 20% to 90% depending on the season.[2] For physical evidence Buss first looks to the guts of humans and apes. The human gut consists mainly of the small intestines, which are responsible for the rapid breakdown of proteins and absorption of nutrients. The ape's gut is primarily colon, which indicates a vegetarian diet. This structural difference supports the hunting hypothesis in being an evolutionary branching point between modern humans and modern primates. Buss also cites human teeth in that fossilized human teeth have a thin enamel coating with very little heavy wear and tear that would result from a plant diet. The absence of thick enamel also indicates that historically humans have maintained a meat-heavy diet.[2] Buss notes that the bones of animals human ancestors killed found at Olduvai Gorge have cut marks at strategic points on the bones that indicate tool usage and provide evidence for ancestral butchers.[2]
Women are theorized to have participated in hunting, either on their own or as a collective group effort.[3] It is suggested that in the past, women targeted low but guaranteed food, whereas men targeted higher risk higher reward food.[4] The Gathering Hypothesis is a view that states men provided the evolution of the current human through hunting while women contributed via gathering.[5] Though criticized by many, it provides clues that both hunting and gathering were patterns of acquiring food and resources.
According to the hunting hypothesis, women are preoccupied with pregnancy and dependent children and so do not hunt because it is dangerous and less profitable. In addition, subsistence labor differentiates as observations suggests gender patterns originate from genetic traits.[3] Another possible explanation for women gathering is their inherent prioritization of rearing offspring, which is difficult to uphold if women were hunting.[6] Hunting is seen as more cost effective for men than for women.[5] The division of labor allows both types of resources (animals and plants) to be utilized.[5] Individual or small group hunting requires patience and skill more than strength, so women are just as capable as men. Plant collecting can be a physically demanding task so strength, endurance, or patience does not explain why women do not regularly hunt large game.[4] Since women hunt while menstruating, and if a child is still being breastfed, the mother may take him or her along in a shoulder sling while hunting or gathering.[4] Women hunt when it is compatible with children, and this usually means communal net hunts and/or hunting small game, and if childcare prevents a woman from hunting when young, the expertise to be an effective hunter later on may not be acquired.[4]
Though the hunting hypothesis is still being debated today, many experts have theorized the impact that women had concerning their involvement with hunter-gatherers being primarily males, was much larger than previously thought.[4][7][8][3] Women in foraging societies do hunt small game regularly and, occasionally, large game.[4] The majority of human's evolutionary history consisted of being hunter-gatherers as such women evolved the necessary traits needed for hunting such as endurance, movement coordination, and athleticism.[7] Hunting big game requires a collaborative effort, thus participation from all abled-bodies was encouraged which included females.[3] In addition, Atlatl or Spear-thrower's required more energy to be utilized so contributions from everyone, including females, would've contributed with mitigating the energy exerted to use Atlatl's.[3] Such examples consist of the Martu women in western Australia, for example, who frequently hunt goannas and skink.[4] Women also participate in communal game drives and can have extensive land knowledge as well, which they use to assist their husbands in hunting.[4] Kelly Robert's example consists of 6 Agta women who are hunters and returned home with a kill 31 percent of the time, whereas men averaged 17 percent.[4] The women's expertise with hunting was further shown with mixed groups of male and female hunters being the most successful, coming home with kills 41 percent of the time.[4] Agta females who have reached the end of their childbearing years, those with children old enough to look after themselves in camp, or those who are sterile are the ones who intentionally hunt.[4] It's noted that women target reliable but low-return-rate foods, whereas men target less reliable but high-return-rate foods.[4] This could be an explanation as to why women weren't commonly documented as hunters.
Buss purports that the hunting hypothesis explains the high level of human male parental investment in offspring as compared to primates. Meat is an economical and condensed food resource in that it can be brought home to feed the young, as it is not efficient to carry low-calorie food across great distances. Thus, the act of hunting and the required transportation of the kill in order to feed offspring is a reasonable explanation for human male provisioning.[2]
Buss suggests that the Hunting hypothesis also explains the advent of strong male coalitions. Although chimpanzees form male-male coalitions, they tend to be temporary and opportunistic. Contrastingly, large game hunters require consistent and coordinated cooperation to succeed in large game hunting. Thus male coalitions were the result of working together to succeed in providing meat for the hunters themselves and their families.[2] Kristen Hawkes suggests further that obtaining resources intended for community consumption increases a male's fitness by appealing to the male's society and thus being in the good favor of both males and females. The male relationship would improve hunting success and create alliances for future conflict and the female relationship would improve direct reproductive success.[2] Buss proposes alternate explanations of emergence of the strong male coalitions. He suggests that male coalitions may have been the result of group-on-group aggression, defense, and in-group political alliances. This explanation does not support the relationship between male coalitions and hunting.[2]
Hawkes proposes that hunters pursue large game and divide the kill across the group. Hunters compete to divvy up the kill to signal courage, power, generosity, prosocial intent, and dedication. By engaging in these activities, hunters receive reproductive benefits and respect.[9] These reproductive benefits lead to greater reproductive success in more skilled hunters.[9] Evidence of these hunting goals that do not only benefit the families of the hunters are in the Ache and Hadza men. Hawkes notes that their hunting techniques are less efficient than alternative methods and are energetically costly, but the men place more importance on displaying their bravery, power, and prosocial intent than on hunting efficiency. This method is different as compared to other societies where hunters retain the control of their kills and signal their intent of sharing. This alternate method aligns with the coalition support hypothesis, in efforts to create and preserve political associations.[9]
The meat from successful large game hunts are more than what a single hunter can consume. Further, hunting success varies by week. One week a hunter may succeed in hunting large game and the next may return with no meat. In this situation Buss suggests that there are low costs to giving away meat that cannot be eaten by the individual hunter on his own and large benefits from the expectation of the returned favor in a week where his hunting is not successful.[2] Hawkes calls this sharing “tolerated theft” and purports that the benefits of reciprocal altruism stem from the result that families will experience “lower daily variation and higher daily average” in their resources.[10]
Provisioning may actually be a form of sexual competition between males for females.[11] Hawkes suggests that male provisioning is a particularly human behavior, which forges the nuclear family.[10] The structure of familial provisioning determines a form of resource distribution. However, Hawkes does acknowledge inconsistencies across societies and contexts such as the fluctuating time courses dedicated to hunting and gathering, which are not directly correlated with return rates, the fact that nutrition value is often chosen over caloric count, and the fact that meat is a more widely spread resource than other resources.[10]
The show-off hypothesis is the concept that more successful men have better mate options. The idea relates back to the fact that meat, the result of hunting expeditions, is a distinct resource in that it comes in large quantities that more often than not the hunter's own family is not able to consume in a timely manner so that the meat doesn't go sour.[2] Also the success of hunting is unpredictable whereas berries and fruits, unless there is a drought or a bad bush, are fairly consistent in seasonality. Kristen Hawkes argues that women favor neighbors opting for men who provide the advantageous, yet infrequent meat feasts.[10] These women may profit from alliance and the resulting feasts, especially in times of shortage. Hawkes suggests that it would be beneficial for women to reward men who employ the “show-off strategy” by supporting them in a dispute, caring for their offspring, or providing sexual favors.[10] The benefits women may gain from their alignment lie in favored treatment of the offspring spawned by the show-off from neighbors.[10] Buss echoes and cites Hawke's thoughts on the show-off's benefits in sexual access, increased likelihood of having children, and the favorable treatment his children would receive from the other members of the society.[2] Hawkes also suggests that show-offs are more likely to live in large groups and thus be less susceptible to predators.[10] Show-offs gain more benefits from just sharing with their family (classical fitness) in the potential favorable treatment from the community and reciprocal altruism from other members of the community.[10]
Hawkes uses the Ache people of Paraguay as evidence for the Show-off hypothesis. Food acquired by men was more widely distributed across the community and inconsistent resources that came in large quantities when acquired were also more widely shared.[10]
While this is represented in the Ache according to Hawkes, Buss notes that this trend is contradicted in the Hadza who evenly distribute the meat across all members of their population and whose hunters have very little control over the distribution. In the Hadza the show-off hypothesis does not have to do with the resources that result from hunting, but from the prestige and risk that is involved in big game hunting. There are possible circuitous benefits such as protection and defense.[2]
The Gathering Hypothesis is the view that men provided critical evolutionary propulsion of the modern human through hunting, whereas women contributed via gathering.[5] In addition, it helps provide for the fact that our ancestor's diets consisted mostly of plant food.[5] It's suggested by David Buss that stone tools were invented not strictly for hunting, but for gathering plants and used for digging them up.[5] This could explain the migration from forests to woodlands as tools allowed easy access to previously used methods. As such, this view results in the hunting part of the modern human coming much later.[5] Though women weren't strictly hunters, a woman's time investment in foraging depended on how much food her husband brought back.[5] Gathering plant foods allows a person to return to camp when necessary, but hunting may require an overnight stay so as to continue tracking the animal in the morning.[4]
The Gathering Hypothesis has been criticized by those who believe it's incapable of explaining our human origins in the primate lineage.[5] A common argument against the Gathering hypothesis is if gathering was the best or most efficient method of acquiring food, then why wouldn’t men just gather and stop wasting their time hunting.[5] The division of labor among men and woman is unaccounted for throughout cultures.[5] Hunting often takes the hunter far away from the home base, selection would favor hunters who could find their way home without getting lost along the way.[5] Locating and gathering edible nuts, berries, fruit, and tubers would require a different set of spatial skills.[5] The high prevalence of male hunters and female gatherers among traditional societies, although not conclusive evidence, provides one more clue that both activities are part of the human pattern of procuring food.[5]

---

# Recent African origin of modern humans

In paleoanthropology, the recent African origin of modern humans or the "Out of Africa" theory (OOA)[a] is the most widely accepted[1][2][3] model of the geographic origin and early migration of anatomically modern humans (Homo sapiens). It follows the early expansions of hominins out of Africa, accomplished by Homo erectus and then Homo neanderthalensis.
The model proposes a "single origin" of Homo sapiens in the taxonomic sense, precluding parallel evolution in other regions of traits considered anatomically modern,[4] but not precluding multiple admixture between H. sapiens and archaic humans in Europe and Asia.[b][5][6] H. sapiens most likely developed in the Horn of Africa between 300,000 and 200,000 years ago,[7][8] although an alternative hypothesis argues that diverse morphological features of H. sapiens appeared locally in different parts of Africa and converged due to gene flow between different populations within the same period.[9][10] The "recent African origin" model proposes that all modern non-African populations are substantially descended from populations of H. sapiens that left Africa after that time.
There were at least several "out-of-Africa" dispersals of modern humans, possibly beginning as early as 270,000 years ago, including 215,000 years ago to at least Greece,[11][12][13] and certainly via northern Africa and the Arabian Peninsula about 130,000 to 115,000 years ago.[20] There is evidence that modern humans had reached China around 80,000 years ago.[21] Practically all of these early waves seem to have gone extinct or retreated back, and present-day humans outside Africa descend mainly from a single expansion about 70,000–50,000 years ago,[22][23][24][7][8][25][26][excessive citations] via the so-called "Southern Route". These humans spread rapidly along the coast of Asia and reached Australia by around 65,000–50,000 years ago,[27][28][c] (though some researchers question the earlier Australian dates and place the arrival of humans there at 50,000 years ago at earliest,[29][30] while others have suggested that these first settlers of Australia may represent an older wave before the more significant out of Africa migration and thus not necessarily be ancestral to the region's later inhabitants[24]) while Europe was populated by an early offshoot which settled the Near East and Europe less than 55,000 years ago.[31][32][33]
In the 2010s, studies in population genetics uncovered evidence of interbreeding that occurred between H. sapiens and archaic humans in Eurasia, Oceania and Africa,[34][35][36] indicating that modern population groups, while mostly derived from early H. sapiens, are to a lesser extent also descended from regional variants of archaic humans.
"Recent African origin", or Out of Africa II, refers to the migration of anatomically modern humans (Homo sapiens) out of Africa after their emergence at c. 300,000 to 200,000 years ago, in contrast to "Out of Africa I", which refers to the migration of archaic humans from Africa to Eurasia from before 1.8 and up to 0.5 million years ago. Omo-Kibish I (Omo I) from southern Ethiopia is the oldest anatomically modern Homo sapiens skeleton currently known (around 233,000 years old).[38] There are even older Homo sapiens fossils from Jebel Irhoud in Morocco which exhibit a mixture of modern and archaic features at around 315,000 years old.[39]
Since the beginning of the 21st century, the picture of "recent single-origin" migrations has become significantly more complex, due to the discovery of modern-archaic admixture and the increasing evidence that the "recent out-of-Africa" migration took place in waves over a long time. As of 2010, there were two main accepted dispersal routes for the out-of-Africa migration of early anatomically modern humans, the "Northern Route" (via Nile Valley and Sinai) and the "Southern Route" via the Bab-el-Mandeb strait.[40]
Beginning 135,000 years ago, tropical Africa experienced megadroughts which drove humans from the land and towards the sea shores, and forced them to cross over to other continents.[49][e]
Fossils of early Homo sapiens were found in Qafzeh and Es-Skhul Caves in Israel and have been dated to 80,000 to 120,000 years ago.[50][51] These humans seem to have either become extinct or retreated back to Africa 70,000 to 80,000 years ago, possibly replaced by southbound Neanderthals escaping the colder regions of ice-age Europe.[22] Hua Liu et al. analyzed autosomal microsatellite markers dating to about 56,000 years ago. They interpret the paleontological fossil as an isolated early offshoot that retracted back to Africa.[23]
The discovery of stone tools in the United Arab Emirates in 2011 at the Faya-1 site in Mleiha, Sharjah, indicated the presence of modern humans at least 125,000 years ago,[14] leading to a resurgence of the "long-neglected" North African route.[15][52][16][17] This new understanding of the role of the Arabian dispersal began to change following results from archaeological and genetic studies stressing the importance of southern Arabia as a corridor for human expansions out of Africa.[53]
In Oman, a site was discovered by Bien Joven in 2011 containing more than 100 surface scatters of stone tools belonging to the late Nubian Complex, known previously only from archaeological excavations in the Sudan. Two optically stimulated luminescence age estimates placed the Arabian Nubian Complex at approximately 106,000 years old. This provides evidence for a distinct Stone Age technocomplex in southern Arabia, around the earlier part of the Marine Isotope Stage 5.[54]
According to Kuhlwilm and his co-authors, Neanderthals contributed genetically to modern humans then living outside of Africa around 100,000 years ago: humans which had already split off from other modern humans around 200,000 years ago, and this early wave of modern humans outside Africa also contributed genetically to the Altai Neanderthals.[55] They found that "the ancestors of Neanderthals from the Altai Mountains and early modern humans met and interbred, possibly in the Near East, many thousands of years earlier than previously thought".[55] According to co-author Ilan Gronau, "This actually complements archaeological evidence of the presence of early modern humans out of Africa around and before 100,000 years ago by providing the first genetic evidence of such populations."[55] Similar genetic admixture events have been noted in other regions as well.[56]
By some 50–70,000 years ago, a subset of the bearers of mitochondrial haplogroup L3 migrated from East Africa into the Near East. It has been estimated that from a population of 2,000 to 5,000 individuals in Africa, only a small group, possibly as few as 150 to 1,000 people, crossed the Red Sea.[57][58] The group that crossed the Red Sea travelled along the coastal route around Arabia and the Persian Plateau to India, which appears to have been the first major settling point.[59] Wells (2003) argued for the route along the southern coastline of Asia, across about 250 kilometres (155 mi), reaching Australia by around 50,000 years ago.
Today at the Bab-el-Mandeb straits, the Red Sea is about 20 kilometres (12 mi) wide, but 50,000 years ago sea levels were 70 m (230 ft) lower (owing to glaciation) and the water channel was much narrower. Though the straits were never completely closed, they were narrow enough to have enabled crossing using simple rafts, and there may have been islands in between.[40][60] Shell middens 125,000 years old have been found in Eritrea,[61] indicating that the diet of early humans included seafood obtained by beachcombing.
The dating of the Southern Dispersal is a matter of dispute.[48] It may have happened either pre- or post-Toba, a catastrophic volcanic eruption that took place between 69,000 and 77,000 years ago at the site of present-day Lake Toba in Sumatra, Indonesia. Stone tools discovered below the layers of ash deposited in India may point to a pre-Toba dispersal but the source of the tools is disputed.[48] An indication for post-Toba is haplo-group L3, that originated before the dispersal of humans out of Africa and can be dated to 60,000–70,000 years ago, "suggesting that humanity left Africa a few thousand years after Toba".[48] Some research showing slower than expected genetic mutations in human DNA was published in 2012, indicating a revised dating for the migration to between 90,000 and 130,000 years ago.[62] Some more recent research suggests a migration out-of-Africa of around 50,000-65,000 years ago of the ancestors of modern non-African populations, similar to most previous estimates.[24][63][64]
Following the fossils dating 80,000 to 120,000 years ago from Qafzeh and Es-Skhul Caves in Israel there are no H. sapiens fossils in the Levant until the Manot 1 fossil from Manot Cave in Israel, dated to 54,700 years ago,[65] though the dating was questioned by Groucutt et al. (2015). The lack of fossils and stone tool industries that can be safely associated with modern humans in the Levant has been taken to suggest that modern humans were outcompeted by Neanderthals until around 55,000 years ago, who would have placed a barrier on modern human dispersal out of Africa through the Northern Route.[66][failed verification] Climate reconstructions also support a Southern Route dispersal of modern humans as the Bab-el-Mandeb strait experienced a climate more conductive to human migration than the northern landbridge to the Levant during the major human dispersal out of Africa.[67]
A 2023 study proposed that Eurasians and Africans genetically diverged ~100,000 years ago. Main Eurasians then lived in the Saudi Peninsula, genetically isolated from at least 85 kya, before expanding north 54 kya. For reference, Homo sapiens and Neanderthals diverged ~500 kya.[68]
It is thought that Australia was inhabited around 65,000–50,000 years ago. As of 2017, the earliest evidence of humans in Australia is at least 65,000 years old,[27][28] while McChesney stated that
...genetic evidence suggests that a small band with the marker M168 migrated out of Africa along the coasts of the Arabian Peninsula and India, through Indonesia, and reached Australia very early, between 60,000 and 50,000 years ago. This very early migration into Australia is also supported by Rasmussen et al. (2011).[31]
Fossils from Lake Mungo, Australia, have been dated to about 42,000 years ago.[69][70] Other fossils from a site called Madjedbebe have been dated to at least 65,000 years ago,[71][72] though some researchers doubt this early estimate and date the Madjedbebe fossils at about 50,000 years ago at the oldest.[29][30]
Phylogenetic data suggests that an early Eastern Eurasian (Eastern non-African) meta-population trifurcated somewhere in eastern South Asia, and gave rise to the Australo-Papuans, the Ancient Ancestral South Indians (AASI), as well as East/Southeast Asians, although Papuans may have also received some gene flow from an earlier group (xOoA), around 2%,[73] next to additional archaic admixture in the Sahul region.[74][75]
According to one study, Papuans could have either formed from a mixture between an East Eurasian lineage and lineage basal to West and East Asians, or as a sister lineage of East Asians with or without a minor basal OoA or xOoA contribution.[76]
A Holocene hunter-gatherer sample (Leang_Panninge) from South Sulawesi was found to be genetically in between East-Eurasians and Australo-Papuans. The sample could be modeled as ~50% Papuan-related and ~50% Basal-East Asian-related (Andamanese Onge or Tianyuan). The authors concluded that Basal-East Asian ancestry was far more widespread and the peopling of Insular Southeast Asia and Oceania was more complex than previously anticipated.[77][78]
In China, the Liujiang man (Chinese: 柳江人) is among the earliest modern humans found in East Asia.[79] The date most commonly attributed to the remains is 67,000 years ago.[80] High rates of variability yielded by various dating techniques carried out by different researchers place the most widely accepted range of dates with 67,000 BP as a minimum, but do not rule out dates as old as 159,000 BP.[80] Liu, Martinón-Torres et al. (2015) claim that modern human teeth have been found in China dating to at least 80,000 years ago.[81]
Tianyuan man from China has a probable date range between 38,000 and 42,000 years ago, while Liujiang man from the same region has a probable date range between 67,000 and 159,000 years ago. According to 2013 DNA tests, Tianyuan man is related "to many present-day Asians and Native Americans".[82][83][84][85][86] Tianyuan is similar in morphology to Liujiang man, and some Jōmon period modern humans found in Japan, as well as modern East and Southeast Asians.[87][88][89]
A 2021 study about the population history of Eastern Eurasia, concluded that distinctive Basal-East Asian (East-Eurasian) ancestry originated in Mainland Southeast Asia at ~50,000BC from a distinct southern Himalayan route, and expanded through multiple migration waves southwards and northwards respectively.[90]
Genetic studies concluded that Native Americans descended from a single founding population that initially split from a Basal-East Asian source population in Mainland Southeast Asia around 36,000 years ago, at the same time at which the proper Jōmon people split from Basal-East Asians, either together with Ancestral Native Americans or during a separate expansion wave. They also show that the basal northern and southern Native American branches, to which all other Indigenous peoples belong, diverged around 16,000 years ago.[91][92] An indigenous American sample from 16,000BC in Idaho, which is craniometrically similar to modern Native Americans as well as Paleosiberians, was found to have largely East-Eurasian ancestry and showed high affinity with contemporary East Asians, as well as Jōmon period samples of Japan, confirming that Ancestral Native Americans split from an East-Eurasian source population in Eastern Siberia.[93]
According to Macaulay et al. (2005), an early offshoot from the southern dispersal with haplogroup N followed the Nile from East Africa, heading northwards and crossing into Asia through the Sinai. This group then branched, some moving into Europe and others heading east into Asia.[32] This hypothesis is supported by the relatively late date of the arrival of modern humans in Europe as well as by archaeological and DNA evidence.[32] Based on an analysis of 55 human mitochondrial genomes (mtDNAs) of hunter-gatherers, Posth et al. (2016) argue for a "rapid single dispersal of all non-Africans less than 55,000 years ago". By 45,000 years ago, modern humans are known to have reached northwestern Europe.[94]
The first lineage to branch off from Mitochondrial Eve was L0. This haplogroup is found in high proportions among the San of Southern Africa and the Sandawe of East Africa. It is also found among the Mbuti people.[95][96] These groups branched off early in human history and have remained relatively genetically isolated since then. Haplogroups L1, L2, and L3 are descendants of L1–L6, and are largely confined to Africa. The macro haplogroups M and N, which are the lineages of the rest of the world outside Africa, descend from L3. L3 is about 70,000 years old, while haplogroups M and N are about 65–55,000 years old.[97][64] The relationship between such gene trees and demographic history is still debated when applied to dispersals.[98]
Of all the lineages present in Africa, the female descendants of only one lineage, mtDNA haplogroup L3, are found outside Africa. If there had been several migrations, one would expect descendants of more than one lineage to be found. L3's female descendants, the M and N haplogroup lineages, are found in very low frequencies in Africa (although haplogroup M1 populations are very ancient and diversified in North and North-east Africa) and appear to be more recent arrivals.[citation needed] A possible explanation is that these mutations occurred in East Africa shortly before the exodus and became the dominant haplogroups thereafter by means of the founder effect. Alternatively, the mutations may have arisen shortly afterwards.
Results from mtDNA collected from aboriginal Malaysians called Orang Asli indicate that the haplogroups M and N share characteristics with original African groups from approximately 85,000 years ago, and share characteristics with sub-haplogroups found in coastal south-east Asian regions, such as Australasia, the Indian subcontinent and throughout continental Asia, which had dispersed and separated from their African progenitor approximately 65,000 years ago. This southern coastal dispersal would have occurred before the dispersal through the Levant approximately 45,000 years ago.[32] This hypothesis attempts to explain why haplogroup N is predominant in Europe and why haplogroup M is absent in Europe. Evidence of the coastal migration is thought to have been destroyed by the rise in sea levels during the Holocene epoch.[99] Alternatively, a small European founder population that had expressed haplogroup M and N at first, could have lost haplogroup M through random genetic drift resulting from a bottleneck (i.e. a founder effect).
The group that crossed the Red Sea travelled along the coastal route around Arabia and Persia until reaching India.[59] Haplogroup M is found in high frequencies along the southern coastal regions of Pakistan and India and it has the greatest diversity in India, indicating that it is here where the mutation may have occurred.[59] Sixty percent of the Indian population belong to Haplogroup M. The indigenous people of the Andaman Islands also belong to the M lineage. The Andamanese are thought to be offshoots of some of the earliest inhabitants in Asia because of their long isolation from the mainland. They are evidence of the coastal route of early settlers that extends from India to Thailand and Indonesia all the way to eastern New Guinea. Since M is found in high frequencies in highlanders from New Guinea and the Andamanese and New Guineans have dark skin and Afro-textured hair, some scientists think they are all part of the same wave of migrants who departed across the Red Sea ~60,000 years ago in the Great Coastal Migration. The proportion of haplogroup M increases eastwards from Arabia to India; in eastern India, M outnumbers N by a ratio of 3:1. Crossing into Southeast Asia, haplogroup N (mostly in the form of derivatives of its R subclade) reappears as the predominant lineage.[citation needed] M is predominant in East Asia, but amongst Indigenous Australians, N is the more common lineage.[citation needed] This haphazard distribution of Haplogroup N from Europe to Australia can be explained by founder effects and population bottlenecks.[100]
A 2002 study of African, European, and Asian populations, found greater genetic diversity among Africans than among Eurasians, and that genetic diversity among Eurasians is largely a subset of that among Africans, supporting the out of Africa model.[102] A large study by Coop et al. (2009) found evidence for natural selection in autosomal DNA outside of Africa. The study distinguishes non-African sweeps (notably KITLG variants associated with skin color), West-Eurasian sweeps (SLC24A5) and East-Asian sweeps (MC1R, relevant to skin color). Based on this evidence, the study concluded that human populations encountered novel selective pressures as they expanded out of Africa.[103] MC1R and its relation to skin color had already been discussed by Harding et al. (2000), p. 1355. According to this study, Papua New Guineans continued to be exposed to selection for dark skin color so that, although these groups are distinct from Africans in other places, the allele for dark skin color shared by contemporary Africans, Andamanese and New Guineans is an archaism. Endicott et al. (2003) suggest convergent evolution. A 2014 study by Gurdasani et al. indicates that the higher genetic diversity in Africa was further increased in some regions by relatively recent Eurasian migrations affecting parts of Africa.[104]
Another promising route towards reconstructing human genetic genealogy is via the JC virus (JCV), a type of human polyomavirus which is carried by 70–90 percent of humans and which is usually transmitted vertically, from parents to offspring, suggesting codivergence with human populations. For this reason, JCV has been used as a genetic marker for human evolution and migration.[105] This method does not appear to be reliable for the migration out of Africa; in contrast to human genetics, JCV strains associated with African populations are not basal. From this Shackelton et al. (2006) conclude that either a basal African strain of JCV has become extinct or that the original infection with JCV post-dates the migration from Africa.
Evidence for archaic human species (descended from Homo heidelbergensis) having interbred with modern humans outside of Africa, was discovered in the 2010s. This concerns primarily Neanderthal admixture in all modern populations except for Sub-Saharan Africans but evidence has also been presented for Denisova hominin admixture in Australasia (i.e. in Melanesians, Aboriginal Australians and some Negritos).[106] The rate of Neanderthal admixture to European and Asian populations as of 2017 has been estimated at between about 2–3%.[107]
Archaic admixture in some Sub-Saharan African populations hunter-gatherer groups (Biaka Pygmies and San), derived from archaic hominins that broke away from the modern human lineage around 700,000 years ago, was discovered in 2011. The rate of admixture was estimated at 2%.[36] Admixture from archaic hominins of still earlier divergence times, estimated at 1.2 to 1.3 million years ago, was found in Pygmies, Hadza and five Sandawe in 2012.[108][35]
From an analysis of Mucin 7, a highly divergent haplotype that has an estimated coalescence time with other variants around 4.5 million years BP and is specific to African populations, it is inferred to have been derived from interbreeding between African modern and archaic humans.[109]
A study published in 2020 found that the Yoruba and Mende populations of West Africa derive between 2% and 19% of their genome from an as-yet unidentified archaic hominin population that likely diverged before the split of modern humans and the ancestors of Neanderthals and Denisovans.[110]
In addition to genetic analysis, Petraglia et al. also examines the small stone tools (microlithic materials) from the Indian subcontinent and explains the expansion of population based on the reconstruction of paleoenvironment. He proposed that the stone tools could be dated to 35 ka in South Asia, and the new technology might be influenced by environmental change and population pressure.[111]
The cladistic relationship of humans with the African apes was suggested by Charles Darwin after studying the behaviour of African apes, one of which was displayed at the London Zoo.[113] The anatomist Thomas Huxley had also supported the hypothesis and suggested that African apes have a close evolutionary relationship with humans.[114] These views were opposed by the German biologist Ernst Haeckel, who was a proponent of the Out of Asia theory. Haeckel argued that humans were more closely related to the primates of South-east Asia and rejected Darwin's African hypothesis.[115][116]
In The Descent of Man, Darwin speculated that humans had descended from apes, which still had small brains but walked upright, freeing their hands for uses which favoured intelligence; he thought such apes were African:
In each great region of the world the living mammals are closely related to the extinct species of the same region. It is, therefore, probable that Africa was formerly inhabited by extinct apes closely allied to the gorilla and chimpanzee; and as these two species are now man's nearest allies, it is somewhat more probable that our early progenitors lived on the African continent than elsewhere. But it is useless to speculate on this subject, for an ape nearly as large as a man, namely the Dryopithecus of Lartet, which was closely allied to the anthropomorphous Hylobates, existed in Europe during the Upper Miocene period; and since so remote a period the earth has certainly undergone many great revolutions, and there has been ample time for migration on the largest scale.
In 1871, there were hardly any human fossils of ancient hominins available. Almost fifty years later, Darwin's speculation was supported when anthropologists began finding fossils of ancient small-brained hominins in several areas of Africa (list of hominina fossils). The hypothesis of recent (as opposed to archaic) African origin developed in the 20th century. The "Recent African origin" of modern humans means "single origin" (monogenism) and has been used in various contexts as an antonym to polygenism. The debate in anthropology had swung in favour of monogenism by the mid-20th century. Isolated proponents of polygenism held forth in the mid-20th century, such as Carleton Coon, who thought as late as 1962 that H. sapiens arose five times from H. erectus in five places.[118]
The historical alternative to the recent origin model is the multiregional origin of modern humans, initially proposed by Milford Wolpoff in the 1980s. This view proposes that the derivation of anatomically modern human populations from H. erectus at the beginning of the Pleistocene 1.8 million years BP, has taken place within a continuous world population. The hypothesis necessarily rejects the assumption of an infertility barrier between ancient Eurasian and African populations of Homo. The hypothesis was controversially debated during the late 1980s and the 1990s.[119] The now-current terminology of "recent-origin" and "Out of Africa" became current in the context of this debate in the 1990s.[120] Originally seen as an antithetical alternative to the recent origin model, the multiregional hypothesis in its original "strong" form is obsolete, while its various modified weaker variants have become variants of a view of "recent origin" combined with archaic admixture.[121] Stringer (2014) distinguishes the original or "classic" Multiregional model as having existed from 1984 (its formulation) until 2003, to a "weak" post-2003 variant that has "shifted close to that of the Assimilation Model".[122][123]
In the 1980s, Allan Wilson together with Rebecca L. Cann and Mark Stoneking worked on genetic dating of the matrilineal most recent common ancestor of modern human populations (dubbed "Mitochondrial Eve"). To identify informative genetic markers for tracking human evolutionary history, Wilson concentrated on mitochondrial DNA (mtDNA), which is maternally inherited. This DNA material mutates quickly, making it easy to plot changes over relatively short times. With his discovery that human mtDNA is genetically much less diverse than chimpanzee mtDNA, Wilson concluded that modern human populations had diverged recently from a single population while older human species such as Neanderthals and Homo erectus had become extinct.[124] With the advent of archaeogenetics in the 1990s, the dating of mitochondrial and Y-chromosomal haplogroups became possible with some confidence. By 1999, estimates ranged around 150,000 years for the mt-MRCA and 60,000 to 70,000 years for the migration out of Africa.[125]
From 2000 to 2003, there was controversy about the mitochondrial DNA of "Mungo Man 3" (LM3) and its possible bearing on the multiregional hypothesis. LM3 was found to have more than the expected number of sequence differences when compared to modern human DNA (CRS).[126] Comparison of the mitochondrial DNA with that of ancient and modern aborigines, led to the conclusion that Mungo Man fell outside the range of genetic variation seen in Aboriginal Australians and was used to support the multiregional origin hypothesis. A reanalysis of LM3 and other ancient specimens from the area published in 2016, showed it to be akin to modern Aboriginal Australian sequences, inconsistent with the results of the earlier study.[127]
The Y chromosome, which is paternally inherited, does not go through much recombination and thus stays largely the same after inheritance. Similar to Mitochondrial Eve, this could be studied to track the male most recent common ancestor ("Y-chromosomal Adam" or Y-MRCA).[128]
The most basal lineages have been detected in West, Northwest and Central Africa, suggesting plausibility for the Y-MRCA living in the general region of "Central-Northwest Africa".[16]
A Stanford University School of Medicine study was done by comparing Y-chromosome sequences and mtDNA in 69 men from different geographic regions and constructing a family tree. It was found that the Y-MRCA lived between 120,000 and 156,000, and the Mitochondrial Eve lived between 99,000 and 148,000 years ago, which not only predates some proposed waves of migration, but also meant that both lived in the African continent around the same time period.[129]
Another study finds a plausible placement in "the north-western quadrant of the African continent" for the emergence of the A1b haplogroup.[130] The 2013 report of haplogroup A00 found among the Mbo people of western present-day Cameroon is also compatible with this picture.[131]
The revision of Y-chromosomal phylogeny since 2011 has affected estimates for the likely geographical origin of Y-MRCA as well as estimates on time depth. By the same reasoning, future discovery of presently-unknown archaic haplogroups in living people would again lead to such revisions. In particular, the possible presence of between 1% and 4% Neanderthal-derived DNA in Eurasian genomes implies that the (unlikely) event of a discovery of a single living Eurasian male exhibiting a Neanderthal patrilineal line would immediately push back T-MRCA ("time to MRCA") to at least twice its current estimate. However, the discovery of a Neanderthal Y-chromosome by Mendez et al. was tempered by a 2016 study that suggests the extinction of Neanderthal patrilineages, as the lineage inferred from the Neanderthal sequence is outside of the range of contemporary human genetic variation.[132] Questions of geographical origin would become part of the debate on Neanderthal evolution from Homo erectus.

---

# Multiregional origin of modern humans

The multiregional hypothesis, multiregional evolution (MRE), or polycentric hypothesis, is a scientific model that provides an alternative explanation to the more widely accepted "Out of Africa" model of monogenesis for the pattern of human evolution.
Multiregional evolution holds that the human species first arose around two million years ago and subsequent human evolution has been within a single, continuous human species. This species encompasses all archaic human forms such as Homo erectus, Denisovans, and Neanderthals as well as modern forms, and evolved worldwide to the diverse populations of anatomically modern humans (Homo sapiens).
The hypothesis contends that the mechanism of clinal variation through a model of "centre and edge" allowed for the necessary balance between genetic drift, gene flow, and selection throughout the Pleistocene, as well as overall evolution as a global species, but while retaining regional differences in certain morphological features.[1] Proponents of multiregionalism point to fossil and genomic data and continuity of archaeological cultures as support for their hypothesis.
The multiregional hypothesis was first proposed in 1984, and then revised in 2003. In its revised form, it is similar to the assimilation model, which holds that modern humans originated in Africa and today share a predominant recent African origin, but have also absorbed small, geographically variable, degrees of admixture from other regional (archaic) hominin species.[2]
The multiregional hypothesis is not currently the most accepted theory of modern human origin among scientists. "The African replacement model has gained the widest acceptance owing mainly to genetic data (particularly mitochondrial DNA) from existing populations. This model is consistent with the realization that modern humans cannot be classified into subspecies or races, and it recognizes that all populations of present-day humans share the same potential."[3] The African replacement model is also known as the "out of Africa" theory, which is currently the most widely accepted model. It proposes that Homo sapiens evolved in Africa before migrating across the world."[4] And: "The primary competing scientific hypothesis is currently recent African origin of modern humans, which proposes that modern humans arose as a new species in Africa around 100-200,000 years ago, moving out of Africa around 50-60,000 years ago to replace existing human species such as Homo erectus and the Neanderthals without interbreeding.[5][6][7][8] This differs from the multiregional hypothesis in that the multiregional model predicts interbreeding with preexisting local human populations in any such migration."[8][9]
The Multiregional hypothesis was proposed in 1984 by Milford H. Wolpoff, Alan Thorne and Xinzhi Wu.[10][11][1] Wolpoff credits Franz Weidenreich's "Polycentric" hypothesis of human origins as a major influence, but cautions that this should not be confused with polygenism, or Carleton Coon's model that minimized gene flow.[11][12][13] According to Wolpoff, multiregionalism was misinterpreted by William W. Howells, who confused Weidenreich's hypothesis with a polygenic "candelabra model" in his publications spanning five decades:
How did Multiregional evolution get stigmatized as polygeny? We believe it comes from the confusion of Weidenreich's ideas, and ultimately of our own, with Coon's. The historic reason for linking Coon's and Weidenreich's ideas came from the mischaracterizations of Weidenreich's Polycentric model as a candelabra (Howells, 1942, 1944, 1959, 1993), that made his Polycentric model appear much more similar to Coon's than it actually was.[14]
Through the influence of Howells, many other anthropologists and biologists have confused multiregionalism with polygenism i.e. separate or multiple origins for different populations. Alan Templeton for example notes that this confusion has led to the error that gene flow between different populations was added to the Multiregional hypothesis as a "special pleading in response to recent difficulties", despite the fact: "parallel evolution was never part of the multiregional model, much less its core, whereas gene flow was not a recent addition, but rather was present in the model from the very beginning"[15] (emphasis in original). Despite this, multiregionalism is still confused with polygenism, or Coon's model of racial origins, from which Wolpoff and his colleagues have distanced themselves.[16][17] Wolpoff has also defended Wiedenreich's Polycentric hypothesis from being labeled polyphyletic. Weidenreich himself in 1949 wrote: "I may run the risk of being misunderstood, namely that I believe in polyphyletic evolution of man".[18]
In 1998, Wu founded a China-specific Multiregional model called "Continuity with [Incidental] Hybridization".[19][20] Wu's variant only applies the Multiregional hypothesis to the East Asian fossil record, and is popular among Chinese scientists.[21] However, James Leibold, a political historian of modern China, has argued the support for Wu's model is largely rooted in Chinese nationalism.[22] Outside of China, the Multiregional hypothesis has limited support, held only by a small number of paleoanthropologists.[23]
Chris Stringer, a leading proponent of the more mainstream recent African origin theory, debated Multiregionalists such as Wolpoff and Thorne in a series of publications throughout the late 1980s and 1990s.[24][25][26][27] Stringer describes how he considers the original Multiregional hypothesis to have been modified over time into a weaker variant that now allows a much greater role for Africa in human evolution, including anatomical modernity (and subsequently less regional continuity than was first proposed).[28]
Stringer distinguishes the original or "classic" Multiregional model as having existed from 1984 (its formulation) until 2003, to a "weak" post-2003 variant that has "shifted close to that of the Assimilation Model".[29][30]
The finding that "Mitochondrial Eve" was relatively recent and African seemed to give the upper hand to the proponents of the Out of Africa hypothesis. But in 2002, Alan Templeton published a genetic analysis involving other loci in the genome as well, and this showed that some variants that are present in modern populations existed already in Asia hundreds of thousands of years ago.[31] This meant that even if our male line (Y chromosome) and our female line (mitochondrial DNA) came out of Africa in the last 100,000 years or so, we have inherited other genes from populations that were already outside of Africa. Since this study other studies have been done using much more data (see Phylogeography).
Proponents of the multiregional hypothesis see regional continuity of certain morphological traits spanning the Pleistocene in different regions across the globe as evidence against a single replacement model from Africa. In general, three major regions are recognized: Europe, China, and Indonesia (often including Australia).[32][33][34] Wolpoff cautions that the continuity in certain skeletal features in these regions should not be seen in a racial context, instead calling them morphological clades; defined as sets of traits that "uniquely characterise a geographic region".[35] According to Wolpoff and Thorne (1981): "We do not regard a morphological clade as a unique lineage, nor do we believe it necessary to imply a particular taxonomic status for it".[36] Critics of multiregionalism have pointed out that no single human trait is unique to a geographical region (i.e. confined to one population and not found in any other) but Wolpoff et al. (2000) note that regional continuity only recognizes combinations of features, not traits if individually accessed, a point they elsewhere compare to the forensic identification of a human skeleton:
Regional continuity ... is not the claim that such features do not appear elsewhere; the genetic structure of the human species makes such a possibility unlikely to the extreme. There may be uniqueness in combinations of traits, but no single trait is likely to have been unique in a particular part of the world although it might appear to be so because of the incomplete sampling provided by the spotty human fossil record.
Combinations of features are "unique" in the sense of being found in only one region, or more weakly limited to one region at high frequency (very rarely in another). Wolpoff stresses that regional continuity works in conjunction with genetic exchanges between populations. Long-term regional continuity in certain morphological traits is explained by Alan Thorne's "centre and edge"[37] population genetics model which resolves Weidenreich's paradox of "how did populations retain geographical distinctions and yet evolve together?". For example, in 2001 Wolpoff and colleagues published an analysis of character traits of the skulls of early modern human fossils in Australia and central Europe. They concluded that the diversity of these recent humans could not "result exclusively from a single late Pleistocene dispersal", and implied dual ancestry for each region, involving interbreeding with Africans.[38]
Thorne held that there was regional continuity in Indonesia and Australia for a morphological clade.[39][40] This sequence is said to consist of the earliest fossils from Sangiran, Java, that can be traced through Ngandong and found in prehistoric and recent Aboriginal Australians. In 1991, Andrew Kramer tested 17 proposed morphological clade features. He found that: "a plurality (eight) of the seventeen non-metric features link Sangiran to modern Australians" and that these "are suggestive of morphological continuity, which implies the presence of a genetic continuum in Australasia dating back at least one million years"[41] but Colin Groves has criticized Kramer's methodology, pointing out that the polarity of characters was not tested and that the study is actually inconclusive.[42] Phillip Habgood discovered that the characters said to be unique to the Australasian region by Thorne are plesiomorphic:
...it is evident that all of the characters proposed... to be 'clade features' linking Indonesian Homo erectus material with Australian Aboriginal crania are retained primitive features present on Homo erectus and archaic Homo sapiens crania in general. Many are also commonly found on the crania and mandibles of anatomically-modern Homo sapiens from other geographical locations, being especially prevalent on the robust Mesolithic skeletal material from North Africa."[43]
Yet, regardless of these criticisms Habgood (2003) allows for limited regional continuity in Indonesia and Australia, recognizing four plesiomorphic features which do not appear in such a unique combination on fossils in any other region: a sagittally flat frontal bone, with a posterior position of minimum frontal breadth, great facial prognathism, and zygomaxillary tuberosities.[44] This combination, Habgood says, has a "certain Australianness about it".
Wolpoff, initially skeptical of Thorne's claims, became convinced when reconstructing the Sangiran 17 Homo erectus skull from Indonesia, when he was surprised that the skull's face to vault angle matched that of the Australian modern human Kow Swamp 1 skull in excessive prognathism. Durband (2007) in contrast states that "features cited as showing continuity between Sangiran 17 and the Kow Swamp sample disappeared in the new, more orthognathic reconstruction of that fossil that was recently completed".[45] Baba et al. who newly restored the face of Sangiran 17 concluded: "regional continuity in Australasia is far less evident than Thorne and Wolpoff argued".[46]
Xinzhi Wu has argued for a morphological clade in China spanning the Pleistocene, characterized by a combination of 10 features.[47][48] The sequence is said to start with Lantian and Peking Man, traced to Dali, to Late Pleistocene specimens (e.g. Liujiang) and recent Chinese. Habgood in 1992 criticized Wu's list, pointing out that most of the 10 features in combination appear regularly on fossils outside China.[49] He did though note that three combined: a non-depressed nasal root, non-projecting perpendicularly oriented nasal bones and facial flatness are unique to the Chinese region in the fossil record and may be evidence for limited regional continuity. However, according to Chris Stringer, Habgood's study suffered from not including enough fossil samples from North Africa, many of which exhibit the small combination he considered to be region-specific to China.[27]
Facial flatness as a morphological clade feature has been rejected by many anthropologists since it is found on many early African Homo erectus fossils, and is therefore considered plesiomorphic,[50] but Wu has responded that the form of facial flatness in the Chinese fossil record appears distinct to other (i.e. primitive) forms. Toetik Koesbardiati in her PhD thesis "On the Relevance of the Regional Continuity Features of the Face in East Asia" also found that a form of facial flatness is unique to China (i.e. only appears there at high frequency, very rarely elsewhere) but cautions that this is the only available evidence for regional continuity: "Only two features appear to show a tendency as suggested by the Multiregional model: flatness at the upper face expressed by an obtuse nasio-frontal angle and flatness at the middle part of the face expressed by an obtuse zygomaxillay angle".
Shovel-shaped incisors are commonly cited as evidence for regional continuity in China.[51][52] Stringer (1992) however found that shovel-shaped incisors are present on >70% of the early Holocene Wadi Halfa fossil sample from North Africa, and common elsewhere.[53] Frayer, et al. (1993) have criticized Stringer's method of scoring shovel-shaped incisor teeth. They discuss the fact that there are different degrees of "shovelled" e.g. trace (+), semi (++), and marked (+++), but that Stringer misleadingly lumped all these together: "...combining shoveling categories in this manner is biologically meaningless and misleading, as the statistic cannot be validly compared with the very high frequencies for the marked shoveling category reported for East Asians."[33] Palaeoanthropologist Fred H. Smith (2009) also emphasizes that: "It is the pattern of shoveling that identities as an East Asian regional feature, not just the occurrence of shoveling of any sort".[2] Multiregionalists argue that marked (+++) shovel-shaped incisors only appear in China at a high frequency, and have <10% occurrence elsewhere.
Since the early 1990s, David W. Frayer has described what he regards as a morphological clade in Europe.[54][55][56] The sequence starts with the earliest dated Neanderthal specimens (Krapina and Saccopastore skulls) traced through the mid-Late Pleistocene (e.g. La Ferrassie 1) to Vindija Cave, and late Upper Palaeolithic Cro-Magnons or recent Europeans. Although many anthropologists consider Neanderthals and Cro Magnons morphologically distinct,[57][58] Frayer maintains quite the opposite and points to their similarities, which he argues is evidence for regional continuity:
"Contrary to Brauer's recent pronouncement that there is a large and generally recognized morphological gap between the Neanderthals and the early moderns, the actual evidence provided by the extensive fossil record of late Pleistocene Europe shows considerable continuity between Neanderthals and subsequent Europeans."[33]
Frayer et al. (1993) consider there to be at least four features in combination that are unique to the European fossil record: a horizontal-oval shaped mandibular foramen, anterior mastoid tubercle, suprainiac fossa, and narrowing of the nasal breadth associated with tooth-size reduction. Regarding the latter, Frayer observes a sequence of nasal narrowing in Neanderthals, following through to late Upper Palaeolithic and Holocene (Mesolithic) crania. His claims are disputed by others,[59] but have received support from Wolpoff, who regards late Neanderthal specimens to be "transitional" in nasal form between earlier Neanderthals and later Cro Magnons.[60] Based on other cranial similarities, Wolpoff et al. (2004) argue for a sizable Neanderthal contribution to modern Europeans.[61]
More recent claims regarding continuity in skeletal morphology in Europe focus on fossils with both Neanderthal and modern anatomical traits, to provide evidence of interbreeding rather than replacement.[62][63][64] Examples include the Lapedo child found in Portugal[65] and the Oase 1 mandible from Peștera cu Oase, Romania,[66] though the "Lapedo child" is disputed by some.[67]
A 1987 analysis of mitochondrial DNA from 147 people by Cann et al. from around the world indicated that their mitochondrial lineages all coalesced in a common ancestor from Africa between 140,000 and 290,000 years ago.[68] The analysis suggested that this reflected the worldwide expansion of modern humans as a new species, replacing, rather than mixing with, local archaic humans outside of Africa. Such a recent replacement scenario is not compatible with the Multiregional hypothesis and the mtDNA results led to increased popularity for the alternative single replacement theory.[69][70][71] According to Wolpoff and colleagues:[72]
When they were first published, the Mitochondrial Eve results were clearly incongruous with Multiregional evolution, and we wondered how the two could be reconciled.
Multiregionalists have responded to what they see as flaws in the Eve theory,[73] and have offered contrary genetic evidences.[74][75][76] Wu and Thorne have questioned the reliability of the molecular clock used to date Eve.[77][78] Multiregionalists point out that Mitochondrial DNA alone can not rule out interbreeding between early modern and archaic humans, since archaic human mitochondrial strains from such interbreeding could have been lost due to genetic drift or a selective sweep.[79][80] Wolpoff for example states that Eve is "not the most recent common ancestor of all living people" since "Mitochondrial history is not population history".[81]
Neanderthal mitochondrial DNA (mtDNA) sequences from Feldhofer and Vindija Cave are substantially different from modern human mtDNA.[82][83][84] Multiregionalists however have discussed the fact that the average difference between the Feldhofer sequence and living humans is less than that found between chimpanzee subspecies,[85][86] and therefore that while Neanderthals were different subspecies, they were still human and part of the same lineage.
Initial analysis of Y chromosome DNA, which like mitochondrial DNA, is inherited from only one parent, was consistent with a recent African replacement model. However, the mitochondrial and Y chromosome data could not be explained by the same modern human expansion out of Africa; the Y chromosome expansion would have involved genetic mixing that retained regionally local mitochondrial lines. In addition, the Y chromosome data indicated a later expansion back into Africa from Asia, demonstrating that gene flow between regions was not unidirectional.[87]
An early analysis of 15 noncoding sites on the X chromosome found additional inconsistencies with the recent African replacement hypothesis. The analysis found a multimodal distribution of coalescence times to the most recent common ancestor for those sites, contrary to the predictions for recent African replacement; in particular, there were more coalescence times near 2 million years ago (mya) than expected, suggesting an ancient population split around the time humans first emerged from Africa as Homo erectus, rather than more recently as suggested by the mitochondrial data. While most of these X chromosome sites showed greater diversity in Africa, consistent with African origins, a few of the sites showed greater diversity in Asia rather than Africa. For four of the 15 gene sites that did show greater diversity in Africa, the sites' varying diversity by region could not be explained by simple expansion from Africa, as would be required by the recent African replacement hypothesis.[88]
Later analyses of X chromosome and autosomal DNA continued to find sites with deep coalescence times inconsistent with a single origin of modern humans,[89][90][91][92][93] diversity patterns inconsistent with a recent expansion from Africa,[94] or both.[95][96] For example, analyses of a region of RRM2P4 (ribonucleotide reductase M2 subunit pseudogene 4) showed a coalescence time of about 2 Mya, with a clear root in Asia,[97][98] while the MAPT locus at 17q21.31 is split into two deep genetic lineages, one of which is common in and largely confined to the present European population, suggesting inheritance from Neanderthals.[99][100][101][102] In the case of the Microcephalin D allele, evidence for rapid recent expansion indicated introgression from an archaic population.[103][104][105][106] However, later analysis, including of the genomes of Neanderthals, did not find the Microcephalin D allele (in the proposed archaic species), nor evidence that it had introgressed from an archaic lineage as previously suggested.[107][108][109]
In 2001, a DNA study of more than 12,000 men from 163 East Asian regions showed that all of them carry a mutation that originated in Africa about 35,000 to 89,000 years ago and these "data do not support even a minimal in situ hominid contribution in the origin of anatomically modern humans in East Asia".[110]
In a 2005 review and analysis of the genetic lineages of 25 chromosomal regions, Alan Templeton found evidence of more than 34 occurrences of gene flow between Africa and Eurasia. Of these occurrences, 19 were associated with continuous restricted gene exchange through at least 1.46 million years ago; only 5 were associated with a recent expansion from Africa to Eurasia. Three were associated with the original expansion of Homo erectus out of Africa around 2 million years ago, 7 with an intermediate expansion out of Africa at a date consistent with the expansion of Acheulean tool technology, and a few others with other gene flows such as an expansion out of Eurasia and back into Africa subsequent to the most recent expansion out of Africa. Templeton rejected a hypothesis of complete recent African replacement with greater than 99% certainty (p < 10−17).[111]
Recent analyses of DNA taken directly from Neanderthal specimens indicates that they or their ancestors contributed to the genome of all humans outside of Africa, indicating there was some degree of interbreeding with Neanderthals before their replacement.[112] It has also been shown that Denisova hominins contributed to the DNA of Melanesians and Australians through interbreeding.[113]
By 2006, extraction of DNA directly from some archaic human samples was becoming possible. The earliest analyses were of Neanderthal DNA, and indicated that the Neanderthal contribution to modern human genetic diversity was no more than 20%, with a most likely value of 0%.[114] By 2010, however, detailed DNA sequencing of the Neanderthal specimens from Europe indicated that the contribution was nonzero, with Neanderthals sharing 1-4% more genetic variants with living non-Africans than with living humans in sub-Saharan Africa.[115][116] In late 2010, a recently discovered non-Neanderthal archaic human, the Denisova hominin from south-western Siberia, was found to share 4–6% more of its genome with living Melanesian humans than with any other living group, supporting admixture between two regions outside of Africa.[117][118] In August 2011, human leukocyte antigen (HLA) alleles from the archaic Denisovan and Neanderthal genomes were found to show patterns in the modern human population demonstrating origins from these non-African populations; the ancestry from these archaic alleles at the HLA-A site was more than 50% for modern Europeans, 70% for Asians, and 95% for Papua New Guineans.[119] Proponents of the multiregional hypothesis believe the combination of regional continuity inside and outside of Africa and lateral gene transfer between various regions around the world supports the multiregional hypothesis. However, "Out of Africa" Theory proponents also explain this with the fact that genetic changes occur on a regional basis rather than a continental basis, and populations close to each other are likely to share certain specific regional SNPs while sharing most other genes in common.[120][121] Migration Matrix theory (A=Mt) indicates that dependent upon the potential contribution of Neanderthal ancestry, we would be able to calculate the percentage of Neanderthal mtDNA contribution to the human species. As we do not know the specific migration matrix, we are unable to input the exact data, which would answer these questions irrefutably.[85]

---

# Early human migrations

Early human migrations are the earliest migrations and expansions of archaic and modern humans across continents. They are believed to have begun approximately 2 million years ago with the early expansions out of Africa by Homo erectus. This initial migration was followed by other archaic humans including H. heidelbergensis, which lived around 500,000 years ago and was the likely ancestor of Denisovans and Neanderthals as well as modern humans. Early hominids had likely crossed land bridges that have now sunk.
Within Africa, Homo sapiens dispersed around the time of its speciation, roughly 300,000 years ago.[note 1] The recent African origin theory suggests that the anatomically modern humans outside of Africa descend from a population of Homo sapiens migrating from East Africa roughly 70–50,000 years ago and spreading along the southern coast of Asia and to Oceania by about 50,000 years ago. Modern humans spread across Europe about 40,000 years ago.
Early Eurasian Homo sapiens fossils have been found in Apidima Cave (Greece) and Misliya Cave (Israel), dated to 210,000 and 194,000–177,000 years old respectively. These fossils seem to represent failed dispersal attempts by early Homo sapiens, who were likely replaced by local Neanderthal populations.[3][4][5]
The migrating modern human populations are known to have interbred with earlier local populations, so that contemporary human populations are descended in small part (below 10% contribution) from regional varieties of archaic humans.[note 2]
After the Last Glacial Maximum, North Eurasian populations migrated to the Americas about 20,000 years ago.[9][10] Arctic Canada and Greenland were reached by the Paleo-Eskimo expansion around 4,000 years ago. Finally, Polynesia was populated within the past 2,000 years in the last wave of the Austronesian expansion.
The earliest humans developed out of australopithecine ancestors about 3 million years ago, most likely in the area of the Kenyan Rift Valley, where the oldest known stone tools have been found. Stone tools recently discovered at the Shangchen site in China and dated to 2.12 million years ago are claimed to be the earliest known evidence of hominins outside Africa, surpassing Dmanisi in Georgia by 300,000 years.[11]
Between 2 and less than a million years ago, Homo spread throughout East Africa and to Southern Africa (Homo ergaster), but not yet to West Africa. Around 1.8 million years ago, Homo erectus migrated out of Africa via the Levantine corridor and Horn of Africa to Eurasia. This migration has been proposed as being related to the operation of the Saharan pump, around 1.9 million years ago.[citation needed] Homo erectus dispersed throughout most of the Old World, reaching as far as Southeast Asia. Its distribution is traced by the Oldowan lithic industry, by 1.3 million years ago extending as far north as the 40th parallel (Xiaochangliang).
Key sites for this early migration out of Africa are Riwat in Pakistan (~2 Ma?[12]), Ubeidiya in the Levant (1.5 Ma) and Dmanisi in the Caucasus (1.81 ± 0.03 Ma, p=0.05[13]).
China shows evidence of Homo erectus from 2.12 mya in Gongwangling, in Lantian county.[14] Two Homo erectus incisors have been found near Yuanmou, southern China, and are dated to 1.7 mya, and a cranium from Lantian has been dated to 1.63 mya. Artefacts from Majuangou III and Shangshazui in the Nihewan basin, northern China, have been dated to 1.6–1.7 mya.[14][15] The archaeological site of Xihoudu (西侯渡) in Shanxi province is the earliest recorded use of fire by Homo erectus, which is dated 1.27 million years ago.[16]
Southeast Asia (Java) was reached about 1.7 million years ago (Meganthropus). Western Europe was first populated around 1.2 million years ago (Atapuerca).[17]
Robert G. Bednarik has suggested that Homo erectus may have built rafts and sailed oceans, a theory that has raised some controversy.[18]
One million years after its dispersal, H. erectus was diverging into new species. H. erectus is a chronospecies and was never extinct, so its "late survival" is a matter of taxonomic convention. Late forms of H. erectus are thought to have survived until after about 0.5 million ago to 143,000 years ago at the latest,[note 3] with derived forms classified as H. antecessor in Europe around 800,000 years ago and H. heidelbergensis in Africa around 600,000 years ago. H. heidelbergensis in its turn spread across East Africa (H. rhodesiensis) and to Eurasia, where it gave rise to Neanderthals and Denisovans.
H. heidelbergensis, Neanderthals and Denisovans expanded north beyond the 50th parallel (Eartham Pit, Boxgrove 500kya, Swanscombe Heritage Park 400kya, Denisova Cave 50 kya). It has been suggested that late Neanderthals may even have reached the boundary of the Arctic, by c. 32,000 years ago, when they were being displaced from their earlier habitats by H. sapiens, based on 2011 excavations at the site of Byzovaya in the Urals (Komi Republic, 65°01′N 57°25′E﻿ / ﻿65.02°N 57.42°E﻿ / 65.02; 57.42).[20]
Other archaic human species are assumed to have spread throughout Africa by this time, although the fossil record is sparse. Their presence is assumed based on traces of admixture with modern humans found in the genome of African populations.[8][21][22][23] Homo naledi, discovered in South Africa in 2013 and tentatively dated to about 300,000 years ago, may represent fossil evidence of such an archaic human species.[24]
Neanderthals spread across the Near East and Europe, while Denisovans appear to have spread across Central and East Asia and to Southeast Asia and Oceania. There is evidence that Denisovans interbred with Neanderthals in Central Asia where their habitats overlapped.[25] Neanderthal evidence has also been found quite late at 33,000 years ago at the 65th latitude of the Byzovaya site in the Ural Mountains. This is far outside of any otherwise known habitat, during a high ice cover period, and perhaps reflects a refugia of near extinction.
Homo sapiens are believed to have emerged in Africa about 300,000 years ago, based in part on thermoluminescence dating of artifacts and remains from Jebel Irhoud, Morocco, published in 2017.[note 4][27] The Florisbad Skull from Florisbad, South Africa, dated to about 259,000 years ago, has also been classified as early Homo sapiens.[28][29][30][31] Previously, the Omo remains, excavated between 1967 and 1974 in Omo National Park, Ethiopia, and dated to 200,000 years ago, were long held to be the oldest known fossils of Homo sapiens.[32]
In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to anatomically modern humans, representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.[33][34]
In July 2019, anthropologists reported the discovery of 210,000 year old remains of a H. sapiens and 170,000 year old remains of a H. neanderthalensis in Apidima Cave in southern Greece, more than 150,000 years older than previous H. sapiens finds in Europe.[35][36][37][38]
Early modern humans expanded to Western Eurasia and Central, Western and Southern Africa from the time of their emergence. While early expansions to Eurasia appear not to have persisted,[39][25] expansions to Southern and Central Africa resulted in the deepest temporal divergence in living human populations. Early modern human expansion in sub-Saharan Africa appears to have contributed to the end of late Acheulean (Fauresmith) industries at about 130,000 years ago, although very late coexistence of archaic and early modern humans, until as late as 12,000 years ago, has been argued for West Africa in particular.[40]
The ancestors of the modern Khoi-San expanded to Southern Africa before 150,000 years ago, possibly as early as before 260,000 years ago,[note 5] so that by the beginning of the MIS 5 "megadrought", 130,000 years ago, there were two ancestral population clusters in Africa, bearers of mt-DNA haplogroup L0 in southern Africa, ancestral to the Khoi-San, and bearers of haplogroup L1-6 in central/eastern Africa, ancestral to everyone else. There was a significant back-migration of bearers of L0 towards eastern Africa between 120 and 75 kya.[note 6]
Expansion to Central Africa by the ancestors of the Central African forager populations (African Pygmies) most likely took place before 130,000 years ago, and certainly before 60,000 years ago.[42][43][44][45][note 7] Wet forest environments were not a major ecological barrier for Homo sapiens as early as around 150,000 years ago.[47][non-primary source needed][48]
The situation in West Africa is difficult to interpret due to a scarcity of fossil evidence. Homo sapiens seems to have reached the western Sahelian zone by 130,000 years ago, while tropical West African sites associated with H. sapiens are known only from after 130,000 years ago. Unlike elsewhere in Africa, archaic Middle Stone Age sites appear to persist until very late, down to the Holocene boundary (12,000 years ago), pointing to the possibility of late survival of archaic humans, and late hybridization with H. sapiens in West Africa.[40]
Populations of Homo sapiens migrated to the Levant and to Europe[dubious – discuss] between 130,000 and 115,000 years ago, and possibly in earlier waves as early as 185,000 years ago.[note 8]
A fragment of a jawbone with eight teeth found at Misliya Cave has been dated to around 185,000 years ago. Layers dating from between 250,000 and 140,000 years ago in the same cave contained tools of the Levallois type which could put the date of the first migration even earlier if the tools can be associated with the modern human jawbone finds.[49][50]
These early migrations do not appear to have led to lasting colonisation and receded by about 80,000 years ago.[25] There is a possibility that this first wave of expansion may have reached China (or even North America[dubious – discuss][51]) as early as 125,000 years ago, but would have died out without leaving a trace in the genome of contemporary humans.[25]
There is some evidence that modern humans left Africa at least 125,000 years ago using two different routes: through the Nile Valley, the Sinai Peninsula and the Levant (Qafzeh Cave: 120,000–100,000 years ago); and a second route through the present-day Bab-el-Mandeb Strait on the Red Sea (at that time, with a much lower sea level and narrower extension), crossing to the Arabian Peninsula[52][53] and settling in places like the present-day United Arab Emirates (125,000 years ago)[54] and Oman (106,000 years ago),[55] and possibly reaching the Indian Subcontinent (Jwalapuram: 75,000 years ago.) Although no human remains have yet been found in these three places, the apparent similarities between the stone tools found at Jebel Faya, those from Jwalapuram and some from Africa suggest that their creators were all modern humans.[56] These findings might give some support to the claim that modern humans from Africa arrived at southern China about 100,000 years ago (Zhiren Cave, Zhirendong, Chongzuo City: 100,000 years ago;[note 9] and the Liujiang hominid (Liujiang County): controversially dated at 139,000–111,000 years ago [61]). Dating results of the Lunadong (Bubing Basin, Guangxi, southern China) teeth, which include a right upper second molar and a left lower second molar, indicate that the molars may be as old as 126,000 years.[62][63]
Since these previous exits from Africa did not leave traces in the results of genetic analyses based on the Y chromosome and on MtDNA, it seems that those modern humans did not survive in large numbers and were assimilated by our major antecessors. An explanation for their extinction (or small genetic imprint) may be the Toba eruption (74,000 years ago), though some argue it scarcely affected human population.[64]
The so-called "recent dispersal" of modern humans took place about 70–50,000 years ago.[65][66][67] It is this migration wave that led to the lasting spread of modern humans throughout the world.
A small group from a population in East Africa, bearing mitochondrial haplogroup L3 and numbering possibly fewer than 1,000 individuals,[68][69] crossed the Red Sea strait at Bab-el-Mandeb, to what is now Yemen, after around 75,000 years ago.[70] A recent review has also shown support for the northern route through the Sinai Peninsula and the Levant.[25] Their descendants spread along the coastal route around Arabia and Persia to South Asia before 55,000 years ago. Other research supports a migration out of Africa between about 65,000 and 50,000 years ago.[65][71][67] The coastal migration between roughly 70,000 and 50,000 years ago is associated with mitochondrial haplogroups M and N, both derivative of L3.
Along the way H. sapiens interbred with Neanderthals and Denisovans,[72] with Denisovan DNA making 0.2% of mainland Asian and Native American DNA.[73]
Migrations continued along the Asian coast to Southeast Asia and Oceania, colonising Australia by around 65,000–50,000 years ago.[74][75][76] By reaching Australia, H. sapiens for the first time expanded its habitat beyond that of H. erectus. Denisovan ancestry is shared by Melanesians, Aboriginal Australians, and smaller scattered groups of people in Southeast Asia, such as the Mamanwa, a Negrito people in the Philippines, suggesting the interbreeding took place in Eastern Asia where the Denisovans lived.[77][78][79] Denisovans may have crossed the Wallace Line, with Wallacea serving as their last refugium.[80][81] Homo erectus had crossed the Lombok gap reaching as far as Flores, but never made it to Australia.[82]
During this time sea level was much lower and most of Maritime Southeast Asia formed one land mass known as Sunda. Migration continued Southeast on the coastal route to the straits between Sunda and Sahul, the continental land mass of present-day Australia and New Guinea. The gaps on the Weber Line are up to 90 km wide,[83] so the migration to Australia and New Guinea would have required seafaring skills. Migration also continued along the coast eventually turning northeast to China and finally reaching Japan before turning inland. This is evidenced by the pattern of mitochondrial haplogroups descended from haplogroup M, and in Y-chromosome haplogroup C.
Sequencing of one Aboriginal genome from an old hair sample in Western Australia revealed that the individual was descended from people who migrated into East Asia between 62,000 and 75,000 years ago. This supports the theory of a single migration into Australia and New Guinea before the arrival of Modern Asians (between 25,000 and 38,000 years ago) and their later migration into North America.[84] This migration is believed to have happened around 50,000 years ago, before Australia and New Guinea were separated by rising sea levels approximately 8,000 years ago.[85][86] This is supported by a date of 50,000–60,000 years ago for the oldest evidence of settlement in Australia,[74][87] around 40,000 years ago for the oldest human remains,[74] the earliest humans artifacts which are at least 65,000 years old[88] and the extinction of the Australian megafauna by humans between 46,000 and 15,000 years ago argued by Tim Flannery,[89] which is similar to what happened in the Americas. The continued use of Stone Age tools in Australia has been much debated.[90]
The population brought to South Asia by coastal migration appears to have remained there for some time, during roughly 60,000 to 50,000 years ago, before spreading further throughout Eurasia. This dispersal of early humans, at the beginning of the Upper Paleolithic, gave rise to the major population groups of the Old World and the Americas.
Towards the West, Upper Paleolithic populations associated with mitochondrial haplogroup R and its derivatives, spread throughout Asia and Europe, with a back-migration of M1 to North Africa and the Horn of Africa several millennia ago. [dubious – discuss]
Presence in Europe is certain after 40,000 years ago, possibly as early as 43,000 years ago,[91] rapidly replacing the Neanderthal population. Contemporary Europeans have Neanderthal ancestry, but it seems likely that substantial interbreeding with Neanderthals ceased before 47,000 years ago, i.e. took place before modern humans entered Europe.[92]
There is evidence from mitochondrial DNA that modern humans have passed through at least one genetic bottleneck, in which genome diversity was drastically reduced. Henry Harpending has proposed that humans spread from a geographically restricted area about 100,000 years ago, the passage through the geographic bottleneck and then with a dramatic growth amongst geographically dispersed populations about 50,000 years ago, beginning first in Africa and thence spreading elsewhere.[93] Climatological and geological evidence suggests evidence for the bottleneck. The explosion of Toba, the largest volcanic eruption of the Quaternary, may have created a 1,000 year cold period, potentially reducing human populations to a few tropical refugia. It has been estimated that as few as 15,000 humans survived. In such circumstances genetic drift and founder effects may have been maximised. The greater diversity amongst African genomes may reflect the extent of African refugia during the Toba incident.[94] However, a recent review highlights that the single-source hypothesis of non-African populations is less consistent with ancient DNA analysis than multiple sources with genetic mixing across Eurasia.[25]
The recent expansion of anatomically modern humans reached Europe around 40,000 years ago from Central Asia and the Middle East, as a result of cultural adaption to big game hunting of sub-glacial steppe fauna.[95] Neanderthals were present both in the Middle East and in Europe, and the arriving populations of anatomically modern humans (also known as "Cro-Magnon" or European early modern humans) interbred with Neanderthal populations to a limited degree. Populations of modern humans and Neanderthal overlapped in various regions such as the Iberian peninsula and the Middle East. Interbreeding may have contributed Neanderthal genes to palaeolithic and ultimately modern Eurasians and Oceanians.
An important difference between Europe and other parts of the inhabited world was the northern latitude. Archaeological evidence suggests humans, whether Neanderthal or Cro-Magnon, reached sites in Arctic Russia by 40,000 years ago.[96]
Cro-Magnon are considered the first anatomically modern humans in Europe. They entered Eurasia by the Zagros Mountains (near present-day Iran and eastern Turkey) around 50,000 years ago, with one group rapidly settling coastal areas around the Indian Ocean and another migrating north to the steppes of Central Asia.[97] Modern human remains dating to 45,000-47,000 have been found in Germany,[98] while finds of 43,000–45,000 years ago have been discovered in Italy[99] and Britain,[100] as well as in the European Russian Arctic from 40,000 years ago.[96][101]
Humans colonised the environment west of the Urals, hunting reindeer especially,[102] but were faced with adaptive challenges; winter temperatures averaged from −20 to −30 °C (−4 to −22 °F) with fuel and shelter scarce. They travelled on foot and relied on hunting highly mobile herds for food. These challenges were overcome through technological innovations: tailored clothing from the pelts of fur-bearing animals; construction of shelters with hearths using bones as fuel; and digging "ice cellars" into the permafrost to store meat and bones.[102][103]
However, from recent research it is believed that the ecological crisis resulting from the eruption in c. 38,000 BCE of the super-volcano in the Phlegrean Fields near Naples, which left much of eastern Europe covered in ash, wiped out both the last Neanderthal and the first Homo Sapiens populations of the early Upper Paleolithic.[104][105] Modern Europeans of today bear no trace of the genomes of the first Homo Sapiens Europeans, but only of those from after the ecological crisis of 38,000 BCE.[106] Modern humans then repopulated Europe from the east after the eruption and the ice age that took place from 38,000 to 36,000 BCE.[107]
A mitochondrial DNA sequence of two Cro-Magnons from the Paglicci Cave in Italy, dated to 23,000 and 24,000 years old (Paglicci 52 and 12), identified the mtDNA as Haplogroup N, typical of the latter group.[108]
The expansion of modern human population is thought to have begun 45,000 years ago, and it may have taken 15,000–20,000 years for Europe to be colonized.[110][111]
During this time, the Neanderthals were slowly being displaced. Because it took so long for Europe to be occupied, it appears that humans and Neanderthals may have been constantly competing for territory. The Neanderthals had larger brains, and were larger overall, with a more robust or heavily built frame, which suggests that they were physically stronger than modern Homo sapiens. Having lived in Europe for 200,000 years, they would have been better adapted to the cold weather. The anatomically modern humans known as the Cro-Magnons, with widespread trade networks, superior technology and bodies likely better suited to running, would eventually completely displace the Neanderthals, whose last refuge was in the Iberian Peninsula. Neanderthals disappeared about 40,000 years ago.[112]
From the extent of linkage disequilibrium, it was estimated that the last Neanderthal gene flow into early ancestors of Europeans occurred 47,000–65,000 years BP. In conjunction with archaeological and fossil evidence, interbreeding is thought to have occurred somewhere in Western Eurasia, possibly the Middle East.[92] Studies show a higher Neanderthal admixture in East Asians than in Europeans.[113][114] North African groups share a similar excess of derived alleles with Neanderthals as non-African populations, whereas Sub-Saharan African groups are the only modern human populations with no substantial Neanderthal admixture.[note 10] The Neanderthal-linked haplotype B006 of the dystrophin gene has also been found among nomadic pastoralist groups in the Sahel and Horn of Africa, who are associated with northern populations. Consequently, the presence of this B006 haplotype on the northern and northeastern perimeter of Sub-Saharan Africa is attributed to gene flow from a non-African point of origin.[note 11]
"Tianyuan man", an individual who lived in China c. 40,000 years ago, showed substantial Neanderthal admixture. A 2017 study of the ancient DNA of Tianyuan Man found that the individual is related to modern Asian and Native American populations.[118] A 2013 study found Neanderthal introgression of 18 genes within the chromosome 3p21.31 region (HYAL region) of East Asians. The introgressive haplotypes were positively selected in only East Asian populations, rising steadily from 45,000 years ago until a sudden increase of growth rate around 5,000 to 3,500 years ago. They occur at very high frequencies among East Asian populations in contrast to other Eurasian populations (e.g. European and South Asian populations). The findings also suggest that this Neanderthal introgression occurred within the ancestral population shared by East Asians and Native Americans.[119]
A 2016 study presented an analysis of the population genetics of the Ainu people of northern Japan as key to the reconstruction of the early peopling of East Asia. The Ainu were found to represent a more basal branch than the modern farming populations of East Asia, suggesting an ancient (pre-Neolithic) connection with northeast Siberians.[120] A 2013 study associated several phenotypical traits associated with Mongoloids with a single mutation of the EDAR gene, dated to c. 35,000 years ago.[note 12][note 13]
Mitochondrial haplogroups A, B and G originated about 50,000 years ago, and bearers subsequently colonized Siberia, Korea and Japan, by about 35,000 years ago. Parts of these populations migrated to North America during the Last Glacial Maximum. Indeed, the Last Glacial Maximum promoted range contractions toward southern regions, followed by posterior range re-expansions toward the north, in North Asia populations that shaped their spatial genetic gradients.[124]
A review paper by Melinda A. Yang (in 2022) summarized and concluded that a distinctive "Basal-East Asian population" referred to as 'East- and Southeast Asian lineage' (ESEA); which is ancestral to modern East Asians, Southeast Asians, Polynesians, and Siberians, originated in Mainland Southeast Asia at ~50,000BC, and expanded through multiple migration waves southwards and northwards respectively. This ESEA lineage gave rise to various sublineages, and is also ancestral to the Hoabinhian hunter-gatherers of Southeast Asia and the ~40,000 year old Tianyuan lineage found in Northern China, but already differentiated and distinct from European-related and Australasian-related lineages, found in other regions of prehistoric Eurasia. The ESEA lineage trifurcated from an earlier East-Eurasian or "eastern non-African" (ENA) meta-population, which also contributed to the formation of Ancient Ancestral South Indians (AASI) as well as to Australasians.[125]
Around 20,000 years ago, approximately 5,000 years after the Neanderthal extinction, the Last Glacial Maximum forced northern hemisphere inhabitants to migrate to several shelters (refugia) until the end of this period. The resulting populations are presumed to have resided in such refuges during the LGM to ultimately reoccupy Europe, where archaic historical populations are considered their descendants. The composition of European populations was later altered by further migrations, notably the Neolithic expansion from the Middle East, and still later the Chalcolithic population movements associated with Indo-European expansion, as well as admixture with diverse populations from North Africa.[126] A Paleolithic site on the Yana River, Siberia, at 71°N, lies well above the Arctic Circle and dates to 27,000 radiocarbon years before present, during glacial times. This site shows that people adapted to this harsh, high-latitude, Late Pleistocene environment much earlier than previously thought.[127]
Paleo-Indians originated from Central Asia, crossing the Beringia land bridge between eastern Siberia and present-day Alaska.[128] Humans lived throughout the Americas by the end of the last glacial period, or more specifically what is known as the late glacial maximum.[128][129][130][131] Details of Paleo-Indian migration to and throughout the American continent, including the dates and the routes traveled, are subject to ongoing research and discussion.[132]
Conventional estimates have it that humans reached North America at some point between 15,000 and 20,000 years ago.[133][134][135][136] The traditional theory is that these early migrants moved when sea levels were significantly lowered due to the Quaternary glaciation,[129][132] following herds of now-extinct pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets.[137] Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America as far as Chile.[138] Any archaeological evidence of coastal occupation during the last Ice Age would now have been covered by the sea level rise, up to a hundred metres since then.[139] The recent finding of indigenous Australasian genetic markers in Amazonia supports that a coastal route and subsequent isolation did occur with some migrants.[140]
The Holocene is taken to begin 12,000 years ago, after the end of the Last Glacial Maximum. During the Holocene climatic optimum, beginning about 9,000 years ago, human populations which had been geographically confined to refugia began to migrate. By this time, most parts of the globe had been settled by H. sapiens; however, large areas that had been covered by glaciers were now re-populated.
This period sees the transition from the Mesolithic to the Neolithic stage throughout the temperate zone. The Neolithic subsequently gives way to the Bronze Age in Old World cultures and the gradual emergence of the historical record in the Near East and China beginning around 4,000 years ago.
Large-scale migrations of the Mesolithic to Neolithic era are thought to have given rise to the pre-modern distribution of the world's major language families such as the Niger-Congo, Nilo-Saharan, Afro-Asiatic, Uralic, Sino-Tibetan or Indo-European phyla. The speculative Nostratic theory postulates the derivation of the major language families of Eurasia (excluding Sino-Tibetan) from a single proto-language spoken at the beginning of the Holocene period.
Evidence published in 2014 from genome analysis of ancient human remains suggests that the modern native populations of Europe largely descend from three distinct lineages: "Western Hunter-Gatherers", derivative of the Cro-Magnon population of Europe, Early European Farmers introduced to Europe from the Near East during the Neolithic Revolution and Ancient North Eurasians who expanded to Europe in the context of the Indo-European expansion.[142] The Ancient North Eurasian component was introduced to Western Europe by people related to the Yamnaya culture.[143] Additional ANE ancestry is found in European populations through Paleolithic interactions with Eastern Hunter-Gatherers.[144]
West-Eurasian back-migrations started in the early Holocene or already earlier in the Paleolithic period (30-15kya), followed by pre-Neolithic and Neolithic migration events from the Middle East, mostly affecting Northern Africa, the Horn of Africa, and wider regions of the Sahel zone and East Africa.[145]
The Nilotic peoples are thought to be derived from an earlier undifferentiated Eastern Sudanic unity by the 3rd millennium BCE. The development of the Proto-Nilotes as a group may have been connected with their domestication of livestock. The Eastern Sudanic unity must have been considerably earlier still, perhaps around the 5th millennium BCE (while the proposed Nilo-Saharan unity would date to the Upper Paleolithic about 15kya). The original locus of the early Nilotic speakers was presumably east of the Nile in what is now South Sudan. The Proto-Nilotes of the 3rd millennium BCE were pastoralists, while their neighbors, the Proto-Central Sudanic peoples, were mostly agriculturalists.[146]
The Niger-Congo phylum is thought to have emerged around 6,000 years ago in West or Central Africa. Its expansion may have been associated with the expansion of Sahel agriculture in the African Neolithic period, following the desiccation of the Sahara in c. 3900 BCE.[147] The Bantu expansion has spread the Bantu languages to Central, Eastern and Southern Africa, partly replacing the indigenous populations of these regions, including the African Pygmies, Hadza people and San people. Beginning about 3,000 years ago, it reached South Africa about 1,700 years ago.[148]
Some evidence (including a 2016 study by Busby et al.) suggests admixture from ancient and recent migrations from Eurasia into parts of Sub-Saharan Africa.[149] Another study (Ramsay et al. 2018) also shows evidence that ancient Eurasians migrated into Africa and that Eurasian admixture in modern Sub-Saharan Africans ranges from 0% to 50%, varying by region and generally higher in the Horn of Africa and parts of the Sahel zone, and found to a lesser degree in certain parts of Western Africa, and Southern Africa (excluding recent immigrants).[150]
The first seaborne human migrations were by the Austronesian peoples [dubious – discuss] originating from Taiwan known as the "Austronesian expansion".[151] Using advanced sailing technologies like catamarans, outrigger boats, and crab claw sails, they built the first sea-going ships and rapidly colonized Island Southeast Asia at around 3000 to 1500 BCE. From the Philippines and Eastern Indonesia they colonized Micronesia by 2200 to 1000 BCE.[151][152]
A branch of the Austronesians reached Island Melanesia between 1600 and 1000 BCE, establishing the Lapita culture (named after the archaeological site in Lapita, New Caledonia, where their characteristic pottery was first discovered). They are the direct ancestors of the modern Polynesians. They ventured into Remote Oceania reaching Vanuatu, New Caledonia, and Fiji by 1200 BCE, and Samoa and Tonga by around 900 to 800 BCE. This was the furthest extent of the Lapita culture expansion. During a period of around 1,500 years, they gradually lost the technology for pottery (likely due to the lack of clay deposits in the islands), replacing it with carved wooden and bamboo containers. Back-migrations from the Lapita culture also merged back Island Southeast Asia in 1500 BCE, and into Micronesia at around 200 BCE. It was not until 700 CE when they started voyaging further into the Pacific Ocean, when they colonized the Cook Islands, the Society Islands, and the Marquesas. From there, they further colonized Hawaii by 900 CE, Rapa Nui by 1000 CE, and New Zealand by 1200 CE.[152][153][154]
In the Indian Ocean, Austronesians from Borneo also colonized Madagascar and the Comoros Islands by around 500 CE. Austronesians remain the dominant ethnolinguistic group of the islands of the Indo-Pacific, and were the first to establish a maritime trade network reaching as far west as East Africa and the Arabian Peninsula. They assimilated earlier Pleistocene to early Holocene human overland migrations through Sundaland like the Papuans and the Negritos in Island Southeast Asia.[151][152] The Austronesian expansion was the last and the most far-reaching Neolithic human migration event.[155]
The Caribbean was one of the last places in the Americas that were settled by humans. The oldest remains are known from the Greater Antilles (Cuba and Hispaniola) dating between 4000 and 3500 BCE, and comparisons between tool-technologies suggest that these peoples moved across the Yucatán Channel from Central America. All evidence suggests that later migrants from 2000 BCE and onwards originated from South America, via the Orinoco region.[156] The descendants of these migrants include the ancestors of the Taíno and Kalinago (Island Carib) peoples.[157]
The earliest inhabitants of North America's central and eastern Arctic are referred to as the Arctic small tool tradition (AST) and existed c. 2500 BCE. AST consisted of several Paleo-Eskimo cultures, including the Independence cultures and Pre-Dorset culture.[158][159]
The Inuit are the descendants of the Thule culture, which emerged from western Alaska around CE 1000 and gradually displaced the Dorset culture.[160][161]

---

# Recent human evolution

Recent human evolution refers to evolutionary adaptation, sexual and natural selection, and genetic drift within Homo sapiens populations, since their separation and dispersal in the Middle Paleolithic about 50,000 years ago. Contrary to popular belief, not only are humans still evolving, their evolution since the dawn of agriculture is faster than ever before.[1][2][3][4] It has been proposed that human culture acts as a selective force in human evolution and has accelerated it;[5] however, this is disputed.[6][7] With a sufficiently large data set and modern research methods, scientists can study the changes in the frequency of an allele occurring in a tiny subset of the population over a single lifetime, the shortest meaningful time scale in evolution.[8] Comparing a given gene with that of other species enables geneticists to determine whether it is rapidly evolving in humans alone. For example, while human DNA is on average 98% identical to chimp DNA, the so-called Human Accelerated Region 1 (HAR1), involved in the development of the brain, is only 85% similar.[2]
Following the peopling of Africa some 130,000 years ago, and the recent Out-of-Africa expansion some 70,000 to 50,000 years ago, some sub-populations of Homo sapiens have been geographically isolated for tens of thousands of years prior to the early modern Age of Discovery. Combined with archaic admixture, this has resulted in relatively significant genetic variation. Selection pressures were especially severe for populations affected by the Last Glacial Maximum (LGM) in Eurasia, and for sedentary farming populations since the Neolithic, or New Stone Age.[9]
Single nucleotide polymorphisms (SNP, pronounced 'snip'), or mutations of a single genetic code "letter" in an allele that spread across a population, in functional parts of the genome can potentially modify virtually any conceivable trait, from height and eye color to susceptibility to diabetes and schizophrenia. Approximately 2% of the human genome codes for proteins and a slightly larger fraction is involved in gene regulation. But most of the rest of the genome has no known function. If the environment remains stable, the beneficial mutations will spread throughout the local population over many generations until it becomes a dominant trait. An extremely beneficial allele could become ubiquitous in a population in as little as a few centuries whereas those that are less advantageous typically take millennia.[10]
Human traits that emerged recently include the ability to free-dive for long periods of time,[11] adaptations for living in high altitudes where oxygen concentrations are low,[2] resistance to contagious diseases (such as malaria),[12] light skin,[13] blue eyes,[14] lactase persistence (or the ability to digest milk after weaning),[15][16] lower blood pressure and cholesterol levels,[17][18] retention of the median artery,[19] reduced prevalence of Alzheimer's disease,[8] lower susceptibility to diabetes,[20] genetic longevity,[20] shrinking brain sizes,[21][22] and changes in the timing of menarche and menopause.[23]
Genetic evidence suggests that a species dubbed Homo heidelbergensis is the last common ancestor of Neanderthals, Denisovans, and Homo sapiens. This common ancestor lived between 600,000 and 750,000 years ago, likely in either Europe or Africa. Members of this species migrated throughout Europe, the Middle East, and Africa and became the Neanderthals in Western Asia and Europe while another group moved further east and evolved into the Denisovans, named after the Denisova Cave in Russia where the first known fossils of them were discovered. In Africa, members of this group eventually became anatomically modern humans. Migrations and geographical isolation notwithstanding, the three descendant groups of Homo heidelbergensis later met and interbred.[24]
Archaeological research suggests that as prehistoric humans swept across Europe 45,000 years ago, Neanderthals went extinct. Even so, there is evidence of interbreeding between the two groups as humans expanded their presence in the continent. While prehistoric humans carried 3–6% Neanderthal DNA, modern humans have only about 2%. This seems to suggest selection against Neanderthal-derived traits.[26] For example, the neighborhood of the gene FOXP2, affecting speech and language, shows no signs of Neanderthal inheritance whatsoever.[27]
Introgression of genetic variants acquired by Neanderthal admixture has different distributions in Europeans and East Asians, pointing to differences in selective pressures.[28] Though East Asians inherit more Neanderthal DNA than Europeans,[27] East Asians, South Asians, Australo-Melanesians, Native Americans, and Europeans all share Neanderthal DNA, so hybridization likely occurred between Neanderthals and their common ancestors coming out of Africa.[29] Their differences also suggest separate hybridization events for the ancestors of East Asians and other Eurasians.[27]
Following the genome sequencing of three Vindija Neanderthals, a draft sequence of the Neanderthal genome was published and revealed that Neanderthals shared more alleles with Eurasian populations—such as French, Han Chinese, and Papua New Guinean—than with sub-Saharan African populations, such as Yoruba and San. According to the authors of the study, the observed excess of genetic similarity is best explained by recent gene flow from Neanderthals to modern humans after the migration out of Africa.[30] But gene flow did not go one way. The fact that some of the ancestors of modern humans in Europe migrated back into Africa means that modern Africans also carry some genetic materials from Neanderthals. In particular, Africans share 7.2% Neanderthal DNA with Europeans but only 2% with East Asians.[29]
Some climatic adaptations, such as high-altitude adaptation in humans, are thought to have been acquired by archaic admixture. An ethnic group known as the Sherpas from Nepal is believed to have inherited an allele called EPAS1, which allows them to breathe easily at high altitudes, from the Denisovans.[24] A 2014 study reported that Neanderthal-derived variants found in East Asian populations showed clustering in functional groups related to immune and haematopoietic pathways, while European populations showed clustering in functional groups related to the lipid catabolic process.[note 1] A 2017 study found correlation of Neanderthal admixture in modern European populations with traits such as skin tone, hair color, height, sleeping patterns, mood and smoking addiction.[31] A 2020 study of Africans unveiled Neanderthal haplotypes, or alleles that tend to be inherited together, linked to immunity and ultraviolet sensitivity.[29]
The gene microcephalin (MCPH1), involved in the development of the brain, likely originated from a Homo lineage separate from that of anatomically modern humans, but was introduced to them around 37,000 years ago, and has become much more common ever since, reaching around 70% of the human population at present. Neanderthals were suggested as one possible origin of this gene.[32] But later studies did not find this gene in the Neanderthal genome[33][34] nor has it been found to be associated with cognitive ability in modern people.[35][36][37]
The promotion of beneficial traits acquired from admixture is known as adaptive introgression.[29]
A study concluded only 1.5–7% of "regions" of the modern human genome to be specific to modern humans. These regions have neither been altered by archaic hominin DNA due to admixture (only a small portion of archaic DNA is inherited per individual but a large portion is inherited across populations overall) nor are shared with Neanderthals or Denisovans in any of the genomes of the used datasets. They also found two bursts of changes specific to modern human genomes which involve genes related to brain development and function.[38][39]
Victorian naturalist Charles Darwin was the first to propose the out-of-Africa hypothesis for the peopling of the world,[40] but the story of prehistoric human migration is now understood to be much more complex thanks to twenty-first-century advances in genomic sequencing.[40][41][42] There were multiple waves of dispersal of anatomically modern humans out of Africa,[43][44][45] with the most recent one dating back to 70,000 to 50,000 years ago.[46][47][48][49] Earlier waves of human migrants might have gone extinct or returned to Africa.[45][50] Moreover, a combination of gene flow from Eurasia back into Africa and higher rates of genetic drift among East Asians compared to Europeans led these human populations to diverge from one another at different times.[40]
Around 65,000 to 50,000 years ago, a variety of new technologies, such as projectile weapons, fish hooks, porcelain, and sewing needles, made their appearance.[51] Bird-bone flutes were invented 30,000 to 35,000 years ago,[52] indicating the arrival of music.[51] Artistic creativity also flowered, as can be seen with Venus figurines and cave paintings.[51] Cave paintings of not just actual animals but also imaginary creatures that could reliably be attributed to Homo sapiens have been found in different parts of the world. Radioactive dating suggests that the oldest of the ones that have been found, as of 2019, are 44,000 years old.[53] For researchers, these artworks and inventions represent a milestone in the evolution of human intelligence, the roots of story-telling, paving the way for spirituality and religion.[51][53] Experts believe this sudden "great leap forward"—as anthropologist Jared Diamond calls it—was due to climate change. Around 60,000 years ago, during the middle of an ice age, it was extremely cold in the far north, but ice sheets sucked up much of the moisture in Africa, making the continent even drier and droughts much more common. The result was a genetic bottleneck, pushing Homo sapiens to the brink of extinction, and a mass exodus from Africa. Nevertheless, it remains uncertain (as of 2003) whether or not this was due to some favorable genetic mutations, for example in the FOXP2 gene, linked to language and speech.[54] A combination of archaeological and genetic evidence suggests that humans migrated along Southern Asia and down to Australia 50,000 years ago, to the Middle East and then to Europe 35,000 years ago, and finally to the Americas via the Siberian Arctic 15,000 years ago.[54]
DNA analyses conducted since 2007 revealed the acceleration of evolution with regards to defenses against disease, skin color, nose shapes, hair color and type, and body shape since about 40,000 years ago, continuing a trend of active selection since humans emigrated from Africa 100,000 years ago. Humans living in colder climates tend to be more heavily built compared to those in warmer climates because having a smaller surface area compared to volume makes it easier to retain heat.[note 2] People from warmer climates tend to have thicker lips, which have large surface areas, enabling them to keep cool. With regards to nose shapes, humans residing in hot and dry places tend to have narrow and protruding noses in order to reduce loss of moisture. Humans living in hot and humid places tend to have flat and broad noses that moisturize inhaled air and retain moisture from exhaled air.[dubious – discuss][citation needed] Humans dwelling in cold and dry places tend to have small, narrow, and long noses in order to warm and moisturize inhaled air. As for hair types, humans from regions with colder climates tend to have straight hair so that the head and neck are kept warm. Straight hair also allows cool moisture to quickly fall off the head. On the other hand, tight and curly hair increases the exposed areas of the scalp, easing the evaporation of sweat and allowing heat to be radiated away while keeping itself off the neck and shoulders. Epicanthic eye folds are believed to be an adaptation protecting the eye from overexposure to ultraviolet radiation, and is presumed to be a particular trait in archaic humans from eastern and southeast Asia. A cold-adaptive explanation for the epicanthic fold is today seen as outdated by some, as epicanthic folds appear in some African populations. Dr. Frank Poirier, a physical anthropologist at Ohio State University, concluded that the epicanthic fold in fact may be an adaptation for tropical regions, and was already part of the natural diversity found among early modern humans.[55][56]
Physiological or phenotypical changes have been traced to Upper Paleolithic mutations, such as the East Asian variant of the EDAR gene, dated to about 35,000 years ago in Southern or Central China. Traits affected by the mutation are sweat glands, teeth, hair thickness and breast tissue.[58] While Africans and Europeans carry the ancestral version of the gene, most East Asians have the mutated version. By testing the gene on mice, Yana G. Kamberov and Pardis C. Sabeti and their colleagues at the Broad Institute found that the mutated version brings thicker hair shafts, more sweat glands, and less breast tissue. East Asian women are known for having comparatively small breasts and East Asians in general tend to have thick hair. The research team calculated that this gene originated in Southern China, which was warm and humid, meaning having more sweat glands would be advantageous to the hunter-gatherers who lived there.[58] A subsequent study from 2021, based on ancient DNA samples, has suggested that the derived variant became dominant among "Ancient Northern East Asians" shortly after the Last Glacial Maximum in Northeast Asia, around 19,000 years ago. Ancient remains from Northern East Asia, such as the Tianyuan Man (40,000 years old) and the AR33K (33,000 years old) specimen lacked the derived EDAR allele, while ancient East Asian remains after the LGM carry the derived EDAR allele.[59][60] The frequency of 370A is most highly elevated in North Asian and East Asian populations.[61]
The most recent Ice Age peaked in intensity between 19,000 and 25,000 years ago and ended about 12,000 years ago. As the glaciers that once covered Scandinavia all the way down to Northern France retreated, humans began returning to Northern Europe from the Southwest, modern-day Spain. But about 14,000 years ago, humans from Southeastern Europe, especially Greece and Turkey, began migrating to the rest of the continent, displacing the first group of humans. Analysis of genomic data revealed that all Europeans since 37,000 years ago have descended from a single founding population that survived the Ice Age, with specimens found in various parts of the continent, such as Belgium. Although this human population was displaced 33,000 years ago, a genetically related group began spreading across Europe 19,000 years ago.[26] Recent divergence of Eurasian lineages was sped up significantly during the Last Glacial Maximum (LGM), the Mesolithic and the Neolithic, due to increased selection pressures and founder effects associated with migration.[62] Alleles predictive of light skin have been found in Neanderthals,[63] but the alleles for light skin in Europeans and East Asians, KITLG and ASIP, are (as of 2012) thought to have not been acquired by archaic admixture but recent mutations since the LGM.[62] Hair, eye, and skin pigmentation phenotypes associated with humans of European descent emerged during the LGM, from about 19,000 years ago.[13] The associated TYRP1 SLC24A5 and SLC45A2 alleles emerge around 19,000 years ago, still during the LGM, most likely in the Caucasus.[62][64] Within the last 20,000 years or so, lighter skin has evolved in East Asia, Europe, North America and Southern Africa. In general, people living in higher latitudes tend to have lighter skin.[3] The HERC2 variation for blue eyes first appears around 14,000 years ago in Italy and the Caucasus.[65]
Inuit adaptation to high-fat diet and cold climate has been traced to a mutation dated the Last Glacial Maximum (20,000 years ago).[66] Average cranial capacity among modern male human populations varies in the range of 1,200 to 1,450 cm3. Larger cranial volumes are associated with cooler climatic regions, with the largest averages being found in populations of Siberia and the Arctic.[note 3][68] Humans living in Northern Asia and the Arctic have evolved the ability to develop thick layers of fat on their faces to keep warm. Moreover, the Inuit tend to have flat and broad faces, an adaptation that reduces the likelihood of frostbites.[69] Both Neanderthal and Cro-Magnons had somewhat larger cranial volumes on average than modern Europeans, suggesting the relaxation of selection pressures for larger brain volume after the end of the LGM.[67]
Australian Aboriginals living in the Central Desert, where the temperature can drop below freezing at night, have evolved the ability to reduce their core temperatures without shivering.[69]
The advent of agriculture has played a key role in the evolutionary history of humanity. Early farming communities benefited from new and comparatively stable sources of food, but were also exposed to new and initially devastating diseases such as tuberculosis, measles, and smallpox. Eventually, genetic resistance to such diseases evolved and humans living today are descendants of those who survived the agricultural revolution and reproduced.[70][5] The pioneers of agriculture faced tooth cavities, protein deficiency and general malnutrition, resulting in shorter statures.[5] Diseases are one of the strongest forces of evolution acting on Homo sapiens. As this species migrated throughout Africa and began colonizing new lands outside the continent around 100,000 years ago, they came into contact with and helped spread a variety of pathogens with deadly consequences. In addition, the dawn of agriculture led to the rise of major disease outbreaks. Malaria is the oldest known of human contagions, traced to West Africa around 100,000 years ago, before humans began migrating out of the continent. Malarial infections surged around 10,000 years ago, raising the selective pressures upon the affected populations, leading to the evolution of resistance.[12]
Examples for adaptations related to agriculture and animal domestication include East Asian types of ADH1B associated with rice domestication,[71] and lactase persistence.[72][73]
As Europeans and East Asians migrated out of Africa, those groups were maladapted and came under stronger selective pressures.[5]
Around 11,000 years ago, as agriculture was replacing hunting and gathering in the Middle East, people invented ways to reduce the concentrations of lactose in milk by fermenting it to make yogurt and cheese. People lost the ability to digest lactose as they matured and as such lost the ability to consume milk. Thousands of years later, a genetic mutation enabled people living in Europe at the time to continue producing lactase, an enzyme that digests lactose, throughout their lives, allowing them to drink milk after weaning and survive bad harvests.[15]
Today, lactase persistence can be found in 90% or more of the populations in Northwestern and Northern Central Europe, and in pockets of Western and Southeastern Africa, Saudi Arabia, and South Asia. It is not as common in Southern Europe (40%) because Neolithic farmers had already settled there before the mutation existed. It is rarer in inland Southeast Asia and Southern Africa. While all Europeans with lactase persistence share a common ancestor for this ability, pockets of lactase persistence outside Europe are likely due to separate mutations. The European mutation, called the LP allele, is traced to modern-day Hungary, 7,500 years ago. In the twenty-first century, about 35% of the human population is capable of digesting lactose after the age of seven or eight.[15] Before this mutation, dairy farming was already widespread in Europe.[74]
A Finnish research team reported that the European mutation that allows for lactase persistence is not found among the milk-drinking and dairy-farming Africans, however. Sarah Tishkoff and her students confirmed this by analyzing DNA samples from Tanzania, Kenya, and Sudan, where lactase persistence evolved independently. The uniformity of the mutations surrounding the lactase gene suggests that lactase persistence spread rapidly throughout this part of Africa. According to Tishkoff's data, this mutation first appeared between 3,000 and 7,000 years ago. This mutation provides some protection against drought and enables people to drink milk without diarrhea, which causes dehydration.[16]
Lactase persistence is a rare ability among mammals.[74] Because it involves a single gene, it is a simple example of convergent evolution in humans. Other examples of convergent evolution, such as the light skin of Europeans and East Asians or the various means of resistance to malaria, are much more complicated.[16]
The light skin pigmentation characteristic of modern Europeans is estimated to have spread across Europe in a "selective sweep" during the Mesolithic (5,000 years ago).[13] Signals for selection in favor of light skin among Europeans was one of the most pronounced, comparable to those for resistance to malaria or lactose tolerance.[75] However, Dan Ju and Ian Mathieson caution in a study addressing 40,000 years of modern human history, "we can assess the extent to which they carried the same light pigmentation alleles that are present today," but explain that c. 40,000 BP Early Upper Paleolithic hunter-gatherers "may have carried different alleles that we cannot now detect", and as a result "we cannot confidently make statements about the skin pigmentation of ancient populations.”[76]
Eumelanin, which is responsible for pigmentation in human skin, protects against ultraviolet radiation while also limiting vitamin D synthesis.[77] Variations in skin color, due to the levels of melanin, are caused by at least 25 different genes, and variations evolved independently of each other to meet different environmental needs.[77] Over the millennia, human skin colors have evolved to be well-suited to their local environments. Having too much melanin can lead to vitamin D deficiency and bone deformities while having too little makes the person more vulnerable to skin cancer.[77] Indeed, Europeans have evolved lighter skin in order to combat vitamin D deficiency in regions with low levels of sunlight. Today, they and their descendants in places with intense sunlight such as Australia are highly vulnerable to sunburn and skin cancer. On the other hand, Inuit have a diet rich in vitamin D and consequently have not needed lighter skin.[78]
Blue eyes are an adaptation for living in regions where the amounts of light are limited because they allow more light to come in than brown eyes.[69] They also seem to have undergone both sexual and frequency-dependent selection.[79][80][75] A research program by geneticist Hans Eiberg and his team at the University of Copenhagen from the 1990s to 2000s investigating the origins of blue eyes revealed that a mutation in the gene OCA2 is responsible for this trait. According to them, all humans initially had brown eyes and the OCA2 mutation took place between 6,000 and 10,000 years ago. It dilutes the production of melanin, responsible for the pigmentation of human hair, eye, and skin color. The mutation does not completely switch off melanin production, however, as that would leave the individual with a condition known as albinism. Variations in eye color from brown to green can be explained via the variation in the amounts of melanin produced in the iris. While brown-eyed individuals share a large area in their DNA controlling melanin production, blue-eyed individuals have only a small region. By examining mitochondrial DNA of people from multiple countries, Eiberg and his team concluded blue-eyed individuals all share a common ancestor.[14]
In 2018, an international team of researchers from Israel and the United States announced their genetic analysis of 6,500-year-old excavated human remains in Israel's Upper Galilee region revealed a number of traits not found in the humans who had previously inhabited the area, including blue eyes. They concluded that the region experienced a significant demographic shift 6,000 years ago due to migration from Anatolia and the Zagros mountains (in modern-day Turkey and Iran) and that this change contributed to the development of the Chalcolithic culture in the region.[81]
Resistance to malaria is a well-known example of recent human evolution. This disease attacks humans early in life. Thus humans who are resistant enjoy a higher chance of surviving and reproducing. While humans have evolved multiple defenses against malaria, sickle cell anemia—a condition in which red blood cells are deformed into sickle shapes, thereby restricting blood flow—is perhaps the best known. Sickle cell anemia makes it more difficult for the malarial parasite to infect red blood cells. This mechanism of defense against malaria emerged independently in Africa and in Pakistan and India. Within 4,000 years it has spread to 10–15% of the populations of these places.[82] Another mutation that enabled humans to resist malaria that is strongly favored by natural selection and has spread rapidly in Africa is the inability to synthesize the enzyme glucose-6-phosphate dehydrogenase, or G6PD.[16]
A combination of poor sanitation and high population densities proved ideal for the spread of contagious diseases which was deadly for the residents of ancient cities. Evolutionary thinking would suggest that people living in places with long-standing urbanization dating back millennia would have evolved resistance to certain diseases, such as tuberculosis and leprosy. Using DNA analysis and archeological findings, scientists from the University College London and the Royal Holloway studied samples from 17 sites in Europe, Asia, and Africa. They learned that, indeed, long-term exposure to pathogens has led to resistance spreading across urban populations. Urbanization is therefore a selective force that has influenced human evolution.[83] The allele in question is named SLC11A1 1729+55del4. Scientists found that among the residents of places that have been settled for thousands of years, such as Susa in Iran, this allele is ubiquitous whereas in places with just a few centuries of urbanization, such as Yakutsk in Siberia, only 70–80% of the population have it.[84]
Evolution to resist infection of pathogens also increased inflammatory disease risk in post-Neolithic Europeans over the last 10,000 years. A study of ancient DNA estimated nature, strength, and time of onset of selections due to pathogens and also found that "the bulk of genetic adaptation occurred after the start of the Bronze Age, <4,500 years ago".[85][86]
Adaptations have also been found in modern populations living in extreme climatic conditions such as the Arctic, as well as immunological adaptations such as resistance against prion caused brain disease in populations practicing mortuary cannibalism, or the consumption of human corpses.[87][88] Inuit have the ability to thrive on the lipid-rich diets consisting of Arctic mammals. Human populations living in regions of high altitudes, such as the Tibetan Plateau, Ethiopia, and the Andes benefit from a mutation that enhances the concentration of oxygen in their blood.[2] This is achieved by having more capillaries, increasing their capacity for carrying oxygen.[3] This mutation is believed to be around 3,000 years old.[2]
A recent adaptation has been proposed for the Austronesian Sama-Bajau, also known as the Sea Gypsies or Sea Nomads, developed under selection pressures associated with subsisting on free-diving over the past thousand years or so.[11][89] As maritime hunter-gatherers, the ability to dive for long periods of times plays a crucial role in their survival. Due to the mammalian dive reflex, the spleen contracts when the mammal dives and releases oxygen-carrying red blood cells. Over time, individuals with larger spleens were more likely to survive the lengthy free-dives, and thus reproduce. By contrast, communities centered around farming show no signs of evolving to have larger spleens. Because the Sama-Bajau show no interest in abandoning this lifestyle, there is no reason to believe further adaptation will not occur.[18]
Advances in the biology of genomes have enabled geneticists to investigate the course of human evolution within centuries. Jonathan Pritchard and a postdoctoral fellow, Yair Field, counted the singletons, or changes of single DNA bases, which are likely to be recent because they are rare and have not spread throughout the population. Since alleles bring neighboring DNA regions with them as they move around the genome, the number of singletons can be used to roughly estimate how quickly the allele has changed its frequency. This approach can unveil evolution within the last 2,000 years or a hundred human generations. Armed with this technique and data from the UK10K project, Pritchard and his team found that alleles for lactase persistence, blond hair, and blue eyes have spread rapidly among Britons within the last two millennia or so. Britain's cloudy skies may have played a role in that the genes for light hair could also cause light skin, reducing the chances of vitamin D deficiency. Sexual selection could also favor blond hair. The technique also enabled them to track the selection of polygenic traits—those affected by a multitude of genes, rather than just one—such as height, infant head circumferences, and female hip sizes (crucial for giving birth).[23] They found that natural selection has been favoring increased height and larger head and female hip sizes among Britons. Moreover, lactase persistence showed signs of active selection during the same period. However, evidence for the selection of polygenic traits is weaker than those affected only by one gene.[90]
A 2012 paper studied the DNA sequence of around 6,500 Americans of European and African descent and confirmed earlier work indicating that the majority of changes to a single letter in the sequence (single nucleotide variants) were accumulated within the last 5,000-10,000 years. Almost three quarters arose in the last 5,000 years or so. About 14% of the variants are potentially harmful, and among those, 86% were 5,000 years old or younger. The researchers also found that European Americans had accumulated a much larger number of mutations than African Americans. This is likely a consequence of their ancestors' migration out of Africa, which resulted in a genetic bottleneck; there were few mates available. Despite the subsequent exponential growth in population, natural selection has not had enough time to eradicate the harmful mutations. While humans today carry far more mutations than their ancestors did 5,000 years ago, they are not necessarily more vulnerable to illnesses because these might be caused by multiple mutations. It does, however, confirm earlier research suggesting that common diseases are not caused by common gene variants.[91] In any case, the fact that the human gene pool has accumulated so many mutations over such a short period of time—in evolutionary terms—and that the human population has exploded in that time mean that humanity is more evolvable than ever before. Natural selection might eventually catch up with the variations in the gene pool, as theoretical models suggest that evolutionary pressures increase as a function of population size.[92]
A study published in 2021 states that the populations of the Cape Verde islands off the coast of West Africa have speedily evolved resistance to malaria within roughly the last 20 generations, since the start of human habitation there. As expected, the residents of the Island of Santiago, where malaria is most prevalent, show the highest prevalence of resistance. This is one of the most rapid cases of change to the human genome measured.[93][94]
Geneticist Steve Jones told the BBC that during the sixteenth century, only a third of English babies survived until the age of 21, compared to 99% in the twenty-first century. Medical advances, especially those made in the twentieth century, made this change possible. Yet while people from the developed world today are living longer and healthier lives, many are choosing to have just a few or no children at all, meaning evolutionary forces continue to act on the human gene pool, just in a different way.[95]
Natural selection affects only 8% of the human genome, meaning mutations in the remaining parts of the genome can change their frequency by pure chance through neutral selection. If natural selective pressures are reduced, then more mutations survive, which could increase their frequency and the rate of evolution. For humans, a large source of heritable mutations is sperm; a man accumulates more and more mutations in his sperm as he ages. Hence, men delaying reproduction can affect human evolution.[2]
A 2012 study led by Augustin Kong suggests that the number of de novo (new) mutations increases by about two per year of delayed reproduction by the father and that the total number of paternal mutations doubles every 16.5 years.[96]
For a long time, medicine has reduced the fatality of genetic defects and contagious diseases, allowing more and more humans to survive and reproduce, but it has also enabled maladaptive traits that would otherwise be culled to accumulate in the gene pool. This is not a problem as long as access to modern healthcare is maintained. But natural selective pressures will mount considerably if that is taken away.[18] Nevertheless, dependence on medicine rather than genetic adaptations will likely be the driving force behind humanity's fight against diseases for the foreseeable future. Moreover, while the introduction of antibiotics initially reduced the mortality rates due to infectious diseases by significant amounts, overuse has led to the rise of antibiotic-resistant strains of bacteria, making many illnesses major causes of death once again.[70]
Human jaws and teeth have been shrinking in proportion with the decrease in body size in the last 30,000 years as a result of new diets and technology. There are many individuals today who do not have enough space in their mouths for their third molars (or wisdom teeth) due to reduced jaw sizes. In the twentieth century, the trend toward smaller teeth appeared to have been slightly reversed due to the introduction of fluoride, which thickens dental enamel, thereby enlarging the teeth.[69]
Recent research suggests that menopause is evolving to occur later. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.[17]
Population geneticist Emmanuel Milot and his team studied recent human evolution in an isolated Canadian island using 140 years of church records. They found that selection favored younger age at first birth among women.[8] In particular, the average age at first birth of women from Coudres Island (Île aux Coudres), 80 km (50 mi) northeast of Québec City, decreased by four years between 1800 and 1930. Women who started having children sooner generally ended up with more children in total who survive until adulthood. In other words, for these French-Canadian women, reproductive success was associated with lower age at first childbirth. Maternal age at first birth is a highly heritable trait.[97]
Human evolution continues during the modern era, including among industrialized nations. Things like access to contraception and the freedom from predators do not stop natural selection.[98] Among developed countries, where life expectancy is high and infant mortality rates are low, selective pressures are the strongest on traits that influence the number of children a human has. It is speculated that alleles influencing sexual behavior would be subject to strong selection, though the details of how genes can affect said behavior remain unclear.[10]
Historically, as a by-product of the ability to walk upright, humans evolved to have narrower hips and birth canals and to have larger heads. Compared to other close relatives such as chimpanzees, childbirth is a highly challenging and potentially fatal experience for humans. Thus began an evolutionary tug-of-war (see Obstetrical dilemma). For babies, having larger heads proved beneficial as long as their mothers' hips were wide enough. If not, both mother and child typically died. This is an example of balancing selection, or the removal of extreme traits. In this case, heads that were too large or hips that were too small were selected against. This evolutionary tug-of-war attained an equilibrium, making these traits remain more or less constant over time while allowing for genetic variation to flourish, thus paving the way for rapid evolution should selective forces shift their direction.[99]
All this changed in the twentieth century as Caesarean sections (also known as C-sections) became safer and more common in some parts of the world.[100] Larger head sizes continue to be favored while selective pressures against smaller hip sizes have diminished. Projecting forward, this means that human heads would continue to grow while hip sizes would not. As a result of increasing fetopelvic disproportion, C-sections would become more and more common in a positive feedback loop, though not necessarily to the extent that natural childbirth would become obsolete.[99][100]
Paleoanthropologist Briana Pobiner of the Smithsonian Institution noted that cultural factors could play a role in the widely different rates of C-sections across the developed and developing worlds. Daghni Rajasingam of the Royal College of Obstetricians observed that the increasing rates of diabetes and obesity among women of reproductive age also boost the demand for C-sections.[100] Biologist Philipp Mitteroecker from the University of Vienna and his team estimated that about six percent of all births worldwide were obstructed and required medical intervention. In the United Kingdom, one quarter of all births involved the C-section while in the United States, the number was one in three. Mitteroecker and colleagues discovered that the rate of C-sections has gone up 10% to 20% since the mid-twentieth century. They argued that because the availability of safe Cesarean sections significantly reduced maternal and infant mortality rates in the developed world, they have induced an evolutionary change. However, "It's not easy to foresee what this will mean for the future of humans and birth," Mitteroecker told The Independent. This is because the increase in baby sizes is limited by the mother's metabolic capacity and modern medicine, which makes it more likely that neonates who are born prematurely or are underweight to survive.[101]
Researchers participating in the Framingham Heart Study, which began in 1948 and was intended to investigate the cause of heart disease among women and their descendants in Framingham, Massachusetts, found evidence for selective pressures against high blood pressure due to the modern Western diet, which contains high amounts of salt, known for raising blood pressure. They also found evidence for selection against hypercholesterolemia, or high levels of cholesterol in the blood.[18] Evolutionary geneticist Stephen Stearns and his colleagues reported signs that women were gradually becoming shorter and heavier. Stearns argued that human culture and changes humans have made on their natural environments are driving human evolution rather than putting the process to a halt.[95] The data indicates that the women were not eating more; rather, the ones who were heavier tended to have more children.[102] Stearns and his team also discovered that the subjects of the study tended to reach menopause later; they estimated that if the environment remains the same, the average age at menopause will increase by about a year in 200 years, or about ten generations. All these traits have medium to high heritability.[10] Given the starting date of the study, the spread of these adaptations can be observed in just a few generations.[18]
By analyzing genomic data of 60,000 individuals of Caucasian descent from Kaiser Permanente in Northern California, and of 150,000 people in the UK Biobank, evolutionary geneticist Joseph Pickrell and evolutionary biologist Molly Przeworski were able to identify signs of biological evolution among living human generations. For the purposes of studying evolution, one lifetime is the shortest possible time scale. An allele associated with difficulty withdrawing from tobacco smoking dropped in frequency among the British but not among the Northern Californians. This suggests that heavy smokers—who were common in Britain during the 1950s but not in Northern California—were selected against. A set of alleles linked to later menarche was more common among women who lived for longer. An allele called ApoE4, linked to Alzheimer's disease, fell in frequency as carriers tended to not live for very long.[23] In fact, these were the only traits that reduced life expectancy Pickrell and Przeworski found, which suggests that other harmful traits probably have already been eradicated. Only among older people are the effects of Alzheimer's disease and smoking visible. Moreover, smoking is a relatively recent trend. It is not entirely clear why such traits bring evolutionary disadvantages, however, since older people have already had children. Scientists proposed that either they also bring about harmful effects in youth or that they reduce an individual's inclusive fitness, or the tendency of organisms that share the same genes to help each other. Thus, mutations that make it difficult for grandparents to help raise their grandchildren are unlikely to propagate throughout the population.[8] Pickrell and Przeworski also investigated 42 traits determined by multiple alleles rather than just one, such as the timing of puberty. They found that later puberty and older age of first birth were correlated with higher life expectancy.[8]
Larger sample sizes allow for the study of rarer mutations. Pickrell and Przeworski told The Atlantic that a sample of half a million individuals would enable them to study mutations that occur among only 2% of the population, which would provide finer details of recent human evolution.[8] While studies of short time scales such as these are vulnerable to random statistical fluctuations, they can improve understanding of the factors that affect survival and reproduction among contemporary human populations.[23]
Evolutionary geneticist Jaleal Sanjak and his team analyzed genetic and medical information from more than 200,000 women over the age of 45 and 150,000 men over the age of 50—people who have passed their reproductive years—from the UK Biobank and identified 13 traits among women and ten among men that were linked to having children at a younger age, having a higher body-mass index,[note 4] fewer years of education, and lower levels of fluid intelligence, or the capacity for logical reasoning and problem solving. Sanjak noted, however, that it was not known whether having children actually made women heavier or being heavier made it easier to reproduce. Because taller men and shorter women tended to have more children and because the genes associated with height affect men and women equally, the average height of the population will likely remain the same. Among women who had children later, those with higher levels of education had more children.[98]
Evolutionary biologist Hakhamanesh Mostafavi led a 2017 study that analyzed data of 215,000 individuals from just a few generations in the United Kingdom and the United States and found a number of genetic changes that affect longevity. The ApoE allele linked to Alzheimer's disease was rare among women aged 70 and over while the frequency of the CHRNA3 gene associated with smoking addiction among men fell among middle-aged men and up. Because this is not itself evidence of evolution, since natural selection only cares about successful reproduction not longevity, scientists have proposed a number of explanations. Men who live longer tend to have more children. Men and women who survive until old age can help take care of both their children and grandchildren, in benefits their descendants down the generations. This explanation is known as the grandmother hypothesis. It is also possible that Alzheimer's disease and smoking addiction are also harmful earlier in life, but the effects are more subtle and larger sample sizes are required in order to study them. Mostafavi and his team also found that mutations causing health problems such as asthma, having a high body-mass index and high cholesterol levels were more common among those with shorter lifespans while mutations leading to delayed puberty and reproduction were more common among long living individuals. According to geneticist Jonathan Pritchard, while the link between fertility and longevity was identified in previous studies, those did not entirely rule out the effects of educational and financial status—people who rank high in both tend to have children later in life; this seems to suggest the existence of an evolutionary trade-off between longevity and fertility.[103]
In South Africa, where large numbers of people are infected with HIV, some have genes that help them combat this virus, making it more likely that they would survive and pass this trait onto their children.[104] If the virus persists, humans living in this part of the world could become resistant to it in as little as hundreds of years. However, because HIV evolves more quickly than humans, it will more likely be dealt with technologically rather than genetically.[10]
A 2017 study by researchers from Northwestern University unveiled a mutation among the Old Order Amish living in Berne, Indiana, that suppressed their chances of having diabetes and extends their life expectancy by about ten years on average. That mutation occurred in the gene called Serpine1, which codes for the production of the protein PAI-1 (plasminogen activator inhibitor), which regulates blood clotting and plays a role in the aging process. About 24% of the people sampled carried this mutation and had a life expectancy of 85, higher than the community average of 75. Researchers also found the telomeres—non-functional ends of human chromosomes—of those with the mutation to be longer than those without. Because telomeres shorten as the person ages, those with longer telomeres tend to live longer. At present, the Amish live in 22 U.S. states plus the Canadian province of Ontario. They live simple lifestyles that date back centuries and generally insulate themselves from modern North American society. They are mostly indifferent towards modern medicine, but scientists do have a healthy relationship with the Amish community in Berne. Their detailed genealogical records make them ideal subjects for research.[20]
In 2020, Teghan Lucas, Maciej Henneberg, Jaliya Kumaratilake gave evidence that a growing share of the human population retained the median artery in their forearms. This structure forms during fetal development but dissolves once two other arteries, the radial and ulnar arteries, develop. The median artery allows for more blood flow and could be used as a replacement in certain surgeries. Their statistical analysis suggested that the retention of the median artery was under extremely strong selection within the last 250 years or so. People have been studying this structure and its prevalence since the eighteenth century.[19][105]
Multidisciplinary research suggests that ongoing evolution could help explain the rise of certain medical conditions such as autism and autoimmune disorders. Autism and schizophrenia may be due to genes inherited from the mother and the father which are over-expressed and which fight a tug-of-war in the child's body. Allergies, asthma, and autoimmune disorders appear linked to higher standards of sanitation, which prevent the immune systems of modern humans from being exposed to various parasites and pathogens the way their ancestors' were, making them hypersensitive and more likely to overreact. The human body is not built from a professionally engineered blue print but a system shaped over long periods of time by evolution with all kinds of trade-offs and imperfections. Understanding the evolution of the human body can help medical doctors better understand and treat various disorders. Research in evolutionary medicine suggests that diseases are prevalent because natural selection favors reproduction over health and longevity. In addition, biological evolution is slower than cultural evolution and humans evolve more slowly than pathogens.[106]
Whereas in the ancestral past, humans lived in geographically isolated communities where inbreeding was rather common,[70] modern transportation technologies have made it much easier for people to travel great distances and facilitated further genetic mixing, giving rise to additional variations in the human gene pool.[102] It also enables the spread of diseases worldwide, which can have an effect on human evolution.[70] Furthermore, climate change may trigger the mass migration of not just humans but also diseases affecting humans.[77] Besides the selection and flow of genes and alleles, another mechanism of biological evolution is epigenetics, or changes not to the DNA sequence itself, but rather the way it is expressed. Scientists already know that chronic illnesses and stress are epigenetic mechanisms.[3]

---

# Ardipithecus

Ardipithecus is a genus of an extinct hominine that lived during the Late Miocene and Early Pliocene epochs in the Afar Depression, Ethiopia. Originally described as one of the earliest ancestors of humans after they diverged from the chimpanzees, the relation of this genus to human ancestors and whether it is a hominin is now a matter of debate.[1] Two fossil species are described in the literature: A. ramidus, which lived about 4.4 million years ago[2] during the early Pliocene, and A. kadabba, dated to approximately 5.6 million years ago (late Miocene).[3] Initial behavioral analysis indicated that Ardipithecus could be very similar to chimpanzees;[1] however, more recent analysis based on canine size and lack of canine sexual dimorphism indicates that Ardipithecus was characterised by reduced aggression,[4] and that they more closely resemble bonobos.[5]
Some analyses describe Australopithecus as being sister to Ardipithecus ramidus specifically.[6] This means that Australopithecus is distinctly more closely related to Ardipithecus ramidus than Ardipithecus kadabba. Cladistically, then, Australopithecus (and eventually Homo sapiens) indeed emerged within the Ardipithecus lineage, and this lineage is not literally extinct.
A. ramidus was named in September 1994. The first fossil found was dated to 4.4 million years ago on the basis of its stratigraphic position between two volcanic strata: the basal Gaala Tuff Complex (G.A.T.C.) and the Daam Aatu Basaltic Tuff (D.A.B.T.).[7] The name Ardipithecus ramidus stems mostly from the Afar language, in which Ardi means "ground/floor" and ramid means "root". The pithecus portion of the name is from the Greek word for "ape".[8]
Like most hominids, but unlike all previously recognized hominins, it had a grasping hallux or big toe adapted for locomotion in the trees. It is not confirmed how many other features of its skeleton reflect adaptation to bipedalism on the ground as well. Like later hominins, Ardipithecus had reduced canine teeth and reduced canine sexual dimorphism.[4]
In 1992–1993 a research team headed by Tim White discovered the first A. ramidus fossils—seventeen fragments including skull, mandible, teeth and arm bones—from the Afar Depression in the Middle Awash river valley of Ethiopia. More fragments were recovered in 1994, amounting to 45% of the total skeleton. This fossil was originally described as a species of Australopithecus, but White and his colleagues later published a note in the same journal renaming the fossil under a new genus, Ardipithecus. Between 1999 and 2003, a multidisciplinary team led by Sileshi Semaw discovered bones and teeth of nine A. ramidus individuals at As Duma in the Gona area of Ethiopia's Afar Region.[9] The fossils were dated to between 4.35 and 4.45 million years old.[10]
Ardipithecus ramidus had a small brain, measuring between 300 and 350 cm3. This is slightly smaller than a modern bonobo or female chimpanzee brain, but much smaller than the brain of australopithecines like Lucy (~400 to 550 cm3) and roughly 20% the size of the modern Homo sapiens brain. Like common chimpanzees, A. ramidus was much more prognathic than modern humans.[11]
The teeth of A. ramidus lacked the specialization of other apes, and suggest that it was a generalized omnivore and frugivore (fruit eater) with a diet that did not depend heavily on foliage, fibrous plant material (roots, tubers, etc.), or hard and or abrasive food. The size of the upper canine tooth in A. ramidus males was not distinctly different from that of females. Their upper canines were less sharp than those of modern common chimpanzees in part because of this decreased upper canine size, as larger upper canines can be honed through wear against teeth in the lower mouth. The features of the upper canine in A. ramidus contrast with the sexual dimorphism observed in common chimpanzees, where males have significantly larger and sharper upper canine teeth than females.[12] Of the living apes, bonobos have the smallest canine sexual dimorphism, although still greater than that displayed by A. ramidus.[4]
The less pronounced nature of the upper canine teeth in A. ramidus has been used to infer aspects of the social behavior of the species and more ancestral hominids. In particular, it has been used to suggest that the last common ancestor of hominids and African apes was characterized by relatively little aggression between males and between groups. This is markedly different from social patterns in common chimpanzees, among which intermale and intergroup aggression are typically high. Researchers in a 2009 study said that this condition "compromises the living chimpanzee as a behavioral model for the ancestral hominid condition."[12] Bonobo canine size and canine sexual dimorphism more closely resembles that of A. ramidus, and as a result, bonobos are now suggested as a behavioural model.[13]
A. ramidus existed more recently than the most recent common ancestor of humans and chimpanzees (CLCA or Pan-Homo LCA) and thus is not fully representative of that common ancestor. Nevertheless, it is in some ways unlike chimpanzees, suggesting that the common ancestor differs from the modern chimpanzee. After the chimpanzee and human lineages diverged, both underwent substantial evolutionary change. Chimp feet are specialized for grasping trees; A. ramidus feet are better suited for walking. The canine teeth of A. ramidus are smaller, and equal in size between males and females, which suggests reduced male-to-male conflict, increased pair-bonding, and increased parental investment. "Thus, fundamental reproductive and social behavioral changes probably occurred in hominids long before they had enlarged brains and began to use stone tools," the research team concluded.[3]
On October 1, 2009, paleontologists formally announced the discovery of the relatively complete A. ramidus fossil skeleton first unearthed in 1994. The fossil is the remains of a small-brained 50-kilogram (110 lb) female, nicknamed "Ardi", and includes most of the skull and teeth, as well as the pelvis, hands, and feet.[14] It was discovered in Ethiopia's harsh Afar desert at a site called Aramis in the Middle Awash region. Radiometric dating of the layers of volcanic ash encasing the deposits suggest that Ardi lived about 4.3 to 4.5 million years ago. This date, however, has been questioned by others. Fleagle and Kappelman suggest that the region in which Ardi was found is difficult to date radiometrically, and they argue that Ardi should be dated at 3.9 million years.[15]
The fossil is regarded by its describers as shedding light on a stage of human evolution about which little was known, more than a million years before Lucy (Australopithecus afarensis), the iconic early human ancestor candidate who lived 3.2 million years ago, and was discovered in 1974 just 74 km (46 mi) away from Ardi's discovery site. However, because the "Ardi" skeleton is no more than 200,000 years older than the earliest fossils of Australopithecus, and may in fact be younger than they are,[15] some researchers doubt that it can represent a direct ancestor of Australopithecus.
Some researchers infer from the form of her pelvis and limbs and the presence of her abductable hallux, that "Ardi" was a facultative biped: bipedal when moving on the ground, but quadrupedal when moving about in tree branches.[3][16][17] A. ramidus had a more primitive walking ability than later hominids, and could not walk or run for long distances.[18] The teeth suggest omnivory, and are more generalised than those of modern apes.[3]
Ardipithecus kadabba is "known only from teeth and bits and pieces of skeletal bones",[14] and is dated to approximately 5.6 million years ago.[3] It has been described as a "probable chronospecies" (i.e. ancestor) of A. ramidus.[3] Although originally considered a subspecies of A. ramidus, in 2004 anthropologists Yohannes Haile-Selassie, Gen Suwa, and Tim D. White published an article elevating A. kadabba to species level on the basis of newly discovered teeth from Ethiopia. These teeth show "primitive morphology and wear pattern" which demonstrate that A. kadabba is a distinct species from A. ramidus.[19]
The specific name comes from the Afar word for "basal family ancestor".[20]
Due to several shared characteristics with chimpanzees, its closeness to ape divergence period, and due to its fossil incompleteness, the exact position of Ardipithecus in the fossil record is a subject of controversy.[21] Primatologist Esteban Sarmiento had systematically compared and concluded that there is not sufficient anatomical evidence to support an exclusively human lineage. Sarmiento noted that Ardipithecus does not share any characteristics exclusive to humans, and some of its characteristics (those in the wrist and basicranium) suggest it diverged from humans prior to the human–gorilla last common ancestor.[22] His comparative (narrow allometry) study in 2011 on the molar and body segment lengths (which included living primates of similar body size) noted that some dimensions including short upper limbs, and metacarpals are reminiscent of humans, but other dimensions such as long toes and relative molar surface area are great ape-like. Sarmiento concluded that such length measures can change back and forth during evolution and are not very good indicators of relatedness (homoplasy).[23]
However, some later studies still argue for its classification in the human lineage. In 2014, it was reported that the hand bones of Ardipithecus, Australopithecus sediba and A. afarensis have the third metacarpal styloid process, which is absent in other apes.[24] Unique brain organisations (such as lateral shift of the carotid foramina, mediolateral abbreviation of the lateral tympanic, and a shortened, trapezoidal basioccipital element) in Ardipithecus are also found only in the Australopithecus and Homo.[25] Comparison of the tooth root morphology with those of the earlier Sahelanthropus also indicated strong resemblance, also pointing to inclusion to the human line.[26]
The Ardipithecus length measures are good indicators of function and together with dental isotope data and the fauna and flora from the fossil site indicate Ardipithecus was mainly a terrestrial quadruped collecting a large portion of its food on the ground. Its arboreal behaviors would have been limited and suspension from branches solely from the upper limbs rare.[23] A comparative study in 2013 on carbon and oxygen stable isotopes within modern and fossil tooth enamel revealed that Ardipithecus fed both arboreally (on trees) and on the ground in a more open habitat, unlike chimpanzees.[28]
In 2015, Australian anthropologists Gary Clark and Maciej Henneberg said that Ardipithecus adults have a facial anatomy more similar to chimpanzee subadults than adults, with a less-projecting face and smaller canines (large canines in primate males are used to compete within mating hierarchies), and attributed this to a decrease in craniofacial growth in favour of brain growth. This is only seen in humans, so they argued that the species may show the first trend towards human social, parenting and sexual psychology.[29] Previously, it was assumed that such ancient human ancestors behaved much like chimps, but this is no longer considered to be a viable comparison.[30] This view has yet to be corroborated by more detailed studies of the growth of A. ramidus. The study also provides support for Stephen Jay Gould's theory in Ontogeny and Phylogeny that the paedomorphic (childlike) form of early hominin craniofacial morphology results from dissociation of growth trajectories.
Clark and Henneberg also argued that such shortening of the skull—which may have caused a descension of the larynx—as well as lordosis—allowing better movement of the larynx—increased vocal ability, significantly pushing back the origin of language to well before the evolution of Homo. They argued that self domestication was aided by the development of vocalization, living in a pro-social society. They conceded that chimps and A. ramidus likely had the same vocal capabilities, but said that A. ramidus made use of more complex vocalizations, and vocalized at the same level as a human infant due to selective pressure to become more social. This would have allowed their society to become more complex. They also noted that the base of the skull stopped growing with the brain by the end of juvenility, whereas in chimps it continues growing with the rest of the body into adulthood; and considered this evidence of a switch from a gross skeletal anatomy trajectory to a neurological development trajectory due to selective pressure for sociability. Nonetheless, their conclusions are highly speculative.[31][29]
According to Scott Simpson, the Gona Project's physical anthropologist, the fossil evidence from the Middle Awash indicates that both A. kadabba and A. ramidus lived in "a mosaic of woodland and grasslands with lakes, swamps and springs nearby," but further research is needed to determine which habitat Ardipithecus at Gona preferred.[9]

---

# Australopithecus

Australopithecus (/ˌɒstrələˈpɪθɪkəs, -loʊ-/, OS-trə-lə-PITH-i-kəs, -⁠loh-;[1] or (/ɒsˌtrələpɪˈθiːkəs/, os-TRA-lə-pi-THEE-kəs[2] from Latin australis 'southern' and Ancient Greek πίθηκος (pithekos) 'ape'[3]) is a genus of early hominins that existed in Africa during the Pliocene and Early Pleistocene. The genera Homo (which includes modern humans), Paranthropus, and Kenyanthropus evolved from some Australopithecus species. Australopithecus is a member of the subtribe Australopithecina,[4][5] which sometimes also includes Ardipithecus,[6] though the term "australopithecine" is sometimes used to refer only to members of Australopithecus. Species include A. garhi, A. africanus, A. sediba, A. afarensis, A. anamensis, A. bahrelghazali, and A. deyiremeda. Debate exists as to whether some Australopithecus species should be reclassified into new genera, or if Paranthropus and Kenyanthropus are synonymous with Australopithecus, in part because of the taxonomic inconsistency.[7][8]
Furthermore, because e.g. A. africanus is more closely related to humans, or their ancestors at the time, than e.g. A. anamensis and many more Australopithecus branches, Australopithecus cannot be consolidated into a coherent grouping without also including the genus Homo and other genera.
The earliest known member of the genus, A. anamensis, existed in eastern Africa around 4.2 million years ago. Australopithecus fossils become more widely dispersed throughout eastern and southern Africa (the Chadian A. bahrelghazali indicates that the genus was much more widespread than the fossil record suggests), before eventually becoming extinct 1.9 million years ago (or 1.2 to 0.6 million years ago if Paranthropus is included). While none of the groups normally directly assigned to this group survived, Australopithecus gave rise to living descendants, as the genus Homo emerged from an Australopithecus species[7][9][10][11][12][excessive citations] at some time between 3 and 2 million years ago.[13]
Australopithecus possessed two of the three duplicated genes derived from SRGAP2 roughly 3.4 and 2.4 million years ago (SRGAP2B and SRGAP2C), the second of which contributed to the increase in number and migration of neurons in the human brain.[14][15] Significant changes to the hand first appear in the fossil record of later A. afarensis about 3 million years ago (fingers shortened relative to thumb and changes to the joints between the index finger and the trapezium and capitate).[16]
The first Australopithecus specimen, the type specimen, was discovered in 1924 in a lime quarry by workers at Taung, South Africa. The specimen was studied by the Australian anatomist Raymond Dart, who was then working at the University of the Witwatersrand in Johannesburg. The fossil skull was from a three-year-old bipedal primate (nicknamed Taung Child) that he named Australopithecus africanus. The first report was published in Nature in February 1925. Dart realised that the fossil contained a number of humanoid features, and so he came to the conclusion that this was an early human ancestor.[17] Later, Scottish paleontologist Robert Broom and Dart set out to search for more early hominin specimens, and several more A. africanus remains from various sites. Initially, anthropologists were largely hostile to the idea that these discoveries were anything but apes, though this changed during the late 1940s.[17]
In 1950, evolutionary biologist Ernst Walter Mayr said that all bipedal apes should be classified into the genus Homo, and considered renaming Australopithecus to Homo transvaalensis.[18] However, the contrary view taken by J.T. Robinson in 1954, excluding australopiths from Homo, became the prevalent view.[18] The first australopithecine fossil discovered in eastern Africa was an A. boisei skull excavated by Mary Leakey in 1959 in Olduvai Gorge, Tanzania. Since then, the Leakey family has continued to excavate the gorge, uncovering further evidence for australopithecines, as well as for Homo habilis and Homo erectus.[17] The scientific community took 20 more years to widely accept Australopithecus as a member of the human family tree.
In 1997, an almost complete Australopithecus skeleton with skull was found in the Sterkfontein caves of Gauteng, South Africa. It is now called "Little Foot" and it is around 3.7 million years old. It was named Australopithecus prometheus[19][20] which has since been placed within A. africanus. Other fossil remains found in the same cave in 2008 were named Australopithecus sediba, which lived 1.9 million years ago. A. africanus probably evolved into A. sediba, which some scientists think may have evolved into H. erectus,[21] though this is heavily disputed.
In 2003, Spanish writer Camilo José Cela Conde and evolutionary biologist Francisco J. Ayala proposed resurrecting the genus Praeanthropus to house Orrorin, A. afarensis, A. anamensis, A. bahrelghazali, and A. garhi,[22] but this genus has been largely dismissed.[23]
With the apparent emergence of the genera Homo, Kenyanthropus, and Paranthropus in the genus Australopithecus, taxonomy runs into some difficulty, as the name of species incorporates their genus. According to cladistics, groups should not be left paraphyletic, where it is kept not consisting of a common ancestor and all of its descendants.[24][25][26][27][28][29] Resolving this problem would cause major ramifications in the nomenclature of all descendent species. Possibilities suggested have been to rename Homo sapiens to Australopithecus sapiens[30] (or even Pan sapiens[31][32]), or to move some Australopithecus species into new genera.[8] A study reported in 2025 reported preliminary success in extracting ancient proteins from an Australopithic tooth, suggesting that paleoproteomics has the potential to provide information about the genetic affinities of the species.[33]
In 2002 and again in 2007, Camilo José Cela Conde et al. suggested that A. africanus be moved to Paranthropus.[7] On the basis of craniodental evidence, Strait and Grine (2004) suggest that A. anamensis and A. garhi should be assigned to new genera.[34] It is debated whether or not A. bahrelghazali should be considered simply a western variant of A. afarensis instead of a separate species.[35][36]
A. anamensis may have descended from or was closely related to Ardipithecus ramidus.[37] A. anamensis shows some similarities to both Ar. ramidus and Sahelanthropus.[37]
Australopiths shared several traits with modern apes and humans, and were widespread throughout Eastern and Northern Africa by 3.5 million years ago (mya). The earliest evidence of fundamentally bipedal hominins is a (3.6 mya) fossil trackway in Laetoli, Tanzania, which bears a remarkable similarity to those of modern humans. The footprints have generally been classified as australopith, as they are the only form of prehuman hominins known to have existed in that region at that time.[38]
According to the Chimpanzee Genome Project, the human–chimpanzee last common ancestor existed about five to six million years ago, assuming a constant rate of mutation. However, hominin species dated to earlier than the date could call this into question.[39] Sahelanthropus tchadensis, commonly called "Toumai", is about seven million years old and Orrorin tugenensis lived at least six million years ago. Since little is known of them, they remain controversial among scientists since the molecular clock in humans has determined that humans and chimpanzees had a genetic split at least a million years later.[citation needed] One theory suggests that the human and chimpanzee lineages diverged somewhat at first, then some populations interbred around one million years after diverging.[39]
The brains of most species of Australopithecus were roughly 35% of the size of a modern human brain[40] with an endocranial volume average of 466 cc (28.4 cu in).[13] Although this is more than the average endocranial volume of chimpanzee brains at 360 cc (22 cu in)[13] the earliest australopiths (A. anamensis) appear to have been within the chimpanzee range,[37] whereas some later australopith specimens have a larger endocranial volume than that of some early Homo fossils.[13]
Most species of Australopithecus were diminutive and gracile, usually standing 1.2 to 1.4 m (3 ft 11 in to 4 ft 7 in) tall. It is possible that they exhibited a considerable degree of sexual dimorphism, males being larger than females.[41] In modern populations, males are on average a mere 15% larger than females, while in Australopithecus, males could be up to 50% larger than females by some estimates. However, the degree of sexual dimorphism is debated due to the fragmentary nature of australopith remains.[41] One paper finds that A. afarensis had a level of dimorphism close to modern humans.[42]
According to A. Zihlman, Australopithecus body proportions closely resemble those of bonobos (Pan paniscus),[43] leading evolutionary biologist Jeremy Griffith to suggest that bonobos may be phenotypically similar to Australopithecus.[44] Furthermore, thermoregulatory models suggest that australopiths were fully hair covered, more like chimpanzees and bonobos, and unlike humans.[45]
The fossil record seems to indicate that Australopithecus is ancestral to Homo and modern humans. It was once assumed that large brain size had been a precursor to bipedalism, but the discovery of Australopithecus with a small brain but developed bipedality upset this theory. Nonetheless, it remains a matter of controversy as to how bipedalism first emerged. The advantages of bipedalism were that it left the hands free to grasp objects (e.g., carry food and young), and allowed the eyes to look over tall grasses for possible food sources or predators, but it is also argued that these advantages were not significant enough to cause the emergence of bipedalism.[citation needed] Earlier fossils, such as Orrorin tugenensis, indicate bipedalism around six million years ago, around the time of the split between humans and chimpanzees indicated by genetic studies. This suggests that erect, straight-legged walking originated as an adaptation to tree-dwelling.[46] Major changes to the pelvis and feet had already taken place before Australopithecus.[47] It was once thought that humans descended from a knuckle-walking ancestor,[48] but this is not well-supported.[49]
Australopithecines have thirty-two teeth, like modern humans. Their molars were parallel, like those of great apes, and they had a slight pre-canine gap (diastema). Their canines were smaller, like modern humans, and with the teeth less interlocked than in previous hominins. In fact, in some australopithecines, the canines are shaped more like incisors.[50] The molars of Australopithecus fit together in much the same way those of humans do, with low crowns and four low, rounded cusps used for crushing. They have cutting edges on the crests.[50] However, australopiths generally evolved a larger postcanine dentition with thicker enamel.[51] Australopiths in general had thick enamel, like Homo, while other great apes have markedly thinner enamel.[50] Robust australopiths wore their molar surfaces down flat, unlike the more gracile species, who kept their crests.[50]
Australopithecus species are thought to have eaten mainly fruit, vegetables, and tubers, and perhaps easy-to-catch animals such as small lizards. Much research has focused on a comparison between the South African species A. africanus and Paranthropus robustus. Early analyses of dental microwear in these two species showed, compared to P. robustus, A. africanus had fewer microwear features and more scratches as opposed to pits on its molar wear facets.[52] Microwear patterns on the cheek teeth of A. afarensis and A. anamensis indicate that A. afarensis predominantly ate fruits and leaves, whereas A. anamensis included grasses and seeds (in addition to fruits and leaves).[53] The thickening of enamel in australopiths may have been a response to eating more ground-bound foods such as tubers, nuts, and cereal grains with gritty dirt and other small particulates which would wear away enamel. Gracile australopiths had larger incisors, which indicates tearing food was important, perhaps eating scavenged meat. Nonetheless, the wearing patterns on the teeth support a largely herbivorous diet.[50]
In 1992, trace-element studies of the strontium/calcium ratios in robust australopith fossils suggested the possibility of animal consumption, as they did in 1994 using stable carbon isotopic analysis.[54] In 2005, fossil animal bones with butchery marks dating to 2.6 million years old were found at the site of Gona, Ethiopia. This implies meat consumption by at least one of three species of hominins occurring around that time: A. africanus, A. garhi, and/or P. aethiopicus.[55] In 2010, fossils of butchered animal bones dated 3.4 million years old were found in Ethiopia, close to regions where australopith fossils were found.[56] However, a 2025 study measuring nitrogen isotope ratios in fossilized teeth determined that Australopithecus was almost entirely vegetarian.[57][58]
Robust australopithecines (Paranthropus) had larger cheek teeth than gracile australopiths, possibly because robust australopithecines had more tough, fibrous plant material in their diets, whereas gracile australopiths ate more hard and brittle foods.[50] However, such divergence in chewing adaptations may instead have been a response to fallback food availability. In leaner times, robust and gracile australopithecines may have turned to different low-quality foods (fibrous plants for the former, and hard food for the latter), but in more bountiful times, they had more variable and overlapping diets.[59][60] In a 1979 preliminary microwear study of Australopithecus fossil teeth, anthropologist Alan Walker theorized that robust australopiths ate predominantly fruit (frugivory).[61]
A study in 2018 found non-carious cervical lesions, caused by acid erosion, on the teeth of A. africanus, probably caused by consumption of acidic fruit.[62]
It is debated if the Australopithecus hand was anatomically capable of producing stone tools.[63] A. garhi was associated with large mammal bones bearing evidence of processing by stone tools, which may indicate australopithecine tool production.[64][65][66][67] Stone tools dating to roughly the same time as A. garhi (about 2.6 mya) were later discovered at the nearby Gona and Ledi-Geraru sites, but the appearance of Homo at Ledi-Geraru (LD 350-1) casts doubt on australopithecine authorship.[68]
In 2010, cut marks dating to 3.4 mya on a bovid leg were found at the Dikaka site, which were at first attributed to butchery by A. afarensis,[69] but because the fossil came from a sandstone unit (and were modified by abrasive sand and gravel particles during the fossilisation process), the attribution to butchery is dubious.[70]
In 2015, the Lomekwi culture was discovered at Lake Turkana dating to 3.3 mya, possibly attributable to Kenyanthropus[71] or A. deyiremeda.[72]

---

# Homo

For other species or subspecies suggested, see below.
Homo (from Latin homō 'human') is a genus of great ape (family Hominidae) that emerged from the genus Australopithecus and encompasses only a single extant species, Homo sapiens (modern humans), along with a number of extinct species (collectively called archaic humans) classified as either ancestral or closely related to modern humans; these include Homo erectus and Homo neanderthalensis. The oldest member of the genus is Homo habilis, with records of just over 2 million years ago.[a] Homo, together with the genus Paranthropus, is probably most closely related to the species Australopithecus africanus within Australopithecus.[4] The closest living relatives of Homo are of the genus Pan (chimpanzees and bonobos), with the ancestors of Pan and Homo estimated to have diverged around 5.7–11 million years ago during the Late Miocene.[5]
H. erectus appeared about 2 million years ago and spread throughout Africa (debatably as another species called Homo ergaster) and Eurasia in several migrations. The species was adaptive and successful, and persisted for more than a million years before gradually diverging into new species around 500,000 years ago.[b][6]
Anatomically modern humans (H. sapiens) emerged close to 300,000 to 200,000 years ago[7] in Africa, and H. neanderthalensis emerged around the same time in Europe and Western Asia. H. sapiens dispersed from Africa in several waves, from possibly as early as 250,000 years ago, and certainly by 130,000 years ago, with the so-called Southern Dispersal, beginning about 70–50,000 years ago,[8][9][10] leading to the lasting colonisation of Eurasia and Oceania by 50,000 years ago. H. sapiens met and interbred with archaic humans in Africa and in Eurasia.[11][12] Separate archaic (non-sapiens) human species including Neanderthals are thought to have survived until around 40,000 years ago.
The Latin noun homō (genitive hominis) means "human being" or "man" in the generic sense of "human being, mankind".[c] The binomial name Homo sapiens was coined by Carl Linnaeus (1758).[d][15] Names for other species of the genus were introduced from the second half of the 19th century (H. neanderthalensis 1864, H. erectus 1892).
The genus Homo has not been strictly defined, even today.[16][17][18] Since the early human fossil record began to slowly emerge from the earth, the boundaries and definitions of the genus have been poorly defined and constantly in flux. Because there was no reason to think it would ever have any additional members, Carl Linnaeus did not even bother to define Homo when he first created it for humans in the 18th century. The discovery of Neanderthal brought the first addition.
The genus Homo was given its taxonomic name to suggest that its member species can be classified as human. And, over the decades of the 20th century, fossil finds of pre-human and early human species from late Miocene and early Pliocene times produced a rich mix for debating classifications. There is continuing debate on delineating Homo from Australopithecus—or, indeed, delineating Homo from Pan. Even so, classifying the fossils of Homo coincides with evidence of: (1) competent human bipedalism in Homo habilis inherited from the earlier Australopithecus of more than four million years ago, as demonstrated by the Laetoli footprints; and (2) human tool culture having begun by 2.5 million years ago to 3 million years ago.[19]
From the late-19th to mid-20th centuries, a number of new taxonomic names, including new generic names, were proposed for early human fossils; most have since been merged with Homo in recognition that Homo erectus was a single species with a large geographic spread of early migrations. Many such names are now regarded as "synonyms" with Homo, including Pithecanthropus,[20] Protanthropus,[21] Sinanthropus,[22] Cyphanthropus,[23] Africanthropus,[24] Telanthropus,[25] Atlanthropus,[26] and Tchadanthropus.[27][28]
Classifying the genus Homo into species and subspecies is subject to incomplete information and remains poorly done. This has led to using common names ("Neanderthal" and "Denisovan"), even in scientific papers, to avoid trinomial names or the ambiguity of classifying groups as incertae sedis (uncertain placement)—for example, H. neanderthalensis vs. H. sapiens neanderthalensis, or H. georgicus vs. H. erectus georgicus.[29] Some recently extinct species in the genus have been discovered only lately and do not as yet have consensus binomial names (see Denisova hominin).[30] Since the beginning of the Holocene, it is likely that Homo sapiens (anatomically modern humans) has been the only extant species of Homo.
John Edward Gray (1825) was an early advocate of classifying taxa by designating tribes and families.[31] Wood and Richmond (2000) proposed that Hominini ("hominins") be designated as a tribe that comprised all species of early humans and pre-humans ancestral to humans back to after the chimpanzee–human last common ancestor, and that Hominina be designated a subtribe of Hominini to include only the genus Homo — that is, not including the earlier upright walking hominins of the Pliocene such as Australopithecus, Orrorin tugenensis, Ardipithecus, or Sahelanthropus.[32] Designations alternative to Hominina existed, or were offered: Australopithecinae (Gregory & Hellman 1939) and Preanthropinae (Cela-Conde & Altaba 2002);[33][34][35] and later, Cela-Conde and Ayala (2003) proposed that the four genera Australopithecus, Ardipithecus, Praeanthropus, and Sahelanthropus be grouped with Homo within Hominini (sans Pan).[34]
Several species, including Australopithecus garhi, Australopithecus sediba, Australopithecus africanus, and Australopithecus afarensis, have been proposed as the ancestor or sister of the Homo lineage.[36][37] These species have morphological features that align them with Homo, but there is no consensus as to which gave rise to Homo.
Especially since the 2010s, the delineation of Homo in Australopithecus has become more contentious. Traditionally, the advent of Homo has been taken to coincide with the first use of stone tools (the Oldowan industry), and thus by definition with the beginning of the Lower Palaeolithic. But in 2010, evidence was presented that seems to attribute the use of stone tools to Australopithecus afarensis around 3.3 million years ago, close to a million years before the first appearance of Homo.[38] LD 350-1, a fossil mandible fragment dated to 2.8 Mya, discovered in 2013 in Afar, Ethiopia, was described as combining "primitive traits seen in early Australopithecus with derived morphology observed in later Homo.[39] Some authors would push the development of Homo close to or even past 3 Mya.[e] This finds support in a recent phylogenetic study in hominins that by using morphological, molecular and radiometric information, dates the emergence of Homo at 3.3 Ma (4.30 – 2.56 Ma).[40] Others have voiced doubt as to whether Homo habilis should be included in Homo, proposing an origin of Homo with Homo erectus at roughly 1.9 Mya instead.[41]
The most salient physiological development between the earlier australopithecine species and Homo is the increase in endocranial volume (ECV), from about 460 cm3 (28 cu in) in A. garhi to 660 cm3 (40 cu in) in H. habilis and further to 760 cm3 (46 cu in) in H. erectus, 1,250 cm3 (76 cu in) in H. heidelbergensis and up to 1,760 cm3 (107 cu in) in H. neanderthalensis. However, a steady rise in cranial capacity is observed already in Autralopithecina and does not terminate after the emergence of Homo, so that it does not serve as an objective criterion to define the emergence of the genus.[42]
Homo habilis emerged about 2.1 Mya. Already before 2010, there were suggestions that H. habilis should not be placed in the genus Homo but rather in Australopithecus.[43][44] The main reason to include H. habilis in Homo, its undisputed tool use, has become obsolete with the discovery of Australopithecus tool use at least a million years before H. habilis.[38] Furthermore, H. habilis was long thought to be the ancestor of the more gracile Homo ergaster (Homo erectus). In 2007, it was discovered that H. habilis and H. erectus coexisted for a considerable time, suggesting that H. erectus is not immediately derived from H. habilis but instead from a common ancestor.[45] With the publication of Dmanisi skull 5 in 2013, it has become less certain that Asian H. erectus is a descendant of African H. ergaster which was in turn derived from H. habilis. Instead, H. ergaster and H. erectus appear to be variants of the same species, which may have originated in either Africa or Asia[46] and widely dispersed throughout Eurasia (including Europe, Indonesia, China) by 0.5 Mya.[47]
Homo erectus has often been assumed to have developed anagenetically from H. habilis from about 2 million years ago. This scenario was strengthened with the discovery of Homo erectus georgicus, early specimens of H. erectus found in the Caucasus, which seemed to exhibit transitional traits[example needed] with H. habilis. As the earliest evidence for H. erectus was found outside of Africa, it was considered plausible that H. erectus developed in Eurasia and then migrated back to Africa. Based on fossils from the Koobi Fora Formation, east of Lake Turkana in Kenya, Spoor et al. (2007) argued that H. habilis may have survived beyond the emergence of H. erectus, so that the evolution of H. erectus would not have been anagenetically, and H. erectus would have existed alongside H. habilis for about half a million years (1.9 to 1.4 million years ago), during the early Calabrian.[45] On 31 August 2023, researchers reported, based on genetic studies, that a human ancestor population bottleneck (from a possible 100,000 to 1000 individuals) occurred "around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction."[48][49]
Weiss (1984) estimated that there have been about 44 billion (short scale) members of the genus Homo from its origins to the evolution of H. erectus, about 56 billion individuals from H. erectus to the Neolithic, and another 51 billion individuals since the Neolithic. This provides the opportunity for an immense amount of new mutational variation to have arisen during human evolution.[50]
A separate South African species Homo gautengensis has been postulated as contemporary with H. erectus in 2010.[51]
A taxonomy of Homo within the great apes is assessed as follows, with Paranthropus and Homo emerging within Australopithecus (shown here cladistically granting Paranthropus, Kenyanthropus, and Homo).[a][f][6][53][52][4][54][55][56][57][58][59][60][excessive citations] The exact phylogeny within Australopithecus is still highly controversial. Approximate radiation dates of daughter clades are shown in millions of years ago (Mya).[40][57] Sahelanthropus and Orrorin, possibly sisters to Australopithecus, are not shown here. The naming of groupings is sometimes muddled as often certain groupings are presumed before any cladistic analysis is performed.[55]
Australopithecines (incl. Australopithecus, Kenyanthropus, Paranthropus, Homo)
Several of the Homo lineages appear to have surviving progeny through introgression into other lines. Genetic evidence indicates an archaic lineage separating from the other human lineages 1.5 million years ago, perhaps H. erectus, may have interbred into the Denisovans about 55,000 years ago.[62][54][63] Fossil evidence shows H. erectus s.s. survived at least until 117,000 yrs ago, and the even more basal H. floresiensis survived until 50,000 years ago. A 1.5-million-year H. erectus-like lineage appears to have made its way into modern humans through the Denisovans and specifically into the Papuans and aboriginal Australians.[54] The genomes of non-sub-Saharan African humans show what appear to be numerous independent introgression events involving Neanderthal and in some cases also Denisovans around 45,000 years ago.[64][63] The genetic structure of some sub-Saharan African groups seems to be indicative of introgression from a west Eurasian population some 3,000 years ago.[58][65]
Some evidence suggests that Australopithecus sediba could be moved to the genus Homo, or placed in its own genus, due to its position with respect to e.g. H. habilis and H. floresiensis.[56][66]
By about 1.8 million years ago, H. erectus is present in both East Africa (H. ergaster) and in Western Asia (H. georgicus). The ancestors of Indonesian H. floresiensis may have left Africa even earlier.[g][56]
Homo erectus and related or derived archaic human species over the next 1.5 million years spread throughout Africa and Eurasia[67][68] (see: Recent African origin of modern humans). Europe is reached by about 0.5 Mya by Homo heidelbergensis.
Homo neanderthalensis and H. sapiens develop after about 300 kya. Homo naledi is present in Southern Africa by 300 kya.
H. sapiens soon after its first emergence spread throughout Africa, and to Western Asia in several waves, possibly as early as 250 kya, and certainly by 130 kya. In July 2019, anthropologists reported the discovery of 210,000 year old remains of a H. sapiens and 170,000 year old remains of a H. neanderthalensis in Apidima Cave, Peloponnese, Greece, more than 150,000 years older than previous H. sapiens finds in Europe.[69][70][71]
Most notable is the Southern Dispersal of H. sapiens around 60 kya, which led to the lasting peopling of Oceania and Eurasia by anatomically modern humans.[11] H. sapiens interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.[72][73]
Among extant populations of H. sapiens, the deepest temporal division is found in the San people of Southern Africa, estimated at close to 130,000 years,[74] or possibly more than 300,000 years ago.[75] Temporal division among non-Africans is of the order of 60,000 years in the case of Australo-Melanesians. Division of Europeans and East Asians is of the order of 50,000 years, with repeated and significant admixture events throughout Eurasia during the Holocene.
Archaic human species may have survived until the beginning of the Holocene, although they were mostly extinct or absorbed by the expanding H. sapiens populations by 40 kya (Neanderthal extinction).
The species status of H. rudolfensis, H. ergaster, H. georgicus, H. antecessor, H. cepranensis, H. rhodesiensis, H. neanderthalensis, Denisova hominin, and H. floresiensis remain under debate. H. heidelbergensis and H. neanderthalensis are closely related to each other and have been considered to be subspecies of H. sapiens.
There has historically been a trend to postulate new human species based on as little as an individual fossil. A "minimalist" approach to human taxonomy recognizes at most three species, H. habilis (2.1–1.5 Mya, membership in Homo questionable), H. erectus (1.8–0.1 Mya, including the majority of the age of the genus, and the majority of archaic varieties as subspecies,[76][77][78] including H. heidelbergensis as a late or transitional variety[79][80][81]) and Homo sapiens (300 kya to present, including H. neanderthalensis and other varieties as subspecies). Consistent definitions and methodology of species delineation are not generally agreed upon in anthropology or paleontology. Indeed, speciating populations of mammals can typically interbreed for several million years after they begin to genetically diverge,[82][83] so all contemporary "species" in the genus Homo would potentially have been able to interbreed at the time, and introgression from beyond the genus Homo can not a priori be ruled out.[84] It has been suggested that H. naledi may have been a hybrid with a late surviving Australipith (taken to mean beyond Homo, ed.),[53] despite the fact that these lineages generally are regarded as long extinct. As discussed above, many introgressions have occurred between lineages, with evidence of introgression after separation of 1.5 million years.

---

# Cold and heat adaptations in humans

Cold and heat adaptations in humans are a part of the broad adaptability of Homo sapiens. Adaptations in humans can be physiological, genetic, or cultural, which allow people to live in a wide variety of climates. There has been a great deal of research done on developmental adjustment, acclimatization, and cultural practices, but less research on genetic adaptations to colder and hotter temperatures.
The human body always works to remain in homeostasis. One form of homeostasis is thermoregulation. Body temperature varies in every individual, but the average internal temperature is 37.0 °C (98.6 °F).[1] Sufficient stress from extreme external temperature may cause injury or death if it exceeds the ability of the body to thermoregulate. Hypothermia can set in when the core temperature drops to 35 °C (95 °F).[2] Hyperthermia can set in when the core body temperature rises above 37.5–38.3 °C (99.5–100.9 °F).[3][4] Humans have adapted to living in climates where hypothermia and hyperthermia were common primarily through culture and technology, such as the use of clothing and shelter.[5]
Modern humans emerged from Africa approximately 70,000 years ago during a period of unstable climate, leading to a variety of new traits among the population.[6][5] When modern humans spread into Europe, they outcompeted Neanderthals. Researchers hypothesize that this suggests early modern humans were more evolutionarily fit to live in various climates.[7][8] This is supported in the variability selection hypothesis proposed by Richard Potts, which says that human adaptability came from environmental change over the long term.[9]
Bergmann's rule states that endothermic animal subspecies living in colder climates have larger bodies than those of the subspecies living in warmer climates.[11] Individuals with larger bodies are better suited for colder climates because larger bodies produce more heat due to having more cells, and have a smaller surface area to volume ratio compared to smaller individuals, which reduces the proportional heat loss. A study by Frederick Foster and Mark Collard found that Bergmann's rule can be applied to humans when the latitude and temperature between groups differ widely.[12]
Allen's rule is a biological rule that says the limbs of endotherms are shorter in cold climates and longer in hot climates. Limb length affects the body's surface area, which helps with thermoregulation. Shorter limbs help to conserve heat, while longer limbs help to dissipate heat.[13] Marshall T. Newman argues that this can be observed in Eskimo, who have shorter limbs than other people and are laterally built.[14]
Paleoanthropologist John F. Hoffecker found that both Bermann's and Allen's biogeographical rules were confirmed, with it being seen that in modern populations, there is a clear trend of shorter distal limb segments in colder environments.[15]
Origins of heat and cold adaptations can be explained by climatic adaptation.[16][17] Ambient air temperature affects how much energy investment the human body must make. The temperature that requires the least amount of energy investment is 21 °C (70 °F).[5] [disputed – discuss] The body controls its temperature through the hypothalamus. Thermoreceptors in the skin send signals to the hypothalamus, which indicate when vasodilation and vasoconstriction should occur.
The human body has two methods of thermogenesis, which produces heat to raise the core body temperature. The first is shivering, which occurs in an unclothed person when the ambient air temperature is under 25 °C (77 °F)[dubious – discuss].[18] It is limited by the amount of glycogen available in the body.[5] The second is non-shivering, which occurs in brown adipose tissue.[19]
Population studies have shown that the San tribe of Southern Africa and the Sandawe of Eastern Africa have reduced shivering thermogenesis in the cold, and poor cold-induced vasodilation in fingers and toes compared to that of Caucasians.[5]
The only mechanism the human body has to cool itself is by sweat evaporation.[5] Sweating occurs when the ambient air temperature is above 35 °C (95 °F)[dubious – discuss] and the body fails to return to the normal internal temperature.[18] The evaporation of the sweat helps cool the blood beneath the skin. It is limited by the amount of water available in the body, which can cause dehydration.[5]
Humans adapted to heat early on. In Africa, the climate selected for traits that helped them stay cool. Also, humans had physiological mechanisms that reduced the rate of metabolism and that modified the sensitivity of sweat glands to provide an adequate amount for cooldown without the individual becoming dehydrated.[17][20]
There are two types of heat the body is adapted to, humid heat and dry heat, but the body adapts to both in similar ways. Humid heat is characterized by warmer temperatures with a high amount of water vapor in the air, while dry heat is characterized by warmer temperatures with little to no vapor, such as desert conditions. With humid heat, the moisture in the air can prevent the evaporation of sweat.[21] Regardless of acclimatization, humid heat poses a far greater threat than dry heat; humans cannot carry out physical outdoor activities at any temperature above 32 °C (90 °F) when the ambient humidity is greater than 95%.[citation needed] When combined with this high humidity, the theoretical limit to human survival in the shade, even with unlimited water, is 35 °C (95 °F) – theoretically equivalent to a heat index of 70 °C (158 °F).[22][23] Dry heat, on the other hand, can cause dehydration, as sweat will tend to evaporate extremely quickly. Individuals with less fat and slightly lower body temperatures can more easily handle both humid and dry heat.[16]
When humans are exposed to certain climates for extended periods of time, physiological changes occur to help the individual adapt to hot or cold climates. This helps the body conserve energy.[19]
The Inuit have more blood flowing into their extremities, and at a hotter temperature, than people living in warmer climates. A 1960 study on the Alacaluf Indians shows that they have a resting metabolic rate 150 to 200 percent higher than the white controls used. The Sami do not have an increase in metabolic rate when sleeping, unlike non-acclimated people.[14] Aboriginal Australians undergo a similar process, where the body cools but the metabolic rate does not increase.[18]
Humans and their evolutionary predecessors in Central Africa have been living in similar tropical climates for millions of years, which means that they have similar thermoregulatory systems.[5]
A study done on the Bantus of South Africa showed that Bantus have a lower sweat rate than that of acclimated and nonacclimated white people. A similar study done on Aboriginal Australians produced similar results, with Indigenous Australians having a much lower sweat rate than Caucasians.[18]
Social adaptations enabled early modern humans to occupy environments with temperatures that were drastically different from that of Africa. (Potts 1998). Culture enabled humans to expand their range to areas that would otherwise be uninhabitable.[18]
Humans have been able to occupy areas of extreme cold through clothing, buildings, and manipulation of fire. Furnaces have further enabled the occupation of cold environments.[18][19]
Historically many Indigenous Australians wore only genital coverings. Studies have shown that the warmth from the fires they build is enough to keep the body from fighting heat loss through shivering.[18] Inuit use well-insulated houses that are designed to transfer heat from an energy source to the living area, which means that the average indoor temperature for coastal Inuit is 10 to 20 °C (50 to 68 °F).[18]
Humans inhabit hot climates, both dry and humid, and have done so for millions of years. Selective use of clothing and technological inventions such as air conditioning allows humans to live in hot climates.
One example is the Chaamba, who live in the Sahara Desert. They wear clothing that traps air in between skin and the clothes, preventing the high ambient air temperature from reaching the skin.[18]

---

# Human evolutionary developmental biology

Human evolutionary developmental biology or informally human evo-devo is the human-specific subset of evolutionary developmental biology. Evolutionary developmental biology is the study of the evolution of developmental processes across different organisms. It is utilized within multiple disciplines, primarily evolutionary biology and anthropology. Groundwork for the theory that "evolutionary modifications in primate development might have led to … modern humans" was laid by Geoffroy Saint-Hilaire, Ernst Haeckel, Louis Bolk, and Adolph Schultz.[1] Evolutionary developmental biology is primarily concerned with the ways in which evolution affects development,[2] and seeks to unravel the causes of evolutionary innovations.[3]
The approach is relatively new, but has roots in Schultz's The physical distinctions of man, from the 1940s. Shultz urged broad comparative studies to identify uniquely human traits.[4]
Brian Hall traces the roots of evolutionary developmental biology in his 2012 paper on its past present and future. He begins with Darwinian evolution and Mendel's genetics, noting the tendency of the followers of both men in the early 20th century to follow separate paths and to set aside and ignore apparently inexplicable problems.[5] Greater understanding of genotypic and phenotypic structures from the 1940s enabled the unification of evolution and genetics in the modern synthesis. Molecular biology then enabled researchers to explore the mechanisms and evolution of embryonic development in molecular detail, including in humans.[5]
Many of the human evolutionary developmental biology studies have been modeled after primate studies and consider the two together in a comparative model. Brain ontogeny and human life history evolution were looked at by Leigh, in a 2006 paper. He compares brain growth patterns for Homo erectus and Homo sapiens to get at the evolution of brain size and weight. Leigh found three different patterns, all of which pointed to the growth rate of H. erectus either matching or exceeding H. erectus.[6] He makes the case that this finding had wide application and relevance to the overall study of human evolution. It is pertinent specifically to the connections between energy expenditure and brain development. These finding are of specific utility in studies on maternal energy expenditure.[6] Comparative study of nonhuman primates, fossils and modern humans to study patterns of brain growth to correlate human life history and brain growth.[6]
Jeremy De Silva and Julie Lesnik examined chimpanzee neonatal brain size to identify implications for brain growth in Homo erectus. This changed the understanding of differences and similarities of post-natal brain growth in humans and chimpanzees. The study found that there was a distinction necessary between growth time and growth rate. The times of growth were strikingly similar, but the rates were not. The paper further advocates the use of fossils to assess brain size in general and in relation to cranial capacity.[7]
Utilization of endocranial volume as a measure for brain size has been a popular methodology with the fossil record since Darwin in the mid 1800s. This measure has been used to access the metabolic requirements for brain growth and the subsequent trade-offs.
Some of the work on human evolutionary developmental biology has centered around the neotenous features that present in humans, but are not shared across the primate spectrum. Steven J. Gould discussed the presentation of neoteny with "terminal additions" in humans.[8] Neoteny is defined as the delayed or slowed development in humans when compared with their non-human primate counterparts. The "terminal additions" were extensions or reductions in the rate and scope of stages of development and growth.[8][pages needed] Gould hypothesized that this process and production of neoteny in humans might be the key feature that ultimately lead to the emotional and communicative nature of humans. He credits this factor as an integral facet of human evolution. However, there have also been cautions against the application of this aspect to group ranking during it inappropriate as a measure of evolutionary achievement.[9]
Early comparative and human studies examined the fossil record to measure features like cranial sizes and capacities so as to infer brain size, growth rate, total growth and potential implications for energy expenditure. Helpful as this is, the static nature of individual fossils presents its own challenge. The phylogenic fossil line is itself a hypothesis, so anything based upon it is equally hypothetical.[10]
Using the fossil record of Neanderthals, modern humans, and chimpanzees, Gunz et al. examined that patterns of endocranial development.[11] They found that there are common features shared between the three, and that modern humans diverge from these common patterns in the first year of life. They concluded that even though much of the developmental results are similar insofar as brain size, the trajectories by which they arrived are not shared. Most of the differences between the two arise post-natally, in the first year, with cognitive development.[11]
There have been a number of studies that not only take incomplete fossil records into consideration, but have attempted to specifically identify the barriers presented by this condition. For example, Kieran McNulty covers the potential utilities and constraints of using incomplete fossil taxa to examine longitudinal development in Australopithecus africanis.[10]
Many studies on development have been human-specific. In his 2011 paper, Bernard Crespi focused on adaptation and genomic conflict in childhood diseases. He considers the evolution of childhood diseases and their risk levels, and finds that both risk and disease have evolved.[12]
Hotchberg and Belsky incorporate a life-history perspective, looking at adolescence. Substantial variation in phenotypic paths and presentations suggest significant environmental influence. They focus on plasticity between stages of development and the factors that shape it. Rate of maturation, fecundity, and fertility were all impacted by environmental circumstances. They argue that early maturation can be positive, reflecting opportunistic actions within specific conditions.[13]
Technological advances that have allowed better and better access to the growth of the human form in utero have proven particularly formative in studies involving focus on genetic and epigenetic development. Bakker et al. look at the interconnected nature of developmental processes and attempt to use fetal vertebral abnormalities as an indicator for other malformations. They found that the origin of the cells was not nearly as highly correlated as the observed developmental signals.[14] In utero development and malformations were correlated in severity.[14]
Freiston and Galis look at the development of ribs, digits, and mammalian asymmetry. They argue that this construction is relevant for the study of disease, the consistency in evolution of body plans, and understanding of developmental constraints.[15] Sexual dimorphism in prenatal digit ratio was found as early as 14 weeks and was maintained whether or not the fleshy finger part was included.[15]
Languages and cognitive function have also been subjects of evolutionary studies. Insofar as language and evolutionary developmental biology, there is tension from the gate. Much of this contention has centered around whether to view and study language as an adaptation in and of itself, or as a by-product of other adaptations. Jackendoff and Pinker have argued for language as an adaptation owing to the interdependent social nature of humans. To support these claims, he points to things like the bi-directionality in language usage and comprehension.[16] This is a counter to the claims by theorists like Noam Chomsky, who argued against language as a human specific adaptation.[17]
Adaptation and adaptive theory has been argued even separate from its utility in the study of language. Gould and Lewontin engage with what they saw as flaws in adaptive theory using the analogy of the spandrels of San Marco. Among the issues identified is the lack of distinction between what trait developed and how it is used, and the underlying reasons or forces that created the novel trait initially.[18] This is particularly difficult to access in intangible language and cognition.
This debate has continued over decades and most often presents in the form of a response and published dialogue between theorists. This continued debate has prompted efforts to marry the two perspectives in a useful way. Fitch argues that these two approaches can be rectified with the study of "neutral computation and mammalian brain development".[19] It may be more useful to consider specific components of neural computation and development, what has been selected for, and to what end.[19]
Ploeger and Galis tackled modular evolvability and developmental constraints in human and other primate evolutionary trajectories. They argue that these should be treated with an interdisciplinary approach across the cognitive sciences. They frame this in the context of:

---

# Paleoanthropology

Paleoanthropology or paleo-anthropology is a branch of paleontology and anthropology which seeks to understand the early development of anatomically modern humans, a process known as hominization, through the reconstruction of evolutionary kinship lines within the family Hominidae, working from biological evidence (such as petrified skeletal remains, bone fragments, footprints) and cultural evidence (such as stone tools, artifacts, and settlement localities).[1][2]
The field draws from and combines primatology, paleontology, biological anthropology, and cultural anthropology. As technologies and methods advance, genetics plays an ever-increasing role, in particular to examine and compare DNA structure as a vital tool of research of the evolutionary kinship lines of related species and genera.
The term paleoanthropology derives from Greek palaiós (παλαιός) "old, ancient", ánthrōpos (ἄνθρωπος) "man, human" and the suffix -logía (-λογία) "study of".
Hominoids are a primate superfamily, the hominid family is currently considered to comprise both the great ape lineages and human lineages within the hominoid superfamily. The "Homininae" comprise both the human lineages and the African ape lineages. The term "African apes" refers only to chimpanzees and gorillas.[3] The terminology of the immediate biological family is currently in flux. The term "hominin" refers to any genus in the human tribe (Hominini), of which Homo sapiens (modern humans) is the only living specimen.[4][5]
In 1758 Carl Linnaeus introduced the name Homo sapiens as a species name in the 10th edition of his work Systema Naturae although without a scientific description of the species-specific characteristics.[6] Since the great apes were considered the closest relatives of human beings, based on morphological similarity, in the 19th century, it was speculated that the closest living relatives to humans were chimpanzees (genus Pan) and gorilla (genus Gorilla), and based on the natural range of these creatures, it was surmised that humans shared a common ancestor with African apes and that fossils of these ancestors would ultimately be found in Africa.[6][7]
The science arguably began in the late 19th century when important discoveries occurred that led to the study of human evolution. The discovery of the Neanderthal in Germany, Thomas Huxley's Evidence as to Man's Place in Nature, and Charles Darwin's The Descent of Man were both important to early paleoanthropological research.
The modern field of paleoanthropology began in the 19th century with the discovery of "Neanderthal man" (the eponymous skeleton was found in 1856, but there had been finds elsewhere since 1830), and with evidence of so-called cave men. The idea that humans are similar to certain great apes had been obvious to people for some time, but the idea of the biological evolution of species in general was not legitimized until after Charles Darwin published On the Origin of Species in 1859.
Though Darwin's first book on evolution did not address the specific question of human evolution—"light will be thrown on the origin of man and his history," was all Darwin wrote on the subject—the implications of evolutionary theory were clear to contemporary readers.
Debates between Thomas Huxley and Richard Owen focused on the idea of human evolution. Huxley convincingly illustrated many of the similarities and differences between humans and apes in his 1863 book Evidence as to Man's Place in Nature. By the time Darwin published his own book on the subject, Descent of Man, it was already a well-known interpretation of his theory—and the interpretation which made the theory highly controversial. Even many of Darwin's original supporters (such as Alfred Russel Wallace and Charles Lyell) balked at the idea that human beings could have evolved their apparently boundless mental capacities and moral sensibilities through natural selection.
Prior to the general acceptance of Africa as the root of genus Homo, 19th-century naturalists sought the origin of humans in Asia. So-called "dragon bones" (fossil bones and teeth) from Chinese apothecary shops were known, but it was not until the early 20th century that German paleontologist, Max Schlosser, first described a single human tooth from Beijing. Although Schlosser (1903) was very cautious, identifying the tooth only as "?Anthropoide g. et sp. indet?," he was hopeful that future work would discover a new anthropoid in China.
Eleven years later, the Swedish geologist Johan Gunnar Andersson was sent to China as a mining advisor and soon developed an interest in "dragon bones". It was he who, in 1918, discovered the sites around Zhoukoudian, a village about 50 kilometers southwest of Beijing. However, because of the sparse nature of the initial finds, the site was abandoned.
Work did not resume until 1921, when the Austrian paleontologist, Otto Zdansky, fresh with his doctoral degree from Vienna, came to Beijing to work for Andersson. Zdansky conducted short-term excavations at Locality 1 in 1921 and 1923, and recovered only two teeth of significance (one premolar and one molar) that he subsequently described, cautiously, as "?Homo sp." (Zdansky, 1927). With that done, Zdansky returned to Austria and suspended all fieldwork.
News of the fossil hominin teeth delighted the scientific community in Beijing, and plans for developing a larger, more systematic project at Zhoukoudian were soon formulated. At the epicenter of excitement was Davidson Black, a Canadian-born anatomist working at Peking Union Medical College. Black shared Andersson’s interest, as well as his view that central Asia was a promising home for early humankind. In late 1926, Black submitted a proposal to the Rockefeller Foundation seeking financial support for systematic excavation at Zhoukoudian and the establishment of an institute for the study of human biology in China.
The Zhoukoudian Project came into existence in the spring of 1927, and two years later, the Cenozoic Research Laboratory of the Geological Survey of China was formally established. Being the first institution of its kind, the Cenozoic Laboratory opened up new avenues for the study of paleogeology and paleontology in China. The Laboratory was the precursor of the Institute of Vertebrate Paleontology and Paleoanthropology (IVPP) of the Chinese Academy of Science, which took its modern form after 1949.
The first of the major project finds are attributed to the young Swedish paleontologist, Anders Birger Bohlin, then serving as the field advisor at Zhoukoudian. He recovered a left lower molar that Black (1927) identified as unmistakably human (it compared favorably to the previous find made by Zdansky), and subsequently coined it Sinanthropus pekinensis.[8] The news was at first met with skepticism, and many scholars had reservations that a single tooth was sufficient to justify the naming of a new type of early hominin. Yet within a little more than two years, in the winter of 1929, Pei Wenzhong, then the field director at Zhoukoudian, unearthed the first complete calvaria of Peking Man. Twenty-seven years after Schlosser’s initial description, the antiquity of early humans in East Asia was no longer a speculation, but a reality.
Excavations continued at the site and remained fruitful until the outbreak of the Second Sino-Japanese War in 1937. The decade-long research yielded a wealth of faunal and lithic materials, as well as hominin fossils. These included 5 more complete calvaria, 9 large cranial fragments, 6 facial fragments, 14 partial mandibles, 147 isolated teeth, and 11 postcranial elements—estimated to represent as least 40 individuals. Evidence of fire, marked by ash lenses and burned bones and stones, were apparently also present,[9] although recent studies have challenged this view.[10] Franz Weidenreich came to Beijing soon after Black’s untimely death in 1934, and took charge of the study of the hominin specimens.
Following the loss of the Peking Man materials in late 1941, scientific endeavors at Zhoukoudian slowed, primarily because of lack of funding. Frantic search for the missing fossils took place, and continued well into the 1950s. After the establishment of the People’s Republic of China in 1949, excavations resumed at Zhoukoudian. But with political instability and social unrest brewing in China, beginning in 1966, and major discoveries at Olduvai Gorge and East Turkana (Koobi Fora), the paleoanthropological spotlight shifted westward to East Africa. Although China re-opened its doors to the West in the late 1970s, national policy calling for self-reliance, coupled with a widened language barrier, thwarted all the possibilities of renewed scientific relationships. Indeed, Harvard anthropologist K. C. Chang noted, "international collaboration (in developing nations very often a disguise for Western domination) became a thing of the past" (1977: 139).
The first paleoanthropological find made in Africa was the 1921 discovery of the Kabwe 1 skull at Kabwe (Broken Hill), Zambia. Initially, this specimen was named Homo rhodesiensis; however, today it is considered part of the species Homo heidelbergensis.[11]
In 1924 in a limestone quarry at Taung, Professor Raymond Dart discovered a remarkably well-preserved juvenile specimen (face and brain endocast), which he named Australopithecus africanus (Australopithecus meaning "Southern Ape"). Although the brain was small (410 cm3), its shape was rounded, unlike the brain shape of chimpanzees and gorillas, and more like the shape seen in modern humans. In addition, the specimen exhibited short canine teeth, and the anterior placement of the foramen magnum was more like the placement seen in modern humans than the placement seen in chimpanzees and gorillas, suggesting that this species was bipedal.
All of these traits convinced Dart that the Taung child was a bipedal human ancestor, a transitional form between ape and human. However, Dart's conclusions were largely ignored for decades, as the prevailing view of the time was that a large brain evolved before bipedality. It took the discovery of additional australopith fossils in Africa that resembled his specimen, and the rejection of the Piltdown Man hoax, for Dart's claims to be taken seriously.
In the 1930s, paleontologist Robert Broom discovered and described a new species at Kromdraai, South Africa. Although similar in some ways to Dart's Australopithecus africanus, Broom's specimen had much larger cheek teeth. Because of this difference, Broom named his specimen Paranthropus robustus, using a new genus name. In doing so, he established the practice of grouping gracile australopiths in the genus Australopithecus and robust australopiths in the genus Paranthropus. During the 1960s, the robust variety was commonly moved into Australopithecus. A more recent consensus has been to return to the original classification of Paranthropus as a separate genus.[12]
The second half of the twentieth century saw a significant increase in the number of paleoanthropological finds made in Africa. Many of these finds were associated with the work of the Leakey family in eastern Africa. In 1959, Mary Leakey's discovery of the Zinj fossin (OH 5) at Olduvai Gorge, Tanzania, led to the identification of a new species, Paranthropus boisei.[13] In 1960, the Leakeys discovered the fossil OH 7, also at Olduvai Gorge, and assigned it to a new species, Homo habilis. In 1972, Bernard Ngeneo, a fieldworker working for Richard Leakey, discovered the fossil KNM-ER 1470 near Lake Turkana in Kenya. KNM-ER 1470 has been interpreted as either a distinct species, Homo rudolfensis, or alternatively as evidence of sexual dimorphism in Homo habilis.[12] In 1967, Richard Leakey reported the earliest definitive examples of anatomically modern Homo sapiens from the site of Omo Kibish in Ethiopia, known as the Omo remains.[14] In the late 1970s, Mary Leakey excavated the famous Laetoli footprints in Tanzania, which demonstrated the antiquity of bipedality in the human lineage.[12] In 1985, Richard Leakey and Alan Walker discovered a specimen they called the Black Skull, found near Lake Turkana. This specimen was assigned to another species, Paranthropus aethiopicus.[15] In 1994, a team led by Meave Leakey announced a new species, Australopithecus anamensis, based on specimens found near Lake Turkana.[12]
Numerous other researchers have made important discoveries in eastern Africa. Possibly the most famous is the Lucy skeleton, discovered in 1973 by Donald Johanson and Maurice Taieb in Ethiopia's Afar Triangle at the site of Hadar. On the basis of this skeleton and subsequent discoveries, the researchers came up with a new species, Australopithecus afarensis.[12] In 1975, Colin Groves and Vratislav Mazák announced a new species of human they called Homo ergaster. Homo ergaster specimens have been found at numerous sites in eastern and southern Africa.[12] In 1994, Tim D. White announced a new species, Ardipithecus ramidus, based on fossils from Ethiopia.[16]
In 1999, two new species were announced. Berhane Asfaw and Tim D. White named Australopithecus garhi based on specimens discovered in Ethiopia's Awash valley. Meave Leakey announced a new species, Kenyanthropus platyops, based on the cranium KNM-WT 40000 from Lake Turkana.[12]
In the 21st century, numerous fossils have been found that add to current knowledge of existing species. For example, in 2001, Zeresenay Alemseged discovered an Australopithecus afarensis child fossil, called Selam, from the site of Dikika in the Afar region of Ethiopia. This find is particularly important because the fossil included a preserved hyoid bone, something rarely found in other paleoanthropological fossils but important for understanding the evolution of speech capacities.[11][12]
Two new species from southern Africa have been discovered and described in recent years. In 2008, a team led by Lee Berger announced a new species, Australopithecus sediba, based on fossils they had discovered in Malapa cave in South Africa.[12] In 2015, a team also led by Lee Berger announced another species, Homo naledi, based on fossils representing 15 individuals from the Rising Star Cave system in South Africa.[17]
New species have also been found in eastern Africa. In 2000, Brigitte Senut and Martin Pickford described the species Orrorin tugenensis, based on fossils they found in Kenya. In 2004, Yohannes Haile-Selassie announced that some specimens previously labeled as Ardipithecus ramidus made up a different species, Ardipithecus kadabba.[12] In 2015, Haile-Selassie announced another new species, Australopithecus deyiremeda, though some scholars are skeptical that the associated fossils truly represent a unique species.[18]
Although most hominin fossils from Africa have been found in eastern and southern Africa, there are a few exceptions. One is Sahelanthropus tchadensis, discovered in the central African country of Chad in 2002. This find is important because it widens the assumed geographic range of early hominins.[12]

---

# List of fictional primates

This list of fictional primates is a subsidiary to the list of fictional animals. The list is restricted to notable non-human primate characters from the world of fiction including chimpanzees, gorillas, orangutans, monkeys, lemurs, and other primates.
This section deals with notable primates who are prominently featured in various video game titles, either as main characters or notable supporting characters.

---

# Mythic humanoids

Mythic humanoids are legendary, folkloric, or mythological creatures that are part human, or that resemble humans through appearance or character. Each culture has different mythical creatures that come from many different origins, and many of these creatures are humanoids. They are often able to talk and in many stories they guide the hero on their journey.

---

# List of individual apes

This is a list of non-human apes of encyclopedic interest. It includes individual chimpanzees, gorillas, orangutans, bonobos, and gibbons that are in some way famous or notable.

---

# Monkeys and apes in space

Before humans went into space in the 1960s, several other animals were launched into space, including numerous other primates, so that scientists could investigate the biological effects of spaceflight. The United States launched flights containing primate passengers primarily between 1948 and 1961 with one flight in 1969 and one in 1985. France launched two monkey-carrying flights in 1967. The Soviet Union and Russia launched monkeys between 1983 and 1996. Most primates were anesthetized before lift-off.
Over thirty-two non-human primates flew in the space program; none flew more than once. Numerous backup primates also went through the programs but never flew. Monkeys and non-human apes from several species were used, including rhesus macaque, crab-eating macaque, squirrel monkeys, pig-tailed macaques, and chimpanzees.
The first primate launched into high subspace, although not a space flight, was Albert I, a rhesus macaque, who on June 18, 1948, rode a rocket flight to over 63 km (39 mi) in Earth's atmosphere on a V-2 rocket. Albert I died of suffocation during the flight and may actually have died in the cramped space capsule before launch.[1][2][3]
On June 14, 1949, Albert II survived a sub-orbital V-2 flight into space (but died on impact after a parachute failure)[2] to become the first monkey, first primate, and first mammal in space. His flight reached 134 km (83 mi) – past the Kármán line of 100 km which designates the beginning of space.[4]
On September 16, 1949, Albert III died below the Kármán line, at 35,000 feet (10.7 km), in an explosion of his V2. On December 8, Albert IV, the second mammal in space, flew on the last monkey V-2 flight and died on impact after another parachute failure[2] after reaching 130.6 km. Alberts, I, II, and IV were rhesus macaques while Albert III was a crab-eating macaque.
Monkeys later flew on Aerobee rockets. On April 18, 1951, a monkey, possibly called Albert V, died due to parachute failure. Yorick, also called Albert VI, along with 11 mouse crewmates, reached 236,000 ft (72 km, 44.7 mi) and survived the landing, on September 20, 1951, the first monkey to do so (the dogs Dezik and Tsygan had survived a trip to space in July of that year), although he died two hours later. Two of the mice also died after recovery; all of the deaths were thought to be related to stress from overheating in the sealed capsule in the New Mexico sun while awaiting the recovery team.[2] Albert VI's flight surpassed the 50-mile boundary the U.S. used for spaceflight but was below the international definition of space. Patricia and Mike, two cynomolgus monkeys, flew on May 21, 1952, and survived, but their flight was only to 26 kilometers.[citation needed]
On December 13, 1958, Gordo, also called Old Reliable, a squirrel monkey, survived being launched aboard Jupiter AM-13 by the US Army.[4] After flying for over 1,500 miles and reaching a height of 310 miles (500 km) before returning to Earth, Gordo landed in the South Atlantic and was killed due to mechanical failure of the parachute recovery system in the rocket nose cone.[4]
On May 28, 1959, aboard the JUPITER AM-18, Miss Able, a rhesus macaque, and Miss Baker, a squirrel monkey from Peru, flew a successful mission. Able was born at the Ralph Mitchell Zoo in Independence, Kansas. They traveled in excess of 16,000 km/h, and withstood 38 g (373 m/s2). Able died June 1, 1959, while undergoing surgery to remove an infected medical electrode, from a reaction to the anesthesia. Baker became the first monkey to survive the stresses of spaceflight and the related medical procedures. Baker died November 29, 1984, at the age of 27 and is buried on the grounds of the United States Space & Rocket Center in Huntsville, Alabama. Able was preserved, and is now on display at the Smithsonian Institution's National Air and Space Museum. Their names were taken from the 1943–1955 US military phonetic alphabet.[5]
On December 4, 1959, from Wallops Island, Virginia, Sam, a rhesus macaque, flew on the Little Joe 2 in the Mercury program to 53 miles high.[4] On January 21, 1960, Miss Sam, also a rhesus macaque, followed, on Little Joe 1B although her flight was only to 8 mi (13 km) in a test of emergency procedures.[6]
Chimpanzees Ham and Enos also flew in the Mercury program, with Ham becoming the first great ape or Hominidae in space.[7] The names "Sam" and "Ham" were acronyms. Sam was named in homage to the School of Aerospace Medicine at Brooks Air Force Base in San Antonio, Texas, and the name "Ham" was taken from Holloman Aerospace Medicine at Holloman Air Force Base, New Mexico.[8] Ham and Enos were among 60 chimpanzees brought to New Mexico by the U.S. Air Force for space flight tests. Six were selected to be trained at Cape Canaveral by Tony Gentry et al.[9]
Goliath, a squirrel monkey, died in the explosion of his Atlas rocket on November 10, 1961. A rhesus macaque called Scatback flew a sub-orbital flight on December 20, 1961, but was lost at sea after landing.[10]
Bonny, a pig-tailed macaque, flew on Biosatellite 3, a mission which lasted from June 29 to July 8, 1969. This was the first multi-day monkey flight but came after longer human spaceflights were common. He died within a day of landing.[11]
Spacelab 3 on the Space Shuttle flight STS-51-B featured two squirrel monkeys named No. 3165 and No. 384-80. The flight was from April 29 to May 6, 1985.[12]
France launched a pig-tailed macaque named Martine on a Vesta rocket on March 7, 1967, and another named Pierrette on March 13. These suborbital flights reached 243 km (151 mi) and 234 km (145 mi), respectively. Martine became the first monkey to survive more than a couple of hours after flying above the international definition of the edge of space (Ham and Enos, launched earlier by the United States, were chimpanzees).[13]
The Soviet /Russian space program used only rhesus macaques in its Bion satellite program in 1980s and 1990s.[14] The names of the monkeys began with sequential letters of the Russian alphabet (А, Б, В, Г, Д, Е, Ё, Ж, З...). The animals all survived their missions but for a single fatality in post-flight surgery, after which the program was canceled.
On December 23, 1969 in Chamical, La Rioja, as part of the 'Operación Navidad' (Operation Christmas), Argentina launched Juan (a tufted capuchin, native to Argentina's Misiones Province) using a two-stage Rigel 04 rocket. It ascended perhaps up to 82 kilometers and then was recovered successfully.[21][22][23] Other sources give 30, 60 or 72 kilometers.[24][25] All of these are below the international definition of space (100 km). Later, on February 1, 1970, the experience was repeated with a female monkey of the same species using an X-1 Panther rocket. Although it reached a higher altitude than its predecessor, it was lost after the capsule's parachute failed.[citation needed]
The PRC spacecraft Shenzhou 2 launched on January 9, 2001. It is rumored that inside the reentry module (precise information is lacking due to the secrecy surrounding China's space program) a monkey, dog, and rabbit rode aloft in a test of the spacecraft's life support systems. The SZ2 reentry module landed in Inner Mongolia on January 16. No images of the recovered capsule appeared in the press, leading to the widespread inference that the flight ended in failure. According to press reports citing an unnamed source, a parachute connection malfunction caused a hard landing.[26]
On January 28, 2013, AFP and Sky News reported that Iran had sent a monkey in a "Pishgam" rocket to a height of 72 miles (116 km) and retrieved "shipment".[27][28] Iranian media gave no details on the timing or location of the launch, while details that were reported raised questions about the claim. Pre-flight and post-flight photos clearly showed different monkeys.[29] The confusion was due to the publishing of an archive photo from 2011 by the Iranian Student News Agency (ISNA). According to Jonathan McDowell, a Harvard astronomer, "They just mixed that footage with the footage of the 2013 successful launch."[30]
On December 14, 2013, AFP and BBC reported that Iran again sent a monkey to space and safely returned it.[31][32] Rhesus macaques Aftab (2013.01.28) and Fargam (2013.12.14) were each launched separately into space and safely returned. Researchers continue to study the effects of the space trip on their offspring.[33][34]
The 2014 animated series All Hail King Julien: Exiled features a horde of highly intelligent chimpanzee cosmonauts, whom they claim the USSR abandoned on a Madagascar islet following the end of the Space Race. Although faithful to "Mother Russia", the chimpanzees vow to take revenge on humankind for declaring their obsolescence.
Folk musician Heather Maloney's song "Albert 1-5" is inspired by the first five macaques used in the United States space program who did not survive their space flights.[35]